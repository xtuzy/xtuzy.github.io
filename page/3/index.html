<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Marcovaldo</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h2 class="post-title"><a href="/2016/05/08/机器学习中使用的神经网络第四讲/">机器学习中使用的神经网络第四讲</a></h2><div class="post-meta">2016-05-08</div><a data-thread-key="2016/05/08/机器学习中使用的神经网络第四讲/" href="/2016/05/08/机器学习中使用的神经网络第四讲/#comments" class="ds-thread-count"></a><div class="post-content"><p>Geoffery Hinton教授的<em>Neuron Networks for Machine Learning</em>的第四讲主要介绍如何使用back propagation算法来学习到词汇的特征表示、Neuron-probabilistic language models和处理大规模输出的方法。</p>
<h2 id="Learning-to-predict-the-next-word"><a href="#Learning-to-predict-the-next-word" class="headerlink" title="Learning to predict the next word"></a>Learning to predict the next word</h2><p>接下来的几小节主要介绍如何使用back propagation算法来学习到词汇的特征表示。我们从一个很简单的例子开始，介绍使用back propagation算法来将词汇间的相关信息转换成特征向量。</p>
<p>下图给出了一个家庭的树状图，我们要做的就是让神经网络去理解树状图中的信息，将其中的信息翻译成一个个命题，如下面第二张图所示。<br></div><p class="readmore"><a href="/2016/05/08/机器学习中使用的神经网络第四讲/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/">机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation</a></h2><div class="post-meta">2016-05-05</div><a data-thread-key="2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/#comments" class="ds-thread-count"></a><div class="post-content"><p>Geoffrey Hinton教授的<em>Neuron Networks for Machine Learning</em>的第三讲主要介绍了线性/逻辑神经网络和BackPropagation，下面是整理的笔记。</p>
<h2 id="Learning-the-weights-of-a-linear-neuron"><a href="#Learning-the-weights-of-a-linear-neuron" class="headerlink" title="Learning the weights of a linear neuron"></a>Learning the weights of a linear neuron</h2><p>这一小节介绍线性神经网络的学习算法。线性神经网络很像感知机，但又有不同：在感知机中，权值向量总是越来越接近好的权值设定；在线性神经网络中，输出总是越来越接近目标输出。在感知机中，每一次更新权值向量，其就更接近每一个“一般可行”的权值向量，这限制了感知机不能应用于更加复杂的网络，因为两个好的权值向量的平均可能是一个坏的。故在多层神经网络中，我们不能使用感知机的学习流程，也不能使用类似的方法来证明学习的可行性。</p>
<p>在多层神经网络中，我们通过判断实际输出是否越来越接近目标输出来判断学习的性能是否在提高。这一策略在解决非凸问题时仍然奏效，但不适合应用于感知机的学习。最简单的例子是使用平方误差的线性神经网络（linear neurons），也称为线性过滤器（linear filter）。如下图所示，y是神经网络对期望输出的一个估计，w是权值向量，x是输入向量，学习的目标是最小化在所有训练样本上犯的错误之和。<br></div><p class="readmore"><a href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/">机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机</a></h2><div class="post-meta">2016-05-04</div><a data-thread-key="2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/#comments" class="ds-thread-count"></a><div class="post-content"><p>最近在Cousera上学习多伦多大学Geoffrey Hinton教授的<em>Nerual Networks for Machine Learning</em>，为保证学习效果，特整理了学习笔记，一方面加深理解，一方面试图将学到的东西讲清楚。</p>
<p>这一讲主要介绍神经网络的结构。</p>
<h2 id="Types-of-nerual-network-architectures"><a href="#Types-of-nerual-network-architectures" class="headerlink" title="Types of nerual network architectures"></a>Types of nerual network architectures</h2><p>这一小节介绍了三种不同的神经网络结构。</p>
<p>首先介绍向前反馈网络（feed forward network），其常见形式如下图所示，第一层是输入（input layer），最后一层是输出（output layer），中间是一层或多层隐匿单元（hidden layer，被称之为deep nerual network）。<br></div><p class="readmore"><a href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/26/机器学习基石第九讲：linear-regression/">机器学习基石第九讲：linear regression</a></h2><div class="post-meta">2016-04-26</div><a data-thread-key="2016/04/26/机器学习基石第九讲：linear-regression/" href="/2016/04/26/机器学习基石第九讲：linear-regression/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第十讲介绍线性回归问题（linear regression problem），从这一讲开始课程介绍具体的机器学习算法。后面的大部分内容，博主已经学过，所以笔记可能会简略。</p>
<h2 id="Linear-Regression-Problem"><a href="#Linear-Regression-Problem" class="headerlink" title="Linear Regression Problem"></a>Linear Regression Problem</h2><p>借助信用卡发放的问题来介绍线性回归，不过这一次不再是分类，而是要让算法根据客户信息给出信用额度。算法为每一个特征赋予一个权重，然后直接计算加权值，以此得到信用额度。<br></div><p class="readmore"><a href="/2016/04/26/机器学习基石第九讲：linear-regression/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/25/机器学习基石第八讲：noise-and-error/">机器学习基石第八讲：noise and error</a></h2><div class="post-meta">2016-04-25</div><a data-thread-key="2016/04/25/机器学习基石第八讲：noise-and-error/" href="/2016/04/25/机器学习基石第八讲：noise-and-error/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第八讲主要介绍噪声和误差度量，笔记整理在下面。</p>
<h2 id="Noise-and-Probabilistic-Target"><a href="#Noise-and-Probabilistic-Target" class="headerlink" title="Noise and Probabilistic Target"></a>Noise and Probabilistic Target</h2><p>现实中的数据很可能含有噪声（noise），例如前面的信用卡发放问题中，有的顾客符合发放标准但没有发给，或者同样情况的顾客有人发了有人没法，再或者顾客的信息不正确等等，VC bound是否在由噪声的情况下工作呢？</p>
<p>继续使用前面抽弹珠的例子，罐子中每一个弹珠代表一个数据，如果$f(x) \not= h(x)$，则将弹珠漆成橘色；如果$f(x) = h(x)$，则将弹珠漆成绿色。引入了噪声之后，原来的y就不是真正的f(x)了。现在想象这样每一个弹珠，它的颜色不停变化，称为probablistic(noisy) marbles，不过罐子内橘色弹珠的比例还是一定的。现在我们记录弹珠被抽出瞬间的颜色，比如一次抽取100颗弹珠，抽出瞬间有20颗是橘色的，那我们就可以估计抽出瞬间整个罐子的橘色弹珠比例为20%。<br></div><p class="readmore"><a href="/2016/04/25/机器学习基石第八讲：noise-and-error/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/25/机器学习基石第七讲：the-vc-dimension/">机器学习基石第七讲：the vc dimension</a></h2><div class="post-meta">2016-04-25</div><a data-thread-key="2016/04/25/机器学习基石第七讲：the-vc-dimension/" href="/2016/04/25/机器学习基石第七讲：the-vc-dimension/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第七讲主要介绍了VC dimension，笔记整理在下面。</p>
<h2 id="Definition-of-VC-Dimension"><a href="#Definition-of-VC-Dimension" class="headerlink" title="Definition of VC Dimension"></a>Definition of VC Dimension</h2><p>上一讲我们找到了B(N,k)的上限，拿它和$N^{k-1}$做一个比较，发现当N够大时，前者比后者小得多。<br></div><p class="readmore"><a href="/2016/04/25/机器学习基石第七讲：the-vc-dimension/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/24/机器学习基石第六讲：theory-of-generalization/">机器学习基石第六讲：theory of generalization</a></h2><div class="post-meta">2016-04-24</div><a data-thread-key="2016/04/24/机器学习基石第六讲：theory-of-generalization/" href="/2016/04/24/机器学习基石第六讲：theory-of-generalization/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第六讲继续讨论“学习是否可行的问题”。</p>
<h2 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h2><p>继续前面的讨论，我们看$m_H(N)$是否会有一个很小的增长速度。回顾前面的四种成长函数及其break point。我们知道k是一个成长函数的break point，那比k大的值全是break point。<br></div><p class="readmore"><a href="/2016/04/24/机器学习基石第六讲：theory-of-generalization/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/22/机器学习基石第五讲：training-versus-testing/">机器学习基石第五讲：training versus testing</a></h2><div class="post-meta">2016-04-22</div><a data-thread-key="2016/04/22/机器学习基石第五讲：training-versus-testing/" href="/2016/04/22/机器学习基石第五讲：training-versus-testing/#comments" class="ds-thread-count"></a><div class="post-content"><p><em>机器学习基石</em>第五讲继续讨论“学习是否可行”这一问题，这一讲比较难，建议大家多看两遍。</p>
<h2 id="Recap-and-Preview"><a href="#Recap-and-Preview" class="headerlink" title="Recap and Preview"></a>Recap and Preview</h2><p>前面的课程得出了这样一个结论：在训练数据集足够大，H中包含的hypothesis个数有限的前提下，我们可以证明每一个hypothesis的$E_{in}$和$E_{out}$是相当的。在这样的结论下，我们就可以从H中选择一个$E_{in}(g) \approx 0$的hypothesis作为我们的g，且大概差不多（PAC）可以说$E_{out}(g) \approx 0$，也就说明了学习是可行的。<br></div><p class="readmore"><a href="/2016/04/22/机器学习基石第五讲：training-versus-testing/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/20/机器学习基石第四讲：feasibility-of-learning/">机器学习基石第四讲：feasibility of learning</a></h2><div class="post-meta">2016-04-20</div><a data-thread-key="2016/04/20/机器学习基石第四讲：feasibility-of-learning/" href="/2016/04/20/机器学习基石第四讲：feasibility-of-learning/#comments" class="ds-thread-count"></a><div class="post-content"><p>刚刚完成<em>机器学习基石</em>的第四节，这一节讨论机器学习的可行性，用到了Hoeffding’s inequality等概率的知识，需要仔细揣摩。笔记整理在下面。</p>
<h2 id="Learning-is-Impossible"><a href="#Learning-is-Impossible" class="headerlink" title="Learning is Impossible?"></a>Learning is Impossible?</h2><p>前面的课程中曾提到过说学习可能是不可行的，为此我们还通过推导来证明PLA算法是否会停下来。那我们再来考虑这个问题，看看学习是否可行。</p>
<p>我们先来用我们的’human learning’来解决一个二元分类问题，如下图示，图中给出了3个标记为-1的图案和标记为+1的图案，然后我们来猜一下下面的那个图案应该标记为什么。<br></div><p class="readmore"><a href="/2016/04/20/机器学习基石第四讲：feasibility-of-learning/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/04/19/机器学习基石第三讲：types-of-learning/">机器学习基石第三讲：types of learning</a></h2><div class="post-meta">2016-04-19</div><a data-thread-key="2016/04/19/机器学习基石第三讲：types-of-learning/" href="/2016/04/19/机器学习基石第三讲：types-of-learning/#comments" class="ds-thread-count"></a><div class="post-content"><p>刚刚完成机器学习基石的第三讲，这一讲主要介绍了机器学习的分类，对何种问题应该使用何种机器学习方法。将笔记整理在下面。</p>
<h2 id="Learning-with-Different-Output-Space"><a href="#Learning-with-Different-Output-Space" class="headerlink" title="Learning with Different Output Space"></a>Learning with Different Output Space</h2><p>前面讲的信用卡发放问题是一个是非题，也就是说最后的输出只有两种，是一个二元分类（binary classification）。下图中给出了更多的二元分类问题的例子，对于这类问题我们要做的就是找到一个hypothesis（超平面或超曲面）能够很好的将下图中的圈圈和叉叉分开。当然，后面我们介绍更多的解决二元分类问题的算法。<br></div><p class="readmore"><a href="/2016/04/19/机器学习基石第三讲：types-of-learning/">阅读更多</a></p></div><nav class="page-navigator"><a class="extend prev" rel="prev" href="/page/2/">上一页</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">下一页</a></nav></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>