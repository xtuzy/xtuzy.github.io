<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>Machine Learning第七周笔记：支持向量机 | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Machine Learning第七周笔记：支持向量机</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Machine Learning第七周笔记：支持向量机</h1><div class="post-meta">Apr 6, 2016<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/04/06/Machine-Learning第七周笔记：支持向量机/" href="/2016/04/06/Machine-Learning第七周笔记：支持向量机/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Support-Vector-Machine"><span class="toc-number">1.</span> <span class="toc-text">Support Vector Machine</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Large-Margin-Classification"><span class="toc-number">1.1.</span> <span class="toc-text">Large Margin Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimization-Objective"><span class="toc-number">1.1.1.</span> <span class="toc-text">Optimization Objective</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Large-Margin-Intuition"><span class="toc-number">1.1.2.</span> <span class="toc-text">Large Margin Intuition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mathematics-Behind-Large-Margin-Classification"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mathematics Behind Large Margin Classification</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kernels"><span class="toc-number">1.2.</span> <span class="toc-text">Kernels</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernel-Ⅰ"><span class="toc-number">1.2.1.</span> <span class="toc-text">Kernel Ⅰ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernel-Ⅱ"><span class="toc-number">1.2.2.</span> <span class="toc-text">Kernel Ⅱ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVMs-in-Practice"><span class="toc-number">1.3.</span> <span class="toc-text">SVMs in Practice</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-An-SVM"><span class="toc-number">1.3.1.</span> <span class="toc-text">Using An SVM</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><p>今天在Cousera上学习了Machine Learning的第七周课程，这一周主要介绍了支持向量机（support vector machine），将学习笔记整理在下面。</p>
<h1 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h1><h2 id="Large-Margin-Classification"><a href="#Large-Margin-Classification" class="headerlink" title="Large Margin Classification"></a>Large Margin Classification</h2><h3 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h3>我们通过逻辑回归来引出支持向量机。下图给出了逻辑回归的$h_{\theta}(x)$及其图像，我们将其中的$\theta^Tx$记为z。从training set中取出一组数据（$x^{(i)}$, $y^{(i)}$），若有$y^{(i)}=1$，则我们应该得到$h_{\theta}(x)\approx1$，观察图像可知$\theta^Tx≫0$；相反，若有$y^{(i)}=0$，则我们应该得到$h_{\theta}(x)\approx0$，观察图像可知$\theta^Tx<<0$。 <a="" id="more">
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__1.png" alt="7_1"></p>
<p>我们来对逻辑回归的损失函数做一下分析，先在下面给出损失函数：<br>$$J_{\theta}(x)=-ylog\frac{1}{1+e^{-\theta^Tx}}-(1-y)log(1-\frac{1}{1+e^{-\theta^Tx}})$$</p>
<p>从training set中取出一组数据（$x^{(i)}$, $y^{(i)}$），当$y^{(i)}=1$时，损失函数的第二项为0，此时损失函数就变成了$-log\frac{1}{1+e^{-z}}$，下图左侧为其图像。观察图像可以看到，当给z一个越来越大的值时，函数值变得越来越接近0。现在我们将函数调整为图像中红线对应的函数，叫做$cost_1(z)$。当$y^{(i)}=0$时，损失函数的第一项为0，此时损失函数就变成了$-(1-y)log(1-\frac{1}{1+e^{-z}})$，下图右侧为其图像。观察图像可以看到，当给z一个越来越小的值时，函数值变得越来越接近0。现在我们将函数调整为图像中红线对应的函数，叫做$cost_0(z)$。那么以前求解hypothesis过程中用到的损失函数<br>$$\min_{\theta}\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}(-logh_{\theta}(x^{(i)}))+(1-y^{(i)})((-log(1-h_{\theta}(x^{(i)}))))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$$<br>调整为<br>$$\min_{\theta}\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}cost_1(\theta^Tx)+(1-y^{(i)})cost_0(\theta^Tx)]+\frac{\lambda}{2m}\sum_{j=1}{n}\theta_j^2$$<br>此时得到的这个函数就是我们在支持向量机中用到的损失函数，求解此最小值问题得到的$\theta$就是SVM的参数。对于给定的training set，$\frac{1}{m}$是一个固定值，其值不影响最终求得的$\theta$，所以我们将其省掉。</p>
<h3 id="Large-Margin-Intuition"><a href="#Large-Margin-Intuition" class="headerlink" title="Large Margin Intuition"></a>Large Margin Intuition</h3><p>首先给出SVM的损失函数：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__2.png" alt="7_2"></p>
在逻辑回归中，我们的初衷是对于training set中的任何一组数据($x^{(i)}$, $y^{(i)}$)，若$y^{(i)}=1$，则应该有$\theta^Tx>0$，而在SVM中改成了$\theta^Tx>1$；若$y^{(i)}=0$，则应该有$\theta^Tx<0$，而在svm中改成了$\theta^tx<-1$。在svm中多出来的这个1保证了其训练出来的参数\theta应该可以更好的拟合数据。 首先假设损失函数中的参数c是一个很大的值，我们来观察svm是如何工作的。在对损失函数最小化的过程中，我们需要让$\sum_{i="1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]$尽可能的趋近于0，此时我们的目标求解下面的最优化问题：" $$min\frac{1}{2}\sum_{i="1}^{n}\theta_j^2$$" $$s.t.　\theta^tx^{(i)}≥1　if　y^{(i)}="1$$" $$\theta^tx^{(i)}≤-1　if　y^{(i)}="0$$" <p="">我们来看下图中的例子，图中的数据点是线性可分的，也就是说可以找到无数条直线（无数个超平面）来将两个类别完全分开。在下图中画出的三条decision boundary中，黑色直线所表示的那条要优于另外两条，因为它更robust地将两个类别分开了。图中两条蓝色直线分别表示两个类别中数据点与黑色decision boundary的最小距离，这一距离称作SVM中分类超平面关于training set的函数间隔。多个超平面也就对应多个函数间隔，我们将其中最大的函数间隔称作最大函数间隔。因此我们要做的就是找到最大间隔及其对应的最大间隔分离超平面，这个最大间隔分离超平面就是我们最终得到的hypothesis。<p></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__11.png" alt="7_11"></p>
<h3 id="Mathematics-Behind-Large-Margin-Classification"><a href="#Mathematics-Behind-Large-Margin-Classification" class="headerlink" title="Mathematics Behind Large Margin Classification"></a>Mathematics Behind Large Margin Classification</h3><p>这一小节我们来介绍SVM涉及到的一些数学，着有助于理解SVM中的最优化目标，帮助我们更快的找到最大间隔分离超平面。我们来回顾一下内积的概念。下图给出了两个二维向量并在坐标系中画出了他们。图中红色线段p表示向量v在向量u上的投影，两个向量的内积可以表示为<br>$$u^Tv=p·\parallel{u}\parallel=u_1v_1+u_2v_2$$<br>其中$\parallel{u}\parallel=\sqrt[2]{u_1^2+u_2^2}$。而在第二个坐标系中，p是负的。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__3.png" alt="7_3"></p>
<p>回到SVM，下图给出了要求解的优化问题，给出了$\theta^Tx^{(i)}$的向量内积表示（这里我们取$x^{(i)}$和、theta都是二维的）：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__4.jpg" alt="7_4"></p>
<p>所以原来的优化问题就转换成了下图所示的优化问题。，给定一个training set，下图给出了两个decision boundary。在图像中选择两个数据点($x^{(1)}$, $y^{(1)}$)和($x^{(2)}$, $y^{(2)}$)，画出分别对应的$p^{(1)}$和$p^{(2)}$。可以看到，左下侧中$p^{(1)}$和$p^{(2)}$的绝对值都很小，为了满足优化函数中的条件函数，会使得$\parallel{\theta^2}\parallel$的值变得很大，这恰好和我们的目标矛盾。而右下侧中$p^{(1)}$和$p^{(2)}$的绝对值都比较大，所以我们就可以选择一个更小的$\parallel{\theta^2}\parallel$来满足我们的目标。这就在数学上解释了右下侧的decision boundary为什么会优于左下侧的decison boundary。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__5.png" alt="7_5"></p>
<h2 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h2><h3 id="Kernel-Ⅰ"><a href="#Kernel-Ⅰ" class="headerlink" title="Kernel Ⅰ"></a>Kernel Ⅰ</h3><p>这一小节我们来介绍处理线性不可分的数据集时经常用到的核技巧（kernel trick）。<br>在没有学习核技巧的情况下，我们需要引入一个多项式来表示decision boundary。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__6.png" alt="7_6"></p>
现在我们在坐标系中选定三个点$l^{(1)}$，$l^{(2)}$，$l^{(3)}$作为landmark，由此出发去产生新的特征。至于如何选择landmark我们后面再给出。具体的方法是对于给定的x有新的特征$f_1=similarity(x, l^{(1)})$，$f_2=similarity(x, l^{(2)})$和$f_3=similarity(x, l^{(3)})$，下图给出了由gaussian kernel给出的$f_i$。当然我们也可以选择其他的kernel来定义。当x很接近$l^{(1)}$时（即有$x≈l^{(1)}$），有$f_1≈1$；当x距离$l^{(1)}$很远时，有$f_1≈0$。对于另外两个特征也是一样，对于training set中任何一个数据都可以通过这种方式来获得新的特征值。
<!--more-->
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__7.png" alt="7_7"></p>
<p>我们给出一个例子，如下图所示。给出了$l^{(1)}$，$f_1$的定义，图像中的横纵坐标为x的两个维度，第三个维度代表新的特征值。只有当$x=l^{(1)}$时，其对应的新特征值$f_1$达到1，随着x越来越远离$l^{(1)}$，x对应的新特征值逐渐趋向于0。比较$\sigma^2$的不同取值下的三维图像可知，随着$\sigma^2$的减小，$f_1$的取值在$l^{(1)}$的周围变化得快，图像变得尖锐；随着$\sigma^2$的增大，$f_1$的取值在$l^{(1)}$的周围变化得慢，图像变得平缓。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__8.png" alt="7_8"></p>
<p>现在我们将前面的东西应用到分类上，假设现在我们的hypothesis如下图所给公式，其中$\theta=(-0.5, 1, 1, 0)^T$。我们在图像中选出3个数据点，分别计算它们和3个landmarks的相似度，带入hypothesis得到对数据点的分类。利用这种方法，我们可以把坐标系中的数据点分成两类：红色曲线包围的是一类，其余的是一类。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__9.png" alt="7_9"></p>
<h3 id="Kernel-Ⅱ"><a href="#Kernel-Ⅱ" class="headerlink" title="Kernel Ⅱ"></a>Kernel Ⅱ</h3><p>现在我们介绍如何选取landmark。下图给出了具体过程：给定training set，其中包含m个数据点。选择m个landmark，其中$l^{(i)}=x^{(i)}$。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__12.png" alt="7_12"></p>
<p>下图给出了使用了kernel的SVM：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__13.png" alt="7_13"></p>
上面的方法不适用于很大的training set，例如数据量m为5000，那就会有5000个landmark，相应的参数$\theta$的维度也是5000，求解参数则需要特别大的计算量。现在我们来讨论一下参数C(=$\frac{1}{\lambda}$)和Gaussian kernel中$\sigma^2$的选取：
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__14.png" alt="7_14"></p>
<h2 id="SVMs-in-Practice"><a href="#SVMs-in-Practice" class="headerlink" title="SVMs in Practice"></a>SVMs in Practice</h2><h3 id="Using-An-SVM"><a href="#Using-An-SVM" class="headerlink" title="Using An SVM"></a>Using An SVM</h3><p>这一小节我们介绍如何实际应用SVM。现在在不同的语言环境下已经有很多求解SVM参数的包供我们使用，现在我们来讨论一下C和kernel的选取。（linear kernel是指我们在使用SVM时没用引入kernel）当我们选取Gaussian kernel时，我们考虑$\sigma^2$的选取，以做到对bias-variance的权衡：当$\sigma^2$取值比较大时，hypothesis偏向于high bias；当$\sigma^2$取值较小时，hypothesis倾向于high variance。当特征值不多，training set较大时，Gaussian kernel可能是个较好的选择。当模型含有的特征数量很大时，核函数的计算量变得很大，这时就不适合用Gaussian kernel了。</p>
<p>并不是所有的similarity function都能构造有效的kernel，kernel需要满足Mercer’s Theorem以保证SVM中的优化问题有解。这里给出了使用率较高的几种kernel：polynomial kernel/string kernel/chi-square kernel/histogram，但最常用的还是linear kernel和Gaussian kernel。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__7__10.png" alt="7_10"></p>
<p><em>Andrew Ng是从逻辑回归出发去介绍的SVM，所以这里的SVM看起来更像是逻辑回归的一个变形，我会在以后的文章中给出对SVM的介绍。</em></p>
</0$，而在svm中改成了$\theta^tx<-1$。在svm中多出来的这个1保证了其训练出来的参数\theta应该可以更好的拟合数据。></0$。></div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/04/06/Machine-Learning第七周笔记：支持向量机/" data-id="cjfgusk9f0001cgur70fitnnz" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a></div><div class="post-nav"><a href="/2016/04/09/Machine-Learning第八周笔记：K-means和降维/" class="pre">Machine Learning第八周笔记：K means和降维</a><a href="/2016/04/03/Machine-Learning第六周笔记二：机器学习系统设计/" class="next">Machine Learning第六周笔记二：机器学习系统设计</a></div><div data-thread-key="2016/04/06/Machine-Learning第七周笔记：支持向量机/" data-title="Machine Learning第七周笔记：支持向量机" data-url="http://marcovaldong.github.io/2016/04/06/Machine-Learning第七周笔记：支持向量机/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/04/06/Machine-Learning第七周笔记：支持向量机/" data-title="Machine Learning第七周笔记：支持向量机" data-url="http://marcovaldong.github.io/2016/04/06/Machine-Learning第七周笔记：支持向量机/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>