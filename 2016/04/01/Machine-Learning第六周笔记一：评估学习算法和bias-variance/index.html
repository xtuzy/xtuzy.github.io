<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>Machine Learning第六周笔记一：评估学习算法和bias-variance | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Machine Learning第六周笔记一：评估学习算法和bias-variance</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Machine Learning第六周笔记一：评估学习算法和bias-variance</h1><div class="post-meta">Apr 1, 2016<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/04/01/Machine-Learning第六周笔记一：评估学习算法和bias-variance/" href="/2016/04/01/Machine-Learning第六周笔记一：评估学习算法和bias-variance/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Evaluating-a-Learning-Algorithm"><span class="toc-number">1.</span> <span class="toc-text">Evaluating a Learning Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Deciding-What-to-Try-Next"><span class="toc-number">1.1.</span> <span class="toc-text">Deciding What to Try Next</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluatinng-a-Hypothesis"><span class="toc-number">1.2.</span> <span class="toc-text">Evaluatinng a Hypothesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Selection-and-Train-Validation-Test-Sets"><span class="toc-number">1.3.</span> <span class="toc-text">Model Selection and Train/Validation/Test Sets</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bias-vs-Variance"><span class="toc-number">2.</span> <span class="toc-text">Bias vs. Variance</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Diagnosing-Bias-vs-Variance"><span class="toc-number">2.1.</span> <span class="toc-text">Diagnosing Bias vs. Variance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularization-and-Bias-Variance"><span class="toc-number">2.2.</span> <span class="toc-text">Regularization and Bias/Variance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Curves"><span class="toc-number">2.3.</span> <span class="toc-text">Learning Curves</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deciding-What-to-Do-Next-Revisited"><span class="toc-number">2.4.</span> <span class="toc-text">Deciding What to Do Next Revisited</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-number">3.</span> <span class="toc-text">Reference</span></a></li></ol></div></div><div class="post-content"><p>入坑机器学习近一个月，学习资料主要是李航的《统计学习方法》、Peter Harrington的《机器学习实战》、Andrew Ng的<em>Machine Learning</em>和台大林田轩的<em>机器学习基石</em>。</p>
<p>只看视频不做笔记的学习效果实在太差，从Andrew Ng的Machine Learning的第六周开始，坚持笔记。</p>
<h1 id="Evaluating-a-Learning-Algorithm"><a href="#Evaluating-a-Learning-Algorithm" class="headerlink" title="Evaluating a Learning Algorithm"></a>Evaluating a Learning Algorithm</h1><h2 id="Deciding-What-to-Try-Next"><a href="#Deciding-What-to-Try-Next" class="headerlink" title="Deciding What to Try Next"></a>Deciding What to Try Next</h2><p>我们继续使用前面课程中提到的房价预测问题。假设我们实现了对房价进行预测的规范化后的线性回归，得到了模型参数。但当我们使用一组新的数据来测试得到的hypothesis时却得到了一个不能接受的错误率。为了提高模型的表现，我们应该采取什么措施呢？<br><a id="more"></a></p>
<ul>
<li>获取更多的训练实例(Get more training examples)</li>
<li>删减一些特征(Try smaller sets of features)</li>
<li>添加特征(Try getting additonal features)</li>
<li>添加多项式特征(Try adding polynomial features)</li>
<li>减小惩罚项$\lambda$(Try decreasing $\lambda$)</li>
<li>增大惩罚项$\lambda$(Try increasing $\lambda$)</li>
</ul>
<p>上面提到的几种是我们调整模型时常用到的方法。但是什么情况下采取哪种调整措施，人们通常是不清楚的。很多人在进行调整时采用了不恰当的措施，结果耗费了大量的时间却未能得到一个好的模型，事倍功半。下面我们就尝试着去探寻一下其中的规律。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__1.png" alt="6_1"></p>
<p>课程中引入了诊断（machine learning diagnostic）这一新概念。诊断是指这样一种测试——我们通过该测试来得知我们的学习算法是否可以很好的工作， 而该测试也能指导我们有针对性的去改善学习算法的性能。对学习算法进行诊断需要花费大量的时间，但这是非常值得的。诊断使我们能够准确且专业地了解算法的性能，在调整参数时就能够做到有法可依，有据可循，避免了瞎猜乱猜。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__2.png" alt="6_2"></p>
<h2 id="Evaluatinng-a-Hypothesis"><a href="#Evaluatinng-a-Hypothesis" class="headerlink" title="Evaluatinng a Hypothesis"></a>Evaluatinng a Hypothesis</h2><p>在使用一组training set来训练我们的参数时，我们需要调整参数以使得training set对应的cost function足够小。但由此得到的hypothesis并不一定就是一个好的hypothesis，它有可能会造成过拟合（overfitting），如下图表示。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__3.jpg" alt="6_3"></p>
<p>上图中的hypothesis是一个简单的一次多项式函数，我们可以通过画出其图像来直观的判断hypothesis的好坏。当问题涉及到大量的特征时（如上图右侧列出的那样），画出图像将是一件非常困难的事情，此时我们需要其他的方法来评估我们的hypothesis。</p>
<p>评估hypothesis的标准方法是这样的：假设我们有下图所示这样的一个data set（当然，现实中的data set的数据量要非常大），将原始的data set分成两部分，前一部分作为通常意义上的training set来训练hypothesis，后一部分则作为test set来检验hypothesis。通常，进行划分的比例为70/30（或者分给training set更多一些）。下面，我们使用m来表示training set的数据量，使用$m_{test}$来表示test set的数据量。需要注意的是，原始的data set中的数据可能遵循某种规律，我们在进行划分前最好现将顺序打乱(使用random函数等)，这样会收到更好的效果。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__4.jpg" alt="6_4"></p>
<p>以线性回归（linear regression）为例，我们来介绍典型的训练和测试步骤。首先，使用前面分割到的training set来训练模型，得到参数$\theta$。训练的过程一般是通过梯度下降（gradient descent）等方法来最小化损失函数（cost function $J(\theta)$）。然后按照下面的函数来计算test set的错误率$J_{test}(\theta)$：<br>    $$J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\theta}(x_{test}^{(i)})-y^{(i)})^2$$</p>
<p>上式中，我们使用平方误差度量（squared error metric）来计算线性回归的test set的错误率。换成逻辑回归（logistics regression）的话，计算公式如下：<br>    $$J_{test}(\theta)=-\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}y_{test}^{(i)}\log {h_{\theta}(x_{test}^{(i)})} + (1-y_{test}^{(i)})\log h_{\theta}(x_{test}^{(i)})$$</p>
<h2 id="Model-Selection-and-Train-Validation-Test-Sets"><a href="#Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="Model Selection and Train/Validation/Test Sets"></a>Model Selection and Train/Validation/Test Sets</h2><p>接着上一部分的内容，我们引入一个新的数据集——交叉验证数据集（cross valid set）。此时，我们将原始的data set按照6/2/2的比例分割成training set、cross valid set、test set三部分，使用m、$m_{cv}$、$m_{test}$三个变量表示三个数据集的数据量。具体分割如下图所示：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__5.png" alt="6_5"></p>
<p>对应的，计算错误率的公式也需要调整：<br>$$J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$$</p>
$$J_{cv}(\theta)=\frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_{\theta}(x_{cv}^{(i)})-y_{cv}^{(i)})^2$$
$$J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$$
<p>下面我们讨论选择模型的问题。选择合适的hypothesis使得模型“just fit”数据，既不“underfit”，也不“overfit”。<br>如下图所示，我们列出多个hypothesis供选择。我们使用前面的training set来训练，分别得到每个hypothesis的参数$\theta$，记为$\theta^{(1)}$，$\theta^{(2)}$，…，$\theta^{(10)}$。下一步，利用得到的参数计算每个hypothesis在cross valid set上的错误率：$J_{cv}(\theta^{(1)})$，$J_{cv}(\theta^{(2)})$，...，$J_{cv}(\theta^{(10)})$。选取$J_{cv}(\theta)$最小的一个hypothesis作为我们最终选取的hypothesis，计算其在test set上的错误率。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__6.png" alt="6_6"></p>
<p>当然，可供我们选择的hypothesis实在太多了，绝非上图给出的这10个。很自然地引出“如何缩小hypothesis的选择区域”这一问题，我们留在将来进行讨论。</p>
<h1 id="Bias-vs-Variance"><a href="#Bias-vs-Variance" class="headerlink" title="Bias vs. Variance"></a>Bias vs. Variance</h1><h2 id="Diagnosing-Bias-vs-Variance"><a href="#Diagnosing-Bias-vs-Variance" class="headerlink" title="Diagnosing Bias vs. Variance"></a>Diagnosing Bias vs. Variance</h2><p>当我们训练得到的hypothesis的性能达不到期望值时，通常只可能存在两种问题：high bias问题和high variance问题。换句话说，通常情况下只可能存在两种问题：欠拟合问题（underfitting problem）和过拟合问题（overfitting problem）。只有准确的判断我们的hypothesis存在哪种问题，才能更好地去修正和改善它。</p>
<p>我们将hypothesis的维度记作d，如$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$的维度是$d=4$。下图为同一组data set对应的不同hypothesis的图像:</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__7.png" alt="6_7"></p>
<p>我们通过图像来观察$J_{train}(\theta)$和$J_{cv}(\theta)$随d变化的变化情况，图像见下图。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__8.png" alt="6_8"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__9.png" alt="6_9"></p>
<p>由上图图像可知，d越小，其对应的hypothesis越简单，也就越有可能存在欠拟合问题；d越大，其对应的hypothesis越复杂，也就越可能存在过拟合问题。换句话说，当出现high bias problem时，$J_{train}(\theta)$和$J_{cv}(\theta)$都会达到一个很高的值；当出现high variance problem时，$J_{train}(\theta)$的值很小，而$J_{cv}(\theta)$的值很大。</p>
<h2 id="Regularization-and-Bias-Variance"><a href="#Regularization-and-Bias-Variance" class="headerlink" title="Regularization and Bias/Variance"></a>Regularization and Bias/Variance</h2><p>下面我们继续讨论正则化系数$\lambda$（the regularization coefficient $\lambda$）对bias-variance的影响。此时损失函数（cost function）更新如下：<br>    $$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(x)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^{m}\theta_j^2$$<br>如下图所示，给定hypothesis，为$\lambda$设置12个不同的取值。在每个$\lambda$的取值下，以损失函数最小化的原则分别求得参数$\theta^{(1)}$，$\theta^{(2)}$，...，$\theta^{(12)}$。然后计算$J_{cv}(\theta^{(1)})$，$J_{cv}(\theta^{(2)})$，...，$J_{cv}(\theta^{(12)})$。选出其中的最小值对应的参数$\theta$作为最终选定的hypothesis。最后计算我们选定的hypothesis在test set上的错误率，查看其性能。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__15.png" alt="6_15"></p>
<p>以上是我们选取正则化系数$\lambda$的过程，下面我们来看一下$J_{train}(\theta)$、$J_{cv}(\theta)$随$\lambda$变化的变化情况。图像如下图所示：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__16.png" alt="6_16"></p>
<hr>
<p>补充一点课外资料。</p>
<p>查阅了大名鼎鼎的<em>Pattern Recognization And Machine Learning</em>，我们找到了其中对bias-variance的介绍，摘抄如下：</p>
<blockquote>
<p>It is instructive to consider a frequentist viewpoint of the model complexity issue, known as the <em>bias-variance</em> trade-off.</p>
<p>As we shall see, there is a trade-off between bias and variance, with very flexible models having low bias and high variance, and relatively rigid models having high bias and low riance. The model with the optimal predictive capability is the one that leads to the best balance between bias and variance. </p>
</blockquote>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__10%20PRML.png" alt="6_10"></p>
<blockquote>
<p>The top row corresponds to a large value of the regularization coefficient $\lambda$ that gives low variance (because the red curves in the left plot looks similar) but high bias (because the two curves in the right plot are very different). Conversely, on the bottom row, for which $\lambda$ is small, there is large variance (shown by the high variability between the red curves in the left plot) but low bias (shown by the good fit between the average model fit and the original sinusoidal function). </p>
<p>We see that small values of $\lambda$ allow the model to become finely tuned to the noise on each individual data set leading to large variance. Conversely, a large value of $lambda$ pulls the weight parameters to towards zero leading to large bias.</p>
</blockquote>
<p>另外在网上找到了首尔大学Biointelligence Lab的PPT（<a href="http://wenku.baidu.com/link?url=347tMgS_3reBuRH1fvalMzZ3UCOriIor19hjDRfloc0s_E7rojR0bbqPIf8jfUP-4G6HX0xB37vvb6tFDJWCPuSRh_qpWZUMn4Zvxkt3Mr_" target="_blank" rel="external">PPT传送门</a>），将介绍bisa-variance的基业slide截图粘贴在下面。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__11%20PRML.jpg" alt=""></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__12%20PRML.jpg" alt=""></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__13%20PRML.png" alt=""></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__14%20PRML.jpg" alt=""></p>
<p>再贴一个知乎上搜来的答案节选，全部答案见<a href="https://www.zhihu.com/question/27068705/answer/35151681" target="_blank" rel="external">orange princle在“机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？”下的回答</a>：</p>
<blockquote>
<p>首先 $Error = Bias + Variance$</p>
<p>Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。</p>
<p>举一个例子，一次打靶实验，目标是为了打到10环，但是实际上只打到了7环，那么这里面的Error就是3。具体分析打到7环的原因，可能有两方面：一是瞄准出了问题，比如实际上射击瞄准的是9环而不是10环；二是枪本身的稳定性有问题，虽然瞄准的是9环，但是只打到了7环。那么在上面一次射击实验中，Bias就是1,反应的是模型期望与真实目标的差距，而在这次试验中，由于Variance所带来的误差就是2，即虽然瞄准的是9环，但由于本身模型缺乏稳定性，造成了实际结果与模型期望之间的差距。</p>
<p>在一个实际系统中，Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。造成这种现象的根本原因是，我们总是希望试图用有限训练样本去估计无限的真实数据。当我们更加相信这些数据的真实性，而忽视对模型的先验知识，就会尽量保证模型在训练样本上的准确度，这样可以减少模型的Bias。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果更加相信我们对于模型的先验知识，在学习模型的过程中对模型增加更多的限制，就可以降低模型的variance，提高模型的稳定性，但也会使模型的Bias增大。Bias与Variance两者之间的trade-off是机器学习的基本主题之一，机会可以在各种机器模型中发现它的影子。</p>
</blockquote>
<h2 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title="Learning Curves"></a>Learning Curves</h2>观察学习曲线（learning curve）是一个评价hypothesis的好方法。给定一个hypothesis，我们观察损失函数$J_{train}(\theta)$、$J_{cv}(\theta)$随着训练样本大小（training set size）m的变化是怎样变化的。如下图所示，给定$h_{\theta}(x)=\theta_0+\theta_1x+\theta_2x^2，$随着m的增大，$J_{train}(\theta)$逐渐增大，$J_{cv}(\theta)$逐渐减小。很好理解，当m过小时，hypothesis几乎可以恰好地拟合training set，因此$J_{train}(\theta)$很小，此时hypothesis通常是overfitting的，所以$J_{cv}(\theta)$的值偏高；随着m的增大，hypothesis能够从training set中学习到更多的信息，能够更好的预测未知数据，因此$J_{cv}(\theta)$逐渐减小，而$J_{train}(\theta)$变大；当m达到一定的值时，hypothesis已经不能拟合所有的训练样本，因此$J_{train}(\theta)$变得很大，此时hypothesis逐渐出现了underfitting problem，而$J_{cv}(\theta)$也只能维持一定的水平而不能继续下降。
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__17.png" alt="6_17"></p>
<p>在出现high bias problem/high variance problem的情况下，我们看一下损失函数$J<em>{train}(\theta)$、$J</em>{cv}(\theta)$随着训练样本大小（training set size）m的变化是怎样变化的。图像见下面两张图。由下图可知，当出现high bias problem时，靠增大training set不太可能改善hypothesis的性能；由第二图知，当出现high variance problem时，增大training set应该是改善hypothesis的一个好方法。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__18.png" alt="6_18"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/MLAnNote__6__19.png" alt="6_19"></p>
<h2 id="Deciding-What-to-Do-Next-Revisited"><a href="#Deciding-What-to-Do-Next-Revisited" class="headerlink" title="Deciding What to Do Next Revisited"></a>Deciding What to Do Next Revisited</h2><p>了解了bias/variance的概念，我们就知道博文最开始列出的几种措施分别是用来解决什么问题的了。</p>
<ul>
<li>Get more training examples       —–&gt;   fix high variance</li>
<li>Try smaller sets of features     —–&gt;   fix high variance</li>
<li>Try getting additonal features   —–&gt;   fix high bias</li>
<li>Try adding polynomial features   —–&gt;   fix high bias</li>
<li>Try decreasing $\lambda$         —–&gt;   fix high bias</li>
<li>Try increasing $\lambda$         —–&gt;   fix high variance</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>写作内容均来自Cousera上Andrew Ng的<em>Machine Learning</em>课程，详见<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="external">传送门</a></p>
<p>写作过程中参考了以下：</p>
<ul>
<li>学霸女神<a href="http://weibo.com/u/2607574543?is_all=1" target="_blank" rel="external">Rachel____Zhang</a>的CSDN博客，见<a href="http://blog.csdn.net/abcjennifer/article/details/7797502" target="_blank" rel="external">传送门</a></li>
<li><em>Pattern Recognition And Machine Learning</em>, P147-152</li>
<li><a href="https://www.zhihu.com/question/27068705/answer/35151681" target="_blank" rel="external">orange princle在“机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？”下的回答</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/04/01/Machine-Learning第六周笔记一：评估学习算法和bias-variance/" data-id="cjfguskdl0009cgurlz6p1i5m" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/机器学习/">机器学习</a></div><div class="post-nav"><a href="/2016/04/03/Machine-Learning第六周笔记二：机器学习系统设计/" class="pre">Machine Learning第六周笔记二：机器学习系统设计</a><a href="/2016/03/27/机器学习基石作业一/" class="next">机器学习基石作业一</a></div><div data-thread-key="2016/04/01/Machine-Learning第六周笔记一：评估学习算法和bias-variance/" data-title="Machine Learning第六周笔记一：评估学习算法和bias-variance" data-url="http://marcovaldong.github.io/2016/04/01/Machine-Learning第六周笔记一：评估学习算法和bias-variance/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/04/01/Machine-Learning第六周笔记一：评估学习算法和bias-variance/" data-title="Machine Learning第六周笔记一：评估学习算法和bias-variance" data-url="http://marcovaldong.github.io/2016/04/01/Machine-Learning第六周笔记一：评估学习算法和bias-variance/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>