<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation</h1><div class="post-meta">May 5, 2016<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a><a href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-the-weights-of-a-linear-neuron"><span class="toc-number">1.</span> <span class="toc-text">Learning the weights of a linear neuron</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-error-surface-for-a-linear-neuron"><span class="toc-number">2.</span> <span class="toc-text">The error surface for a linear neuron</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-the-weights-of-a-logistic-output-neuron"><span class="toc-number">3.</span> <span class="toc-text">Learning the weights of a logistic output neuron</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-backpropagation-algorithm"><span class="toc-number">4.</span> <span class="toc-text">The backpropagation algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-the-derivatives-computed-by-backpropagation"><span class="toc-number">5.</span> <span class="toc-text">Using the derivatives computed by backpropagation</span></a></li></ol></div></div><div class="post-content"><p>Geoffrey Hinton教授的<em>Neuron Networks for Machine Learning</em>的第三讲主要介绍了线性/逻辑神经网络和BackPropagation，下面是整理的笔记。</p>
<h2 id="Learning-the-weights-of-a-linear-neuron"><a href="#Learning-the-weights-of-a-linear-neuron" class="headerlink" title="Learning the weights of a linear neuron"></a>Learning the weights of a linear neuron</h2><p>这一小节介绍线性神经网络的学习算法。线性神经网络很像感知机，但又有不同：在感知机中，权值向量总是越来越接近好的权值设定；在线性神经网络中，输出总是越来越接近目标输出。在感知机中，每一次更新权值向量，其就更接近每一个“一般可行”的权值向量，这限制了感知机不能应用于更加复杂的网络，因为两个好的权值向量的平均可能是一个坏的。故在多层神经网络中，我们不能使用感知机的学习流程，也不能使用类似的方法来证明学习的可行性。</p>
<p>在多层神经网络中，我们通过判断实际输出是否越来越接近目标输出来判断学习的性能是否在提高。这一策略在解决非凸问题时仍然奏效，但不适合应用于感知机的学习。最简单的例子是使用平方误差的线性神经网络（linear neurons），也称为线性过滤器（linear filter）。如下图所示，y是神经网络对期望输出的一个估计，w是权值向量，x是输入向量，学习的目标是最小化在所有训练样本上犯的错误之和。<br><a id="more"></a><br><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__1.png" alt="3_1"></p>
<p>这里很直接就可以写出一系列的等式，每一个等式对应一个训练样本，从中解出最优的权值向量。这是标准的工程方法，为什么不使用它呢？一方面，我们想要得到一个真正的神经网络可以使用的方法；另一方法，我们又希望最终的方法可以一般化的应用于多层神经网络、非线性神经网络。分析的方法比较依赖于已有的线性特性和平方误差的衡量方法，而迭代的方法则更容易一般化更加复杂的模型上去。</p>
<p>下面使用一个简单的例子来介绍和证明迭代方法。假设你每天都去自助餐厅吃午饭，你点了鱼（fish）、薯条（chips）、番茄酱（ketchup），每天点的东西一样但量不同，服务员仅告诉你每天的价格。吃了几天后，你应该就可以推断出每个菜的价格了。首先，你随机的给出一组菜价，然后使用迭代的方法来不断修正菜价，直至得到一个恰当的菜价。下面给出了计算公式：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__2.png" alt="3_2"></p>
<p>下图给出了一顿午餐每个菜的份额和花费，是已知的；图中红色的数字是每个菜的实际单价，是未知的，也就是我们要求的。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__3.png" alt="3_3"></p>
<p>我们首先随机的猜出一组菜价(50, 50, 50)，然后计算整顿饭的价格，如下图所示，可以看到误差为350。然后给出了“delta-rule”，用来调整权值向量，其中的$\epsilon$是参数。当参数$\epsilon$取值$1/35$时，算出了每个权值的变化分别是+20，+50，+30，从而得到了新的权值向量(70, 100, 80)。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__4.png" alt="3_4"></p>
<p>下图给出了delta-rule：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__5.png" alt="3_5"></p>
<p>其实这里讲的还是感知机，我们在Andrew Ng的课程里已经学过了。通过迭代得到的权值向量可能不是perfect的，但应该是使得误差足够小的一个解。如果学习步长足够小，学习时间足够长，那最终得到的权值向量应该足够接近最优解。</p>
<p>下图给出了online delta-rule和perceptron的联系。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__6.png" alt="3_6"></p>
<h2 id="The-error-surface-for-a-linear-neuron"><a href="#The-error-surface-for-a-linear-neuron" class="headerlink" title="The error surface for a linear neuron"></a>The error surface for a linear neuron</h2><p>这一小节我们来观察误差曲面，从而理解线性神经网络是如何学习的。下图给出了一个二维权值向量对应的误差曲面（其实，这里给出的是两个截面，第一个图是纵截面，第二个图是横截面），整个曲面是一个碗装。对于多层或者非线性神经网络，其误差曲面将更加复杂。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__7.png" alt="3_7"></p>
<p>这里给出了在线学习和批量学习分别对应的收敛过程示意图，关于二者的更多内容参见<a href="http://blog.csdn.net/majordong100/article/details/51150623" target="_blank" rel="external"><em>Machine Learning第十周笔记：大规模机器学习</em></a>，这里不再展开。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__8.png" alt="3_8"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__9.png" alt="3_9"></p>
<h2 id="Learning-the-weights-of-a-logistic-output-neuron"><a href="#Learning-the-weights-of-a-logistic-output-neuron" class="headerlink" title="Learning the weights of a logistic output neuron"></a>Learning the weights of a logistic output neuron</h2><p>这一小节介绍逻辑神经网络（logistic neurons），这里仅作作图，不再展开，因为这里的逻辑神经网络其实就是逻辑回归算法。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__10.png" alt="3_10"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__11.png" alt="3_11"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__12.png" alt="3_12"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__13.png" alt="3_13"></p>
<h2 id="The-backpropagation-algorithm"><a href="#The-backpropagation-algorithm" class="headerlink" title="The backpropagation algorithm"></a>The backpropagation algorithm</h2><p>这一小节介绍back propagation算法，其在多层神经网络的训练中是一个举足轻重的算法。前面已经学习了逻辑回归，现在我们来看一下如何学习到隐匿单元的权值。添加了隐匿单元的神经会变得更加强大，但通过人工提取特征的方式来添加隐匿单元实在太困难了，我们希望能有一种方法来代替人工提取特征的过程。人工提取特征通常是猜一些特征，然后重复进行跟踪、计算犯错率、修正的一系列循环，我们希望让机器来实现这一循环。</p>
<p>我们通常是随机扰动一个权值，然后观察是否会提高学习的性能。若是的话，则保留该扰动。这可以看做是增强式学习（reinforcement learning）的一种形式。一个一个去扰动权值的方法效率实在太低了，这其中可能包含了许多无用功，而back propagation则好多了。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__14.png" alt="3_14"></p>
<p>我们可以同时扰动所有的权值，然后衡量权值的改变是否带来了性能的提升，但这种方法需要我们做大量工作，跟踪模型在每一个训练样本上的影响。一个更好的idea是随机的扰动隐匿单元，一旦我们知道隐匿单元基于给定训练样本需要作出什么样的改变时，我们就可以计算出如何修正权值。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__15.png" alt="3_15"></p>
<p>back propagation背后的思想是，我们不必了解隐匿单元具体应该做什么，但我们可以计算随着隐匿单元的活动误差会发生怎样的变化，可以在同一时间求得所有隐匿单元的error derivatives。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__16.png" alt="3_16"></p>
<p>下面几张图给出了backpropagation的一个简述，这里运用了链式法则。想深入了解backpropagation请阅读知乎上各位大神的解释：<a href="https://www.zhihu.com/question/27239198" target="_blank" rel="external">如何直观的解释back propagation算法？</a></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__17.png" alt="3_17"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__18.png" alt="3_18"></p>
<h2 id="Using-the-derivatives-computed-by-backpropagation"><a href="#Using-the-derivatives-computed-by-backpropagation" class="headerlink" title="Using the derivatives computed by backpropagation"></a>Using the derivatives computed by backpropagation</h2><p>弄清楚如何求得多层网络中所有权值的误差导数是学习到有效的神经网络的关键。在得到一个实际的学习流程之前我们还有很多问题有待解决，例如我们需要确定权值的更新频率。再或者，我们需要搞清楚如何来防止过拟合现象。</p>
<p>对于单个训练样本，back propagation算法是计算出每一个权值的导数的有效方法，但其还不是一个完整的算法，我们还需要给出算法的各个细节。例如，最优化问题和一般化问题，这两个问题将在第六讲和第七讲展开，这里做一个简要概述。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__19.png" alt="3_19"></p>
<p>下面是最优化问题（optimization issue）的一个简述，其中关于online vs batch可以参见<a href="http://blog.csdn.net/majordong100/article/details/51150623" target="_blank" rel="external"><em>Machine Learning第十周笔记：大规模机器学习</em></a></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__20.png" alt="3_20"></p>
<p>训练数据中包含了从输入到输出这一映射的规律性等信息，但它同时还包含了下面的两种噪声：</p>
<ul>
<li>The target values may be unreliable (usually only a minor worry).</li>
<li>There is sampling error. There will be accidental regularities just because of the particular training cases that were chosen. </li>
</ul>
<p>在去拟合模型时，我们无法分辨哪些规律性是真实的那些是由样本误差造成的，模型同时拟合了这两种规律性。如果模型适应性很强，很好得拟合了样本误差，那对模型则是一个灾难。下图给出了过拟合的一个简单例子。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__3__21.png" alt="3_21"></p>
<p>下面列出了避免过拟合的几种方法，这些方法将在第七讲中展开。</p>
<ul>
<li>Weight-decay</li>
<li>Weight-sharing</li>
<li>Early stopping</li>
<li>Model avraging</li>
<li>Bayesian fitting of neural nets</li>
<li>Dropout</li>
<li>Generative pre-training</li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" data-id="cjfguskle001vcgurp181rjrq" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Neural-Network/">Neural Network</a></div><div class="post-nav"><a href="/2016/05/08/机器学习中使用的神经网络第四讲/" class="pre">机器学习中使用的神经网络第四讲</a><a href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" class="next">机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机</a></div><div data-thread-key="2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" data-title="机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation" data-url="http://marcovaldong.github.io/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" data-title="机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation" data-url="http://marcovaldong.github.io/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>