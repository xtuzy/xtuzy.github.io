<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>机器学习中使用的神经网络第九讲 | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习中使用的神经网络第九讲</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习中使用的神经网络第九讲</h1><div class="post-meta">May 21, 2016<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a><a href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/05/21/机器学习中使用的神经网络第九讲/" href="/2016/05/21/机器学习中使用的神经网络第九讲/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview-of-ways-to-improve-generalization"><span class="toc-number">1.</span> <span class="toc-text">Overview of ways to improve generalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Limiting-the-size-of-the-weights"><span class="toc-number">2.</span> <span class="toc-text">Limiting the size of the weights</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-noise-as-a-regularizer"><span class="toc-number">3.</span> <span class="toc-text">Using noise as a regularizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-to-the-full-Bayesian"><span class="toc-number">4.</span> <span class="toc-text">Introduction to the full Bayesian</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Bayesian-interpretation-of-weight-decay"><span class="toc-number">5.</span> <span class="toc-text">The Bayesian interpretation of weight decay</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MacKay’s-quick-and-dirty-method-of-setting-weight-costs"><span class="toc-number">6.</span> <span class="toc-text">MacKay’s quick and dirty method of setting weight costs</span></a></li></ol></div></div><div class="post-content"><p>Geoffery Hinton教授的<em>Neuron Networks for Machine Learning</em>的第八讲为可选部分，好像很难，这里就先跳过了，以后有用的时候再回来补。第九讲介绍了如何避免过拟合，提高模型的泛化能力。</p>
<p>这是Cousera上的课程<a href="https://www.coursera.org/course/neuralnets" target="_blank" rel="external">链接</a></p>
<h2 id="Overview-of-ways-to-improve-generalization"><a href="#Overview-of-ways-to-improve-generalization" class="headerlink" title="Overview of ways to improve generalization"></a>Overview of ways to improve generalization</h2><p>这一小节，我们介绍如何在网络有过多能力处理过多容量的训练数据集时，如何通过降低过拟合来提高网络模型的泛化能力。下面将介绍几种控制网络容量的方法，以及如何设置度量参数。下图回顾了什么是过拟合（overfitting），由于是以前讲过的东西，这里只贴截图，不再文字叙述。<br><a id="more"></a><br><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__1.png" alt="9_1"></p>
<p>下图列出了防止过拟合的四种方法，其中第二种方法——适当调节网络的能力（regulate the capacity appropriately）——会在本讲中重点展开，而后面的两种会在以后的课程中介绍。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__2.png" alt="9_2"></p>
<p>我们通常通过以下四种方式或它们的组合来实现对网络能力的控制。</p>
<ul>
<li>Architecture: Limit the number of hidden layers and the number of units per layer.</li>
<li>Early stopping: Start with small weights and stop the learning before it overfits.</li>
<li>Weight-decay: Penalize large weights using penalties or constraints on their squared values (L2 penalty) or absolute values (L1 penalty).</li>
<li>Noise: Add noise to the weights or the activities.</li>
</ul>
<p>在使用这些方法时，我们需要设置一些元参数（meta parameters），如隐含单元的数量、层数、惩罚权重等。我们可能会用一些可能的值来设置元参数，然后通过测试数据集筛选出最好的那组参数。但这种方法得来的参数具有对测试集的针对性，再换一组测试数据集可能性能就不佳了。例如一个极端的例子：假设有一个测试数据集，其中的输出都不是通过输入得来的，而是随机标记的。那么由此测试集筛选得来的元参数肯定是经不起别的测试集的检验的。</p>
<p>一个更好的方法是将整个数据集分成训练集、验证集、测试集三部分，训练集用来训练模型的参数，验证集用来筛选出性能最好的参数，测试集用来得到模型性能的无偏估计。这个模型性能的无偏估计肯定比验证集上的性能要低，原因同上面试一个道理。为了尽可能降低模型过拟合验证集的可能性，我们将整个数据集分成同等大小的N+1份，一份作最终的测试集，另外N份（记为$S_1$到$S_N$）用来作训练集和验证集：每次选择其中一份作验证集$S_i$，其余N-1份作训练集，然后计算出在验证集上的错误率$E_i$。这里$S_i$从$S_1$遍历到$S_N$，从而得到N个错误率，然后以这N个错误率的平均为衡量指标来选择参数，再用最终的测试集得到模型性能的无偏估计。这个方法称之为N-fold交叉验证（N-fold Cross Validation），我的叙述不是特别清楚，这里给出一个链接：<a href="http://mp.weixin.qq.com/s?__biz=MzIzODExMDE5MA==&amp;mid=400415610&amp;idx=1&amp;sn=381dd64fbdc7c130a6314fb65815d4e1#rd" target="_blank" rel="external">交叉验证简介</a>。</p>
<p>需要注意的是，N-fold交叉验证得到的N个估计（就是错误率）不是相互独立的。一个很极端的例子是，如果恰巧有一个子集里边都只包含一种类别的数据，那无论该子集做训练集还是验证集，最终的结果其泛化能力肯定很差。</p>
<p>对于大规模的数据集和大的模型，反复训练拥有不同元参数的模型其代价是极其高昂的。代价低的一种做法是，训练之初设置比较小的参数，然后随着模型的训练逐渐增大参数，直至模型在交叉验证集的表现开始变坏。但是如何衡量表现开始变坏是很困难的，所以我们可以在确定表现已经变坏了之后停止训练，然后再返回去找出表现最好的那个点。</p>
<p>Hinton还说，由于权值没有时间增大，模型的capacity被限制住了。下面解释小的权重能够限制capacity的原因。</p>
<p>考虑入下图所示的一个神经网络，即时隐含单元使用逻辑单元，小的权值参数会使得这些单元的输入很接近零，那输出也就落在了逻辑曲线中间的接近线性的那一段上。也就是说，晓得权值参数使得逻辑隐含单元的表现很像线性单元，那整个网络也就很接近一个将输入直接映射到输出的线性神经网络。随着权值参数的增大，隐含单元恢复了逻辑单元的capacity，模型对训练集的拟合能力开始逐渐增强。而最终得到的模型对验证集的拟合程度先增大后减小，当拟合程度开始减小的时候应该就是停止训练的时候。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__3.png" alt="9_3"></p>
<h2 id="Limiting-the-size-of-the-weights"><a href="#Limiting-the-size-of-the-weights" class="headerlink" title="Limiting the size of the weights"></a>Limiting the size of the weights</h2><p>这一小节介绍通过通过限制权值的size来控制网络的capacity，标准的方法是引入一个惩罚项来防止权值变得过大。伴随着一些隐含的假设存在，有着小权值的神经网络要比大权值的简单很多。我们可以使用几种不同的方法来限制权值的size，使得传入隐含单元的权值向量不会超过某个确定的长度。</p>
<p>标准的方法是使用L2权值惩罚项来限制权值的size，其是指在损失函数上加上权值的平方作为惩罚项。在神经网络中，L2有时被称之为权值衰减，因为该惩罚项的导数一直限制着权值变大。下图给出了损失函数的公式，其中权值平方和的系数$\lambda$称之为权值损失（weight cost），其决定的惩罚的强弱。下图中给出了当损失函数导数为零时权值的取值，该取值其实就是权值可以取得最大值。（再大的话，损失函数又开始上升了）</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__4.png" alt="9_4"></p>
<p>下图列出了L2权值损失的作用。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__5.png" alt="9_5"></p>
<p>下图给出了L1权值惩罚项，该惩罚项是权值的绝对值，图像是V形的，见下图。L1权值惩罚项一个很好的作用是使得很多权值接近于零，方便我们理解神经网络中到底发生了什么（我们只需要注意少数几个不接近零的权值即可）。有时我们还会使用使某几个权值一直取较大值的惩罚项。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__6.png" alt="9_6"></p>
<p>除了引入权值惩罚项，我们还可以引入权值约束，例如对每一单元的输入权值向量，我们可以约束其平方和的最大值不得超过某一个上限。下图列举了权值约束的优点。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__7.png" alt="9_7"></p>
<h2 id="Using-noise-as-a-regularizer"><a href="#Using-noise-as-a-regularizer" class="headerlink" title="Using noise as a regularizer"></a>Using noise as a regularizer</h2><p>这一小节介绍另一种限制网络capacity的方法——使用噪声正则化（using noise as regularizer）。我们可以将噪声添加到权值上或者activities（也就是单元）上，从而达到限制网络capacity的目的，防止出现过拟合。</p>
<p>假设我们在输入中加入高斯噪声（Gaussian noise），那么在进入下一层之前噪声的方差已经被平方权值放大。如下图所示，在一个简单的网络中，输出与输入线性相关，被放大的噪声也被添加到了输出中，同时也就影响了（增大了）平方误差。所以当输入有噪声的时候，最小化平方误差其实也是在最小化权值的平方和。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__8.png" alt="9_8"></p>
<p>下图给出了一个数学推导，不过想不明白这里为什么把平方和展开式的中间项省掉了。根据推导，我们可以看到在输入中添加噪声实际上就等同于添加了一个权值惩罚项。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__9.png" alt="9_9"></p>
<p>在更复杂的网络中对权值添加高斯噪声，虽不完全等同于添加权值惩罚项，但表现更好，尤其是在循环神经网络中。Alex Grave在其用于手写识别的循环神经网络中加入了噪声，结果证明性能有明显改善。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__10.png" alt="9_10"></p>
<p>我们还可以在activities中使用噪声来作为正则化（using noise in the activities as a regularizer）。大概意思是说，对于使用逻辑函数的隐含单元，其输出肯定介于0和1之间，现在我们在前向中用一个二进制函数来代替隐含单元中的逻辑函数，随机的输出0或者1，计算出输出。然后在反向中，我们再使用正确的方法去做修正。由此得来的模型可能在训练集上的表现变差，且训练速度变慢，但其在测试集上的表现有着显著提高。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__11.png" alt="9_11"></p>
<h2 id="Introduction-to-the-full-Bayesian"><a href="#Introduction-to-the-full-Bayesian" class="headerlink" title="Introduction to the full Bayesian"></a>Introduction to the full Bayesian</h2><p>这一小节我们通过一个简单的投硬币的例子，来介绍贝叶斯方法（bayesian approach）。贝叶斯方法的主要思想不是直接去寻找模型最可能的参数设置，而是考虑所有可能的参数设置，并根据已有数据来得出每种参数设置的可能性有多大。下图给出了贝叶斯框架：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__12.png" alt="9_12"></p>
<p>这里用抛硬币的例子来介绍贝叶斯方法，下图是一些假设。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__13.png" alt="9_13"></p>
<p>上面假设每一次抛硬币时抛出人头的概率为p，且上图给出了一组特定的结果———53次人头，47次国徽，那么我们就可以计算出出现这个结果的概率为$P(D)=p^{53}(1-p)^{47}$。现在我们说，如何最大化P(D)，也就是如何使发生这种结果的概率最大呢？可以看到P(D)只与p有关，下图给出了求导的过程，得到了当p=0.53时P(D)取到最大值。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__14.png" alt="9_14"></p>
<p>但是通过一个例子来求p肯定不合理的，例如只抛一次，抛出了人头，那我们就说p为1，那当然是不合理的。说0.53比1要合理，是因为我们已经有了一个先验的概率分布（p应该是0.5）。</p>
<p>下面的东西没听太明白，这里先只贴图。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__15.png" alt="9_15"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__16.png" alt="9_16"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__17.png" alt="9_17"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__18.png" alt="9_18"></p>
<p>总的来说，完全贝叶斯方法与最大似然估计（maximum likelihood estimation）不同，其是根据一个先验概率去求出一个后验概率（在已有观察序列，即训练数据下选择参数w的一个概率）。</p>
<h2 id="The-Bayesian-interpretation-of-weight-decay"><a href="#The-Bayesian-interpretation-of-weight-decay" class="headerlink" title="The Bayesian interpretation of weight decay"></a>The Bayesian interpretation of weight decay</h2><p>这一小节，我们介绍权值惩罚项的贝叶斯解释。在完全贝叶斯方法中，我们尝试去计算一个模型所有可能参数的后验概率。而我们要用贝叶斯方法找的其实是这样一组参数，其能够在先验概率（即我们先验的认为参数应该是什么样的）和已有的观察之间做一个最好的折中。该方法叫做Maximum alpha Posteriori learning，当我们用权衰减（weight decay）来控制模型的capacity时，其能够很好的解释发生了什么。</p>
<p>下面我们来讨论一下，在监督式最大似然学习（supervised maximum likelihood learning）中当我们去最小化平方误差时，发生了什么。找到一个权值向量能够最小化平方误差就等价于找到一个权值向量能够最大化正确答案的概率密度。为了看到这种等价性，我们假定正确的答案可以由神经网络的输出加上高斯噪声得来。也就是说，我们首先由神经网络得到输出，然后在输出上加上一些高斯噪声，然后问我们现在得到的东西就是正确答案的可能性有多大。下图给出了一个模型输出与目标输出的一个关系：图中曲线是一条高斯分布曲线，模型输出是这条曲线的中心位置，我们要的就是让目标输出的概率密度足够大。（个人理解，欢迎指正。）</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__19.png" alt="9_19"></p>
<p>然后就有了下面的数学公式。$input_c$是已知输入，$y_c$是由$input_c$和参数$w$得来的输出，$t_c$是目标输出。然后下面是已知神经网络输出加上高斯噪声求目标输出的概率密度的一个公式，再对公式变一下形（两边取log对数，再加上一个负号），就得到了最后的等式。根据最后这个等式，我们就得到了前面的那个等价性：当我们最小化平方误差时，其实我们就是在最小化等式的右端，也就是在最小化等式的左端，那也就是在最大化$logp(t_c|y_c)$，就是在最大化正确答案的一个概率密度。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__20.png" alt="9_20"></p>
<p>贝叶斯方法是找到所有可能的权值向量的后验概率分布，但在实际中要得到一个非线性网络众多权值向量的后验概率相当困难，常用的是使用蒙特·卡罗方法（Monte Carlo methods）来得到一个近似分布。现在简单一点，我们只找出最可能的那个权值向量：从一些随机的权值向量出发，在提高P(W|D)的方向上改进权值向量，最终从中选出一个最好的。不过这样得来的是一个局部最优。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__21.png" alt="9_21"></p>
<p>下图给出了求解maximum sums of log probabilities的原因，我的理解是我们要求的是在给定训练数据上，模型的输出是目标输出的可能的最大值。这里假定在不同训练样本上的输出错误时相互独立。然后，为了计算上的方便，我们使用对数来计算，因为对数函数是单调的。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__22.png" alt="9_22"></p>
<p>下图是最大后验概率学习（maximum a posteriori learning）的损失函数。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__23.png" alt="9_23"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__24.png" alt="9_24"></p>
<p>最后我们得到了队权衰减（weight decay）的一个贝叶斯解释，如下图所示。最终，我们得到了一个和前面讲权值惩罚项时类似的损失函数。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__25.png" alt="9_25"></p>
<h2 id="MacKay’s-quick-and-dirty-method-of-setting-weight-costs"><a href="#MacKay’s-quick-and-dirty-method-of-setting-weight-costs" class="headerlink" title="MacKay’s quick and dirty method of setting weight costs"></a>MacKay’s quick and dirty method of setting weight costs</h2><p>这一小节，我们介绍由<a href="http://www.inference.phy.cam.ac.uk/mackay/" target="_blank" rel="external">David Mackay</a>发明的一种不使用交叉验证集就能确定权值惩罚项的方法。下面是几张截图。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__26.png" alt="9_26"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__27.png" alt="9_27"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__9__28.png" alt="9_28"></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/05/21/机器学习中使用的神经网络第九讲/" data-id="cjfguskm1001ycgurtxtswltv" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Neural-Network/">Neural Network</a></div><div class="post-nav"><a href="/2016/05/30/机器学习中使用的神经网络第十讲/" class="pre">机器学习中使用的神经网络第十讲</a><a href="/2016/05/18/机器学习中使用的神经网络第七讲/" class="next">机器学习中使用的神经网络第七讲</a></div><div data-thread-key="2016/05/21/机器学习中使用的神经网络第九讲/" data-title="机器学习中使用的神经网络第九讲" data-url="http://marcovaldong.github.io/2016/05/21/机器学习中使用的神经网络第九讲/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/05/21/机器学习中使用的神经网络第九讲/" data-title="机器学习中使用的神经网络第九讲" data-url="http://marcovaldong.github.io/2016/05/21/机器学习中使用的神经网络第九讲/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>