<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机 | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机</h1><div class="post-meta">May 4, 2016<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a><a href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" href="/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Types-of-nerual-network-architectures"><span class="toc-number">1.</span> <span class="toc-text">Types of nerual network architectures</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Perceptrons-The-first-generation-of-nerual-networks"><span class="toc-number">2.</span> <span class="toc-text">Perceptrons: The first generation of nerual networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-geometrical-view-of-perceptrons"><span class="toc-number">3.</span> <span class="toc-text">A geometrical view of perceptrons</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-the-learning-works"><span class="toc-number">4.</span> <span class="toc-text">Why the learning works</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-perceptrons-can’t-do"><span class="toc-number">5.</span> <span class="toc-text">What perceptrons can’t do</span></a></li></ol></div></div><div class="post-content"><p>最近在Cousera上学习多伦多大学Geoffrey Hinton教授的<em>Nerual Networks for Machine Learning</em>，为保证学习效果，特整理了学习笔记，一方面加深理解，一方面试图将学到的东西讲清楚。</p>
<p>这一讲主要介绍神经网络的结构。</p>
<h2 id="Types-of-nerual-network-architectures"><a href="#Types-of-nerual-network-architectures" class="headerlink" title="Types of nerual network architectures"></a>Types of nerual network architectures</h2><p>这一小节介绍了三种不同的神经网络结构。</p>
<p>首先介绍向前反馈网络（feed forward network），其常见形式如下图所示，第一层是输入（input layer），最后一层是输出（output layer），中间是一层或多层隐匿单元（hidden layer，被称之为deep nerual network）。<br><a id="more"></a><br><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__1.png" alt="2_1"></p>
<p>循环神经网络（recurrent nerual network）要更强大一些，里面加入了有向环（directed cycle）。下图给出了循环神经网络的示意图，列出了其主要特点。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__2.png" alt="2_2"></p>
<p>循环神经网络是模拟序列数据的一种非常自然的方法。隐匿单元之间有着联系，它们在时间上的表现恰似很深的网络，在每一个时间步长上隐匿单元的状态决定了下一个时间步长上隐匿单元的状态。循环神经网络与向前反馈网络的不同之处在于其每一个隐匿单元对应的权重矩阵（weight matrix）都是相同的，且在每一个时间片上都有输入。另外，循环神经网络能够在很长一段时间内在隐匿单元上保存信息。只是我们很难有效利用这一潜能，不过近来的一些算法已经能够做到这一点。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__3.png" alt="2_3"></p>
<p>对称连接网络（symmetrically connected networks），其很像循环网络，但单元之间的连接是对称的。对称连接网络与循环网络相比易于分析，其遵循一个energy function，因此应用范围有限。另外，没有隐匿单元的对称连接网络被称为hopfield net。</p>
<h2 id="Perceptrons-The-first-generation-of-nerual-networks"><a href="#Perceptrons-The-first-generation-of-nerual-networks" class="headerlink" title="Perceptrons: The first generation of nerual networks"></a>Perceptrons: The first generation of nerual networks</h2><p>这一小节介绍感知机（perceptron），下图给出了典型的感知机的工作流程和结构。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__4.png" alt="2_4"></p>
<p>下图给出了感知机的一个发展历程。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__5.png" alt="2_5"></p>
<p>下面介绍了二元阈值神经网络（binary threshold neurons），其实就是感知机。博主已经学过了感知机，所以这里只作截图，不再展开。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__6.png" alt="2_6"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__7.png" alt="2_7"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__8.png" alt="2_8"></p>
<h2 id="A-geometrical-view-of-perceptrons"><a href="#A-geometrical-view-of-perceptrons" class="headerlink" title="A geometrical view of perceptrons"></a>A geometrical view of perceptrons</h2><p>这一小节从几何角度来分析感知机是如何学习的。首先，我们要知道什么是权空间（weight space），其是一个高维空间，其中的每一个点代表权值的一组特殊设定。我们用平面来代表训练样本，那么学习就是要找到一个权值向量能够能处于所有平面正确的一侧。注意，这里提到的平面往往是超平面。在权空间（weight space）中，感知机的每一个权值都有一个维度，权空间的每一个点都代表权值的一个特殊设定。假定我们不考虑阈值（threshold），那可以将每一个训练样本视作权值空间中的一个过原点的超平面。而代表权值设定的点必须位于代表训练样本的超平面的某一侧，这样才有可能得到正确的结果。下面给出了一个例子。在二维平面中，我们用下图中一条黑色直线来代表一组训练样本，这条直线过原点，且其输入向量（蓝色线段）与该直线垂直。该训练样本的正确结果为1，可以看到，位于黑色直线上面一侧，也就是与输入向量的夹角小于90度的权值是好的；位于黑色直线下面一侧，也就是于输入向量夹角大于90度的权值是坏的。下面第二张图给出了一个训练样本结果为0的例子，可以看到，其权值的选择与上一个例子恰恰相反。着说明输入向量限制了权值的选择。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__9.png" alt="2_9"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__10.png" alt="2_10"></p>
<p>下图将前面的两个训练样本放到了一张图中，可以看到合适的权值点处于一个锥形区域之中。对于高维的问题，合适的权值点应该处于一个超锥形立体区域（hyper-cone）内，由此也可以说明问题是凸的。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__11.png" alt="2_11"></p>
<h2 id="Why-the-learning-works"><a href="#Why-the-learning-works" class="headerlink" title="Why the learning works"></a>Why the learning works</h2><p>这一小节讨论学习是否可行的问题。通过讨论这一问题，我们对感知机会有更深的理解。假定存在一个权值向量对所有的训练样本都能得到正确的结果，如下图中绿色点所示，称之为fesible factor，当然feasible factor应该是有无穷多个的。我们用平方距离$d_a^2+d_b^2$来衡量每一个可行权值向量与当前全值向量的距离。每一次当前权值向量在某一训练样本上发生错误时，算法会更新当前权值向量，使其更接近每一个可行权值向量。这种做法几乎总是对的，但存在一个错误。例如可行权值向量位于下图中金色点的位置，当前权值向量仍位于错误一侧，输入向量又相当得大，当我们更新当前权值向量，会使得其与金色的可行权值向量距离更大。这与我们的期望恰恰相反，需要作出调整。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__12.png" alt="2_12"></p>
<p>下面提到了一般可行的权值向量，如下图所示。每一次感知机犯错时，当前权值向量与所有一般可行权值向量的距离都至少要减小更新向量的一个长度。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__13.png" alt="2_13"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__14.png" alt="2_14"></p>
<h2 id="What-perceptrons-can’t-do"><a href="#What-perceptrons-can’t-do" class="headerlink" title="What perceptrons can’t do"></a>What perceptrons can’t do</h2><p>这一小节讨论感知机的局限性，其局限性来源于特征的选择。如果选择了恰当的特征，那感知机几乎可以处理任何问题；而如果选择了错误的特征，那感知机可以做的事情就大大被限制住了，所以选择特征是一件非常困难的事情。比如说，你要用感知机去判断一个句子是否是一个符合逻辑的英文句子，那你需要定义大量的特征，然后学习如何去处理它们以决定某一个给定的句子是否为一个好的英文句子。</p>
<p>当然，如果用人工去提取特征并得到了足够的特征，那感知机几乎可以解决任何问题。例如如果输入向量中的元素都是二元的，我们可以得到任何可能的一个分布，此时感知机不受限制。一旦选定人工提取的特征，感知机就有了很强的一个限制。例如，二元阈值神经网络（binary threshold neuron，也就是感知机）不能做下图中的事情。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__15.png" alt="2_15"></p>
<p>下面给出了一个几何视角，即二元阈值神经网络不可能将下图中的四个点标记成图中的样子。我们将类似图中这四个点的训练样本称为线性不可分的。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__16.png" alt="2_16"></p>
<p>后边的东西没听懂，不过貌似不影响后面的学习。希望有理解的同学给讲解一下。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__17.png" alt="2_17"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__18.png" alt="2_18"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__19.png" alt="2_19"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__2__20.png" alt="2_20"></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" data-id="cjfguskm80021cgurqhxk8emg" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Neural-Network/">Neural Network</a></div><div class="post-nav"><a href="/2016/05/05/机器学习中使用的神经网络第三讲：线性-逻辑神经网络和BackPropagation/" class="pre">机器学习中使用的神经网络第三讲：线性/逻辑神经网络和BackPropagation</a><a href="/2016/04/26/机器学习基石第九讲：linear-regression/" class="next">机器学习基石第九讲：linear regression</a></div><div data-thread-key="2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" data-title="机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机" data-url="http://marcovaldong.github.io/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" data-title="机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机" data-url="http://marcovaldong.github.io/2016/05/04/机器学习中使用的神经网络第二讲笔记：神经网络的结构和感知机/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>