<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>机器学习中使用的神经网络第十讲 | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习中使用的神经网络第十讲</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习中使用的神经网络第十讲</h1><div class="post-meta">May 30, 2016<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a><a href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/05/30/机器学习中使用的神经网络第十讲/" href="/2016/05/30/机器学习中使用的神经网络第十讲/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-it-helps-to-combine-models"><span class="toc-number">1.</span> <span class="toc-text">Why it helps to combine models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mixtures-of-experts"><span class="toc-number">2.</span> <span class="toc-text">Mixtures of experts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-idea-of-full-Bayesian-learning"><span class="toc-number">3.</span> <span class="toc-text">The idea of full Bayesian learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Making-full-Bayesian-learning-practical"><span class="toc-number">4.</span> <span class="toc-text">Making full Bayesian learning practical</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dropout"><span class="toc-number">5.</span> <span class="toc-text">Dropout</span></a></li></ol></div></div><div class="post-content"><p>Geoffery Hinton教授的<em>Neuron Networks for Machine Learning</em>的第十讲介绍了如何combine模型，并进一步从实际应用的角度介绍了完全贝叶斯方法。</p>
<h2 id="Why-it-helps-to-combine-models"><a href="#Why-it-helps-to-combine-models" class="headerlink" title="Why it helps to combine models"></a>Why it helps to combine models</h2><p>这一小节，我们讨论在做预测时为什么要结合许多模型。使用多个模型可以在拟合真正的规则和拟合样本错误之间做一个很好的折中。</p>
<p>我们已经知道，当训练数据比较少时容易出现过拟合，如果我们平均一下许多不同模型的预测，那我们就可以降低过拟合的程度。对于回归来讲，当模型的capacity较弱时容易出现high-bias；当模型的capacity太强时，模型就会过多的拟合样本错误，从而出现high-variance。而结合多个不同的模型，我们就可以实现一个更好的折中。<br><a id="more"></a><br>下面我们来分析如何对单独的模型（individual model）和平均的模型（an average of models）进行比较分析。在任意一个单个的测试集上，一些单独的模型的预测结果可能要比结合的模型的预测要好，这很正常。而在不同的样本上，不同的单独的预测器可各有优劣。如果单个的预测器之间相差很大的话，那平均下来，结合后的预测器肯定比单个的都要强，所以我们应该尝试着让单个的预测器差距大一些（犯的错很不相同，但各自的性能还是可以的）。</p>
<p>现在，我们来看一下网络结合背后的数学推导。如下图所示，$\overline{y}$是N个不同模型对同一个输入的预测的平均，而$<(t-y_i)^2>_i$表示目标输出t与每一个预测值$y_i$的误差平方的均值。我们可以将$&lt;&gt;_i$看成一个取平均的符号。注意推导的最后一步，平方和展开式的结合项为零，可以直接省去。可以看到，目标输出t与每一个预测值$y_i$的误差平方的均值可以写成目标输出t与结合后的预测$\overline{y}$的误差平方再加上一个正值。这就是说，结合后预测要比结合前的好。</(t-y_i)^2></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__1.png" alt="10_1"></p>
<p>下图给出了一个示意图，表示了不同的预测和目标输出的一个差距：红色的表示不好的预测，其与目标输出t的距离要比$\overline{y}$与t的距离大得多；绿色的表示好的预测，其与目标输出t的距离要比$\overline{y}$与t的距离小。但因为是使用平方误差，所以不好的预测占一个主导地位。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__2.png" alt="10_2"></p>
<p>然后我们再做一个数学上的计算，我们假设$\overline{y}$到good guy和bad guy的距离相等，然后做一个计算得到了上图中的等式。但这样的等式并非总成立，这里主要是因为我们使用的是平方误差。换一个误差衡量方式，等式就未必成立了。下图给出了一个例子。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__3.png" alt="10_3"></p>
<p>下图列出了很多让预测器不同的方法：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__4.png" alt="10_4"></p>
<p>下图给出了通过使用不同的训练数据集来得到不同模型的方法：</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__5.png" alt="10_5"></p>
<h2 id="Mixtures-of-experts"><a href="#Mixtures-of-experts" class="headerlink" title="Mixtures of experts"></a>Mixtures of experts</h2><p>本小节介绍多专家模型（the mixture of experts model），该模型的思想是训练多个神经网络（也就是多个专家），每个神经网络（专家）被指定（specialize）应用于数据集的不同部分。这就是说，数据集可能有着多个不同的来源（different regimes，意思是说数据集中的数据的产生方式不同，这里我翻译成了“不同的来源”），不同来源提供的数据差距较大（但真实），因此我们为每一个来源的数据一个指定的神经网络来处理，而且模型还有一个managing neural net用来判断一个输入应该交给哪一个神经网络来处理。</p>
<p>对于较小的数据集，该模型的表现可能不太好，但随着数据集规模的增大，该模型的表现会有明显的提高。更重要的是，单个的模型往往善于处理一部分数据，不擅长处理另外一部分数据（在这部分数据上犯错多），而多专家系统则很好的解决了这个问题：系统中的每一个神经网络，也就是每一个专家都会有一个擅长的数据区域，在这组区域上其要比其他专家表现得好。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__6.png" alt="10_6"></p>
<p>下图给出了局部模型（very local models）与全局模型（fully global models）的一个比较。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__7.png" alt="10_7"></p>
<p>多专家系统是单一全局模型或者多个局部模型的一个很好的折中，但我们现在面临的一个很重要的问题就是如何将数据集分成不同的部分。下图展示了划分数据集的几种方法：按照输入到输出的映射，可以将图中数据分成两组，一组对应那条红色的抛物线，一组对应那条绿色的抛物线；仅按照输入作一个聚类的话，就被图中那条蓝色直线分成了两类。这里划分训练数据集的目的在于能够从每一个cluster中的输入和输出很好得得到一个局部模型。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__8.png" alt="10_8"></p>
<p>下面我们来介绍使模型cooperate的损失函数，下图使用了前一小节介绍的东西，这样训练得来的模型要比单独训练每个模型的性能更好。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__9.png" alt="10_9"></p>
<p>下图给出了averaging models为什么能够使模型cooperation的原因。下图右侧是除模型i外所有模型对某一输入的预测的平均，中间的t是目标输出，左侧$y_i$是模型i的预测。当我们加上$y_i$去计算一个新的平均值时，这个值肯定要更接近t，从而实现了一点点修正。所以，为了使平均值越来越接近t，我们就需要$y_i$左移。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__10.png" alt="10_10"></p>
<p>但实际上，我们希望做的是让$y_i$越来越接近目标t，而这种做法就会使得模型specialization。下图给出了一个使模型specialization的损失函数。这里的损失是一个期望值，其中的$p_i$是我们在处理该组输入数据时会使用模型i的概率。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__11.png" alt="10_11"></p>
<p>多专家系统使用了第二种损失函数的形式，下图给出了示意图。在多专家系统中有一个softmax gating network，对于一个给定的输入，该网络输出每一个专家对该输入的处理的可靠度。而系统最终的损失函数就是每个专家的输出与目标的差值平方再乘上可靠度。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__12.png" alt="10_12"></p>
<p>下图给出了两个偏导。前一个偏导得到的梯度表示一个专家应做的修正：如果该专家的可靠度（$P_i$）较小，那么该梯度值也就很小，也就意味着该专家（神经网络）只需要做一个很小的调整（只需要为现在犯的这个错误负很小的责任）；如果该专家的可靠度（$p_i$）较大，那么该梯度也就比较大，也就意味着该专家（神经网络）需要做一个大的修正（需要为现在犯的这个错误负很大的责任）。后一个偏导得来的梯度用来修正专家的可靠度：如果一个专家犯的错比平均错误来的小，那我们就要增大其可靠度；如果一个专家犯的错误比平均错误来的大，那我们就应该降低其可靠度。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__13.png" alt="10_13"></p>
<p>在这一小节的最后hinton给出了多专家系统的一个更好的损失函数。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__14.png" alt="10_14"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__15.png" alt="10_15"></p>
<h2 id="The-idea-of-full-Bayesian-learning"><a href="#The-idea-of-full-Bayesian-learning" class="headerlink" title="The idea of full Bayesian learning"></a>The idea of full Bayesian learning</h2><p>这一小节继续介绍完全贝叶斯方法，介绍在实际中如何应用它。在完全贝叶斯方法中，我们不是尝试着去找到仅仅一组参数设置，而是去求出所有可能的参数设置的后验概率分布（也就是对于每一组可能的参数设置都有一个后验概率密度，积分起来恰为一）。然后在预测阶段，我们计算出所有参数设置对应的预测值，然后每个预测值乘上该参数设置的后验概率再求和得到一个加权值，这个加权值就是模型最后的输出。当然，这两个过程都需要做大量的计算工作。还有一点，完全贝叶斯方法在较小的数据集上仍可以使用较复杂的模型。</p>
<p>前面我们已经讲过，复杂模型在较小的数据集上会出现过拟合的问题。但前面说的过拟合是发生在我们前面得到的那个最佳参数设置上，完全贝叶斯方法要所有可能的参数设置，这就大大较小了过拟合的可能。在较小的数据集上，完全贝叶斯方法考虑的参数设置会特别多（可用的参数设置所在的空间比较大），因此最终的预测也会很模糊。随着数据集的增大，完全贝叶斯考虑的参数设置越来越有指向性（可用参数设置所在空间变得越来越小），最终的预测也会越来越准确。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__16.png" alt="10_16"></p>
<p>下图给出了拟合的一个典型例子：右上图中有六个数据点，红色曲线表示一条五次多项式，明显过拟合。但是如果我们基于一个合理的先验条件，如多项式的次数为5，去使用完全贝叶斯方法求得所有可能的参数设置。下图右下给出了几条可能的参数设置对应的曲线。可以看到，这些曲线差别很大，但如果由所有曲线的预测值去计算出一个加权值，会发现这些加权值大概会落在绿色直线的附近。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__17.png" alt="10_17"></p>
<p>对于为什么数据集的大小会影响我们的先验条件和模型的复杂度，从贝叶斯的角度出发是没有原因的。下面介绍在一个仅有几个参数的神经网络中去近似的使用完全贝叶斯学习。</p>
<p>我们在参数空间上放置一个网格（a grid），然后对每一个参数选定几个值，然后我们就得到了参数空间上的若干网格点。每一个网格点对应一组参数，我们就可以判断每一组参数预测数据的性能如何。这里没有引入梯度下降，我们需要考虑的只是参数空间中的一系列参数点。一旦确定了每个参数点的后验概率，我们就可以对预测集中的数据做预测了。整个过程的计算量都很大，但在较小的数据集上，该方法要比最大似然法或者最大后验概率法好很多。下图中给出了一个概率计算公式。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__18.png" alt="10_18"></p>
<p>下图给出了完全贝叶斯学习的一个例子。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/NNML__10__19.png" alt="10_19"></p>
<h2 id="Making-full-Bayesian-learning-practical"><a href="#Making-full-Bayesian-learning-practical" class="headerlink" title="Making full Bayesian learning practical"></a>Making full Bayesian learning practical</h2><p>这一小节我们介绍如何在有数千甚至百万之多的权值参数的神经网络中使用完全贝叶斯方法，这里使用的方法称之为蒙特卡罗方法（Monte Carlo<br>method）。我们使用一个随机数生成器以随机的方式在权值向量空间中来回移动，不过该生成器要遵循降低损失的bias。如果顺利的话，我们得到的样本权值向量会按比例的遵循后验概率分布。</p>
<p>通过一系列的权值向量的样本，我们可以得到完全贝叶斯方法的一个好的近似。网格点的数量是参数数量的指数级倍数，因此对于少数数量的参数我们可能不能构成网格（这点不太明白）。如果存在足够大的数据集使得大部分的参数向量差别较大，可能只有的一小部分网格点（参数向量）能够有效的去做预测，我们就可以取估计这一小部分的参数向量。下图给出了一个概率计算公式，这里的权值向量$w_i$为参数样本。既然使用的是样本，那不可避免的就会引入噪声。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__20.png" alt="10_20"></p>
<p>下面两张图说明了在标准的反向传播（back propagation）中发生了什么。下图右侧是一个权值空间的示意图，其可能有着很高的维度且无边界（unbounded）。在这个空间中，我们已经标注了一些轮廓线，轮廓线上的参数对应的损失函数是相等的。反向传播就是从某一个参数向量出发，然后沿着梯度的方向向损失曲面的谷底移动。最终，我们要么取得一个局部最小值，要么陷入一个平台区域。现在如果我们使用a sampling method的话，我们从某一个地方出发，在每一次用梯度对权值做更新时都会引入高斯噪声，因此权值向量可能一直来回晃动永远无法确定下来（不收敛）。就是说，权值参数可能会一直从参数空间的某一个区域跳到另一个区域，当然这些区域都很接近一个局部最小值。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__21.png" alt="10_21"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__22.png" alt="10_22"></p>
<p>现在一个很重要的问题是参数向量多久会遍历一次所有可能的参数设置。现在我们想到的一个方法是，在反向传播没进行10000步时我们保存一下权值向量。图中红点表示我们漫游（wander around）权值空间时得到的一些样本点，当然，这些样本点中的一些可能对应较高的损失（因为那些损失曲面的谷底部分可能比较大）。the deepest minimum部位可能会包含最多的红点，其他次最小值部位也会包含一些红点。不过这些红点不会是局部最小值对应的参数向量，因为它们是noisy sample。</p>
<p>上述的过程称之为马尔科夫链蒙特卡罗方法（Markov Chain Monte Carlo method），该方法使得在拥有数千个参数的模型上使用完全贝叶斯方法变得可行。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__23.png" alt="10_23"></p>
<p>完全贝叶斯学习还可以和小批量梯度下降法结合起来使用。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__24.png" alt="10_24"></p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>这一小节，我们介绍一种新的combine大量神经网络的方法——dropout，该方法不用事先单独的训练每一个神经网络。在使用每个训练样本训练网络时，我们会随机的忽略一些隐含单元，所以每一次训练都会得到一个不同的模型。那么，问题来了，我们如何在测试时对前面得到的不同模型做一个平均呢，答案就是我们使用大量的权值共享（weight sharing）。</p>
<p>下面我们介绍两种方法来实现combine多个模型的不同输出。假设现在有模型A和模型B，模型对每个输入都给出三个标记的可能性，如下图中给出了模型A对输入的三个标记的可能性分别是0.3、0.2、0.5，模型B对输入给出了三个标记的可能性分别为0.1、0.8、0.1。第一种方法计算两个模型输出为同一个标记的可能性的平均值，如下图中给出了combine后输出三种标记的可能性分别为0.2、0.5、0.3。第二种方法计算的则是<a href="http://baike.baidu.com/link?url=LZR8TmaFgsvcMUzgkR3Cm-rw2jkeNNMulwFBNe8Mye-TB7msNLjBnYGLVsZ3dgZTYo-I27LW0G0QQQcz_WmvtjAHCi2GlJhZvLziU0Nya6NPtSIeOXjkMCJ0pGuZowxmMMsii8a0HX-uE1DkvjHPD_" target="_blank" rel="external">几何平均</a>（geometirc means），如下图所示。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__25.png" alt="10_25"></p>
<p>下面介绍dropout，该方法可能不如真正的贝叶斯方法好，但其具有更强的可行性。考虑一个只有一个隐含层的神经网络，每次使用一个训练样本来训练网络时，我们随机的忽略隐含层的隐含单元（每个隐含单元被忽略的几率为0.5），这样就得到一个模型。通过这种方式，理论上我们可以得到$2^H$种模型的样本。当然实际上我们只能得到很少的一些模型的样本，且每个样本通过通过仅一组训练数据得到的。然后hinton说这里要权值共享（weight sharing），但我没想明白到底怎么实现共享。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__26.png" alt="10_26"></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__27.png" alt="10_27"></p>
<p>下面介绍在测试阶段，我们应该做些什么。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__28.png" alt="10_28"></p>
<p>下图是当网络有多个隐含层时，我们应该怎么做。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__29.png" alt="10_29"></p>
<p>在输入层上，我们也可以使用dropout，不过要给一个更高的概率（前面说隐含单元每一次存在的概率为0.5，而输入单元存在的概率要高很多），这在实际中已有应用。</p>
<p>下图提到了dropout的性能，其能降低过拟合的程度。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__30.png" alt="10_30"></p>
<p>下图从另一个角度来解释了dropout。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/10__31.png" alt="10_31"></p>
<blockquote>
<p>There’s another way to think about dropout, which is how I originally arrived at the idea. And you’ll see it’s a bit related to mixtures of experts, and what’s going wrong when all the experts cooperate, what’s preventing specialization? So, if a hidden unit knows which other hidden units are present, it can co-adapt to the other hidden units on the training data. What that means is, the real signal that’s training a hidden unit is, try to fix up the error that’s leftover when all the other hidden units have had their say.</p>
<p>That’s what’s being back propagated to train the weights of each hidden unit. Now, that’s going to cause complex co-adaptations between the hidden units. And these are likely to go wrong when there’s a change in the data. So, a new test data, If you rely on a complex co-adaptation to get things right on the training data, it’s quite likely to not work nearly so well on new test data. It’s like the idea that a big, complex conspiracy involving lots of people is almost certain to go wrong because there’s always things you didn’t think of. And if there’s a large number of people involved, one of them will behave in an unexpected way. And then, the others will be doing the wrong thing. It’s much better if you want conspiracies, to have lots of little conspiracies. Then, when unexpected things happen, many of the little conspiracies will fail, but some of them will still succeed.</p>
<p>So, by using dropout, we force a hidden unit to work with combinatorially many other sets of hidden units. And that makes it much more likely to do something that’s individually useful rather than only useful because of the way particular other hidden units are collaborating with it. But it is also going to tend to do something that’s individually useful and is different from what other hidden units do. It needs to be something that’s marginally useful, given what its co workers tend to achieve. And I think this is what’s giving nets with dropout, their very good performance.</p>
</blockquote>
<p>总体理解的不是很透彻，以后有了新的理解之后会做修正补充。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/05/30/机器学习中使用的神经网络第十讲/" data-id="cjfguskpc0028cgurqgndal1k" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Neural-Network/">Neural Network</a></div><div class="post-nav"><a href="/2016/06/12/使用Theano实现kaggle手写识别/" class="pre">使用Theano实现kaggle手写识别</a><a href="/2016/05/21/机器学习中使用的神经网络第九讲/" class="next">机器学习中使用的神经网络第九讲</a></div><div data-thread-key="2016/05/30/机器学习中使用的神经网络第十讲/" data-title="机器学习中使用的神经网络第十讲" data-url="http://marcovaldong.github.io/2016/05/30/机器学习中使用的神经网络第十讲/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/05/30/机器学习中使用的神经网络第十讲/" data-title="机器学习中使用的神经网络第十讲" data-url="http://marcovaldong.github.io/2016/05/30/机器学习中使用的神经网络第十讲/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>