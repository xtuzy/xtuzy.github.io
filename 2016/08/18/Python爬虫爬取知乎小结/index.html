<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>Python爬虫爬取知乎小结 | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Python爬虫爬取知乎小结</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Python爬虫爬取知乎小结</h1><div class="post-meta">Aug 18, 2016<span> | </span><span class="category"><a href="/categories/爬虫/">爬虫</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/08/18/Python爬虫爬取知乎小结/" href="/2016/08/18/Python爬虫爬取知乎小结/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#模拟登录"><span class="toc-number">1.</span> <span class="toc-text">模拟登录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#获取用户基本信息"><span class="toc-number">2.</span> <span class="toc-text">获取用户基本信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#获取某个答案的所有点赞者名单"><span class="toc-number">3.</span> <span class="toc-text">获取某个答案的所有点赞者名单</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#提取用户头像"><span class="toc-number">4.</span> <span class="toc-text">提取用户头像</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#抓取某个问题的所有答案"><span class="toc-number">5.</span> <span class="toc-text">抓取某个问题的所有答案</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据库存取数据"><span class="toc-number">6.</span> <span class="toc-text">数据库存取数据</span></a></li></ol></div></div><div class="post-content"><p>最近学习了一点网络爬虫，并实现了使用python来爬取知乎的一些功能，这里做一个小的总结。网络爬虫是指通过一定的规则自动的从网上抓取一些信息的程序或脚本。我们知道机器学习和数据挖掘等都是从大量的数据出发，找到一些有价值有规律的东西，而爬虫则可以帮助我们解决获取数据难的问题，因此网络爬虫是我们应该掌握的一个技巧。</p>
<p>python有很多开源工具包供我们使用，我这里使用了requests、BeautifulSoup4、json等包。requests模块帮助我们实现http请求，bs4模块和json模块帮助我们从获取到的数据中提取一些想要的信息，几个模块的具体功能这里不具体展开。下面我分功能来介绍如何爬取知乎。<br><a id="more"></a></p>
<h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><p>要想实现对知乎的爬取，首先我们要实现模拟登录，因为不登录的话好多信息我们都无法访问。下面是登录函数，这里我直接使用了知乎用户<a href="https://www.zhihu.com/people/fireling" target="_blank" rel="external">fireling</a>的登录函数，具体如下。其中你要在函数中的data里填上你的登录账号和密码，然后在爬虫之前先执行这个函数，不出意外的话你就登录成功了，这时你就可以继续抓取想要 的数据。注意，在首次使用该函数时，程序会要求你手动输入captcha码，输入之后当前文件夹会多出cookiefile文件和zhihucaptcha.gif，前者保留了cookie信息，后者则保存了验证码，之后再去模拟登录时，程序会自动帮我们填上验证码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></div><div class="line">    url = <span class="string">'http://www.zhihu.com'</span></div><div class="line">    loginURL = <span class="string">'http://www.zhihu.com/login/email'</span></div><div class="line"></div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">"User-Agent"</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:41.0) Gecko/20100101 Firefox/41.0'</span>,</div><div class="line">        <span class="string">"Referer"</span>: <span class="string">"http://www.zhihu.com/"</span>,</div><div class="line">        <span class="string">'Host'</span>: <span class="string">'www.zhihu.com'</span>,</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    data = &#123;</div><div class="line">        <span class="string">'email'</span>: <span class="string">'you@example.com'</span>,</div><div class="line">        <span class="string">'password'</span>: <span class="string">'**************'</span>,</div><div class="line">        <span class="string">'rememberme'</span>: <span class="string">"true"</span>,</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">global</span> s</div><div class="line">    s = requests.session()</div><div class="line">    <span class="keyword">global</span> xsrf</div><div class="line">    <span class="keyword">if</span> os.path.exists(<span class="string">'cookiefile'</span>):</div><div class="line">        <span class="keyword">with</span> open(<span class="string">'cookiefile'</span>) <span class="keyword">as</span> f:</div><div class="line">            cookie = json.load(f)</div><div class="line">        s.cookies.update(cookie)</div><div class="line">        req1 = s.get(url, headers=headers)</div><div class="line">        soup = BeautifulSoup(req1.text, <span class="string">"html.parser"</span>)</div><div class="line">        xsrf = soup.find(<span class="string">'input'</span>, &#123;<span class="string">'name'</span>: <span class="string">'_xsrf'</span>, <span class="string">'type'</span>: <span class="string">'hidden'</span>&#125;).get(<span class="string">'value'</span>)</div><div class="line">        <span class="comment"># 建立一个zhihu.html文件,用于验证是否登陆成功</span></div><div class="line">        <span class="keyword">with</span> open(<span class="string">'zhihu.html'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(req1.content)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        req = s.get(url, headers=headers)</div><div class="line">        <span class="keyword">print</span> req</div><div class="line"></div><div class="line">        soup = BeautifulSoup(req.text, <span class="string">"html.parser"</span>)</div><div class="line">        xsrf = soup.find(<span class="string">'input'</span>, &#123;<span class="string">'name'</span>: <span class="string">'_xsrf'</span>, <span class="string">'type'</span>: <span class="string">'hidden'</span>&#125;).get(<span class="string">'value'</span>)</div><div class="line"></div><div class="line">        data[<span class="string">'_xsrf'</span>] = xsrf</div><div class="line"></div><div class="line">        timestamp = int(time.time() * <span class="number">1000</span>)</div><div class="line">        captchaURL = <span class="string">'http://www.zhihu.com/captcha.gif?='</span> + str(timestamp)</div><div class="line">        <span class="keyword">print</span> captchaURL</div><div class="line"></div><div class="line">        <span class="keyword">with</span> open(<span class="string">'zhihucaptcha.gif'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">            captchaREQ = s.get(captchaURL, headers=headers)</div><div class="line">            f.write(captchaREQ.content)</div><div class="line">        loginCaptcha = raw_input(<span class="string">'input captcha:\n'</span>).strip()</div><div class="line">        data[<span class="string">'captcha'</span>] = loginCaptcha</div><div class="line">        <span class="keyword">print</span> data</div><div class="line">        loginREQ = s.post(loginURL, headers=headers, data=data)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> loginREQ.json()[<span class="string">'r'</span>]:</div><div class="line">            <span class="keyword">print</span> s.cookies.get_dict()</div><div class="line">            <span class="keyword">with</span> open(<span class="string">'cookiefile'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">                json.dump(s.cookies.get_dict(), f)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">'login fail'</span></div></pre></td></tr></table></figure>
<p>需要注意的是，在login函数中有一个全局变量s=reequests.session()，我们用这个全局变量来访问知乎，整个爬取过程中，该对象都会保持我们的持续模拟登录。</p>
<h2 id="获取用户基本信息"><a href="#获取用户基本信息" class="headerlink" title="获取用户基本信息"></a>获取用户基本信息</h2><p>知乎上每个用户都有一个唯一ID，例如我的ID是marcovaldong，那么我们就可以通过访问地址 <a href="https://www.zhihu.com/people/marcovaldong" target="_blank" rel="external">https://www.zhihu.com/people/marcovaldong</a> 来访问我的主页。个人主页中包含了居住地、所在行业、性别、教育情况、获得的赞数、感谢数、关注了哪些人、被哪些人关注等信息。因此，我首先介绍如何通过爬虫来获取某一个知乎用户的一些信息。下面的函数get_userInfo(userID)实现了爬取一个知乎用户的个人信息，我们传递给该用户一个用户ID，该函数就会返回一个 list，其中包含昵称、ID、居住地、所在行业、性别、所在公司、职位、毕业学校、专业、赞同数、感谢数、提问数、回答数、文章数、收藏数、公共编辑数量、关注的人数、被关注的人数、主页被多少个人浏览过等19个数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_userInfo</span><span class="params">(userID)</span>:</span></div><div class="line">    user_url = <span class="string">'https://www.zhihu.com/people/'</span> + userID</div><div class="line">    response = s.get(user_url, headers=header_info)</div><div class="line">    <span class="comment"># print response</span></div><div class="line">    soup = BeautifulSoup(response.content, <span class="string">'lxml'</span>)</div><div class="line">    name = soup.find_all(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'name'</span>&#125;)[<span class="number">1</span>].string</div><div class="line">    <span class="comment"># print 'name: %s' % name</span></div><div class="line">    ID = userID</div><div class="line">    <span class="comment"># print 'ID: %s' % ID</span></div><div class="line">    location = soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'location item'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> location == <span class="keyword">None</span>:</div><div class="line">        location = <span class="string">'None'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        location = location.string</div><div class="line">    <span class="comment"># print 'location: %s' % location</span></div><div class="line">    business = soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'business item'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> business == <span class="keyword">None</span>:</div><div class="line">        business = <span class="string">'None'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        business = business.string</div><div class="line">    <span class="comment"># print 'business: %s' % business</span></div><div class="line">    gender = soup.find(<span class="string">'input'</span>, &#123;<span class="string">'checked'</span>: <span class="string">'checked'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> gender == <span class="keyword">None</span>:</div><div class="line">        gender = <span class="string">'None'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        gender = gender[<span class="string">'class'</span>][<span class="number">0</span>]</div><div class="line">    <span class="comment"># print 'gender: %s' % gender</span></div><div class="line">    employment = soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'employment item'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> employment == <span class="keyword">None</span>:</div><div class="line">        employment = <span class="string">'None'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        employment = employment.string</div><div class="line">    <span class="comment"># print 'employment: %s' % employment</span></div><div class="line">    position = soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'position item'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> position == <span class="keyword">None</span>:</div><div class="line">        position = <span class="string">'None'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        position = position.string</div><div class="line">    <span class="comment"># print 'position: %s' % position</span></div><div class="line">    education = soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'education item'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> education == <span class="keyword">None</span>:</div><div class="line">        education = <span class="string">'None'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        education = education.string</div><div class="line">    <span class="comment"># print 'education: %s' % education</span></div><div class="line">    major = soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'education-extra item'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> major == <span class="keyword">None</span>:</div><div class="line">        major = <span class="string">'None'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        major = major.string</div><div class="line">    <span class="comment"># print 'major: %s' % major</span></div><div class="line"></div><div class="line">    agree = int(soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'zm-profile-header-user-agree'</span>&#125;).strong.string)</div><div class="line">    <span class="comment"># print 'agree: %d' % agree</span></div><div class="line">    thanks = int(soup.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'zm-profile-header-user-thanks'</span>&#125;).strong.string)</div><div class="line">    <span class="comment"># print 'thanks: %d' % thanks</span></div><div class="line">    infolist = soup.find_all(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>: <span class="string">'item'</span>&#125;)</div><div class="line">    asks = int(infolist[<span class="number">1</span>].span.string)</div><div class="line">    <span class="comment"># print 'asks: %d' % asks</span></div><div class="line">    answers = int(infolist[<span class="number">2</span>].span.string)</div><div class="line">    <span class="comment"># print 'answers: %d' % answers</span></div><div class="line">    posts = int(infolist[<span class="number">3</span>].span.string)</div><div class="line">    <span class="comment"># print 'posts: %d' % posts</span></div><div class="line">    collections = int(infolist[<span class="number">4</span>].span.string)</div><div class="line">    <span class="comment"># print 'collections: %d' % collections</span></div><div class="line">    logs = int(infolist[<span class="number">5</span>].span.string)</div><div class="line">    <span class="comment"># print 'logs: %d' % logs</span></div><div class="line">    followees = int(infolist[len(infolist)<span class="number">-2</span>].strong.string)</div><div class="line">    <span class="comment"># print 'followees: %d' % followees</span></div><div class="line">    followers = int(infolist[len(infolist)<span class="number">-1</span>].strong.string)</div><div class="line">    <span class="comment"># print 'followers: %d' % followers</span></div><div class="line">    scantime = int(soup.find_all(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'zg-gray-normal'</span>&#125;)[len(soup.find_all(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'zg-gray-normal'</span>&#125;))<span class="number">-1</span>].strong.string)</div><div class="line">    <span class="comment"># print 'scantime: %d' % scantime</span></div><div class="line"></div><div class="line">    info = (name, ID, location, business, gender, employment, position,</div><div class="line">            education, major, agree, thanks, asks, answers, posts,</div><div class="line">            collections, logs, followees, followers, scantime)</div><div class="line">    <span class="keyword">return</span> info</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    login()</div><div class="line">    userID = <span class="string">'marcovaldong'</span></div><div class="line">    info = get_userInfo(userID)</div><div class="line">    <span class="keyword">print</span> <span class="string">'The information of '</span> + userID + <span class="string">' is: '</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(info)):</div><div class="line">        <span class="keyword">print</span> info[i]</div></pre></td></tr></table></figure>
<p>下图是我的主页的部分截图，从上面可以看到这19个数据，下面第二张图是终端上显示的我的这19个数据，我们可以作个对照，看看是否全部抓取到了。这个函数我用了很长时间来调试，因为不同人的主页的信息完整程度是不同的，如果你在使用过程中发现了错误，欢迎告诉我。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/marcovaldong&#39;szhuye.png" alt=""></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/zhuyexinxixianshi.png" alt=""></p>
<h2 id="获取某个答案的所有点赞者名单"><a href="#获取某个答案的所有点赞者名单" class="headerlink" title="获取某个答案的所有点赞者名单"></a>获取某个答案的所有点赞者名单</h2><p>知乎上有一个问题是<a href="https://www.zhihu.com/question/36338520" target="_blank" rel="external">如何写个爬虫程序扒下知乎某个回答所有点赞用户名单？</a>，我参考了<a href="https://www.zhihu.com/people/loveQt" target="_blank" rel="external">段小草</a>的这个答案<a href="https://www.zhihu.com/question/20899988/answer/49749466" target="_blank" rel="external">如何入门Python爬虫</a>，然后有了下面的这个函数。</p>
<p>这里先来大概的分析一下整个流程。我们要知道，知乎上的每一个问题都有一个唯一ID，这个可以从地址中看出来，例如问题<a href="https://www.zhihu.com/question/38808048" target="_blank" rel="external">2015 年有哪些书你读过以后觉得名不符实？</a>的地址为 <a href="https://www.zhihu.com/question/38808048" target="_blank" rel="external">https://www.zhihu.com/question/38808048</a> ，其中38808048就是其ID。而每一个问题下的每一个答案也有一个唯一ID，例如该问题下的最高票答案<a href="https://www.zhihu.com/question/38808048/answer/81388411" target="_blank" rel="external">2015 年有哪些书你读过以后觉得名不符实？ - 余悦的回答 - 知乎</a>的地址链接为 <a href="https://www.zhihu.com/question/38808048/answer/81388411" target="_blank" rel="external">https://www.zhihu.com/question/38808048/answer/81388411</a> ，末尾的81388411就是该答案在该问题下的唯一ID。不过我们这里用到的不是这两个ID，而是我们在抓取点赞者名单时的唯一ID，此ID的获得方法是这样：例如我们打算抓取<a href="https://www.zhihu.com/question/23370588/answer/25341076" target="_blank" rel="external">如何评价《人间正道是沧桑》这部电视剧？ - 老编辑的回答 - 知乎</a>的点赞者名单，首先打开firebug，点击“5321 人赞同”时，firebug会抓取到一个“GET voters_profile”的一个包，把光标放在上面，会看到一个链接 <a href="https://www.zhihu.com/answer/5430533/voters_profile" target="_blank" rel="external">https://www.zhihu.com/answer/5430533/voters_profile</a> ，其中的5430533才是我们在抓取点赞者名单时用到的一个唯一ID。注意此ID只有在答案被赞过后才有。(在这安利一下《人间正道是沧桑》这部电视剧，该剧以杨立青三兄妹的恩怨情仇为线索，从大革命时期到解放战争，比较全面客观的展现了国共两党之间的主义之争，每一次看都会新的认识和体会。)</p>
<p>在拿到唯一ID后，我们用requests模块去get到知乎返回的信息，其中有一个json语句，该json语句中包含点赞者的信息。另外，我们在网页上浏览点赞者名单时，一次只能看到20条，每次下拉到名单底部时又加载出20条信息，再加载20条信息时所用的请求地址也包含在前面的json语句中。因此我们需要从json语句中提取出点攒着信息和下一个请求地址。在网页上浏览点赞者名单时，我们可以看到点赞者的昵称、头像、获得了多少赞同和感谢，以及提问和回答的问题数量，这里我提取了每个点赞者的昵称、主页地址（也就是用户ID）、赞同数、感谢数、提问数和回答数。关于头像的提取，我会在下面的函数中实现。</p>
<p>在提取到点赞者名单后，我将者信息保存了以唯一ID命名的txt文件中。下面是函数的具体实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">Zhihu = <span class="string">'http://www.zhihu.com'</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_voters</span><span class="params">(ans_id)</span>:</span></div><div class="line">    <span class="comment"># 直接输入问题id(这个id在点击“等人赞同”时可以通过监听网络得到)，关注者保存在以问题id命名的.txt文件中</span></div><div class="line">    login()</div><div class="line">    file_name = str(ans_id) + <span class="string">'.txt'</span></div><div class="line">    f = open(file_name, <span class="string">'w'</span>)</div><div class="line">    source_url = Zhihu + <span class="string">'/answer/'</span> +str(ans_id) +<span class="string">'/voters_profile'</span></div><div class="line">    source = s.get(source_url, headers=header_info)</div><div class="line">    <span class="keyword">print</span> source</div><div class="line">    content = source.content</div><div class="line">    <span class="keyword">print</span> content    <span class="comment"># json语句</span></div><div class="line">    data = json.loads(content)   <span class="comment"># 包含总赞数、一组点赞者的信息、指向下一组点赞者的资源等的数据</span></div><div class="line">    <span class="comment"># 打印总赞数</span></div><div class="line">    txt1 = <span class="string">'总赞数'</span></div><div class="line">    <span class="keyword">print</span> txt1.decode(<span class="string">'utf-8'</span>)</div><div class="line">    total = data[<span class="string">'paging'</span>][<span class="string">'total'</span>]   <span class="comment"># 总赞数</span></div><div class="line">    <span class="keyword">print</span> data[<span class="string">'paging'</span>][<span class="string">'total'</span>]   <span class="comment"># 总赞数</span></div><div class="line">    <span class="comment"># 通过分析，每一组资源包含10个点赞者的信息（当然，最后一组可能少于10个），所以需要循环遍历</span></div><div class="line">    nextsource_url = source_url     <span class="comment"># 从第0组点赞者开始解析</span></div><div class="line">    num = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> nextsource_url!=Zhihu:</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            nextsource = s.get(nextsource_url, headers=header_info)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            time.sleep(<span class="number">2</span>)</div><div class="line">            nextsource = s.get(nextsource_url, headers=header_info)</div><div class="line">        <span class="comment"># 解析出点赞者的信息</span></div><div class="line">        nextcontent = nextsource.content</div><div class="line">        nextdata = json.loads(nextcontent)</div><div class="line">        <span class="comment"># 打印每个点赞者的信息</span></div><div class="line">        <span class="comment"># txt2 = '打印每个点赞者的信息'</span></div><div class="line">        <span class="comment"># print txt2.decode('utf-8')</span></div><div class="line">        <span class="comment"># 提取每个点赞者的基本信息</span></div><div class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> nextdata[<span class="string">'payload'</span>]:</div><div class="line">            num += <span class="number">1</span></div><div class="line">            <span class="keyword">print</span> num</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                soup = BeautifulSoup(each, <span class="string">'lxml'</span>)</div><div class="line">                tag = soup.a</div><div class="line">                title = tag[<span class="string">'title'</span>]    <span class="comment"># 点赞者的用户名</span></div><div class="line">                href = <span class="string">'http://www.zhihu.com'</span> + str(tag[<span class="string">'href'</span>])    <span class="comment"># 点赞者的地址</span></div><div class="line">                <span class="comment"># 获取点赞者的数据</span></div><div class="line">                list = soup.find_all(<span class="string">'li'</span>)</div><div class="line">                votes = list[<span class="number">0</span>].string  <span class="comment"># 点赞者获取的赞同</span></div><div class="line">                tks = list[<span class="number">1</span>].string  <span class="comment"># 点赞者获取的感谢</span></div><div class="line">                ques = list[<span class="number">2</span>].string  <span class="comment"># 点赞者提出的问题数量</span></div><div class="line">                ans = list[<span class="number">3</span>].string  <span class="comment"># 点赞者回答的问题数量</span></div><div class="line">                <span class="comment"># 打印点赞者信息</span></div><div class="line">                string = title + <span class="string">'  '</span> + href + <span class="string">'  '</span> + votes + tks + ques + ans</div><div class="line">                f.write(string + <span class="string">'\n'</span>)</div><div class="line">                <span class="keyword">print</span> string</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                txt3 = <span class="string">'有点赞者的信息缺失'</span></div><div class="line">                f.write(txt3.decode(<span class="string">'utf-8'</span>) + <span class="string">'\n'</span>)</div><div class="line">                <span class="keyword">print</span> txt3.decode(<span class="string">'utf-8'</span>)</div><div class="line">                <span class="keyword">continue</span></div><div class="line">        <span class="comment"># 解析出指向下一组点赞者的资源</span></div><div class="line">        nextsource_url = Zhihu + nextdata[<span class="string">'paging'</span>][<span class="string">'next'</span>]</div><div class="line">    f.close()</div></pre></td></tr></table></figure>
<p>注意，点赞者名单中会有匿名用户，或者有用户被注销，这时我们抓取不到此用户的信息，我这里在txt文件中添加了一句“有点赞者的信息缺失”。</p>
<p>使用同样的方法，我们就可以抓取到一个用户的关注者名单和被关注者名单，下面列出了这两个函数。但是关注者名单抓取函数有一个问题，每次使用其抓取大V的关注者名单时，当抓取到第10020个follower的时候程序就会报错，好像知乎有访问限制一般。这个问题，我还没有找到解决办法，希望有solution的告知一下。因为没有看到有用户关注10020+个人，因此抓取被关注者名单函数暂时未发现报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_followees</span><span class="params">(username)</span>:</span></div><div class="line">    <span class="comment"># 直接输入用户名，关注者保存在以用户名命名的.txt文件中</span></div><div class="line">    followers_url = <span class="string">'http://www.zhihu.com/people/'</span> + username + <span class="string">'/followees'</span></div><div class="line">    file_name = username + <span class="string">'.txt'</span></div><div class="line">    f = open(file_name, <span class="string">'w'</span>)</div><div class="line">    data = s.get(followers_url, headers=header_info)</div><div class="line">    <span class="keyword">print</span> data  <span class="comment"># 访问服务器成功，返回&lt;responce 200&gt;</span></div><div class="line">    content = data.content  <span class="comment"># 提取出html信息</span></div><div class="line">    soup = BeautifulSoup(content, <span class="string">"lxml"</span>)   <span class="comment"># 对html信息进行解析</span></div><div class="line">    <span class="comment"># 获取关注者数量</span></div><div class="line">    totalsen = soup.select(<span class="string">'span[class*="zm-profile-section-name"]'</span>)</div><div class="line">    total = int(str(totalsen[<span class="number">0</span>]).split(<span class="string">' '</span>)[<span class="number">4</span>])     <span class="comment"># 总的关注者数量</span></div><div class="line">    txt1 = <span class="string">'总的关注者人数：'</span></div><div class="line">    <span class="keyword">print</span> txt1.decode(<span class="string">'utf-8'</span>)</div><div class="line">    <span class="keyword">print</span> total</div><div class="line">    follist = soup.select(<span class="string">'div[class*="zm-profile-card"]'</span>)  <span class="comment"># 记录有关注者信息的list</span></div><div class="line">    num = <span class="number">0</span> <span class="comment"># 用来在下面显示正在查询第多少个关注者</span></div><div class="line">    <span class="keyword">for</span> follower <span class="keyword">in</span> follist:</div><div class="line">        tag =follower.a</div><div class="line">        title = tag[<span class="string">'title'</span>]    <span class="comment"># 用户名</span></div><div class="line">        href = <span class="string">'http://www.zhihu.com'</span> + str(tag[<span class="string">'href'</span>])    <span class="comment"># 用户地址</span></div><div class="line">        <span class="comment"># 获取用户数据</span></div><div class="line">        num +=<span class="number">1</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'%d   %f'</span> % (num, num / float(total))</div><div class="line">        <span class="comment"># Alist = follower.find_all(has_attrs)</span></div><div class="line">        Alist = follower.find_all(<span class="string">'a'</span>, &#123;<span class="string">'target'</span>: <span class="string">'_blank'</span>&#125;)</div><div class="line">        votes = Alist[<span class="number">0</span>].string  <span class="comment"># 点赞者获取的赞同</span></div><div class="line">        tks = Alist[<span class="number">1</span>].string  <span class="comment"># 点赞者获取的感谢</span></div><div class="line">        ques = Alist[<span class="number">2</span>].string  <span class="comment"># 点赞者提出的问题数量</span></div><div class="line">        ans = Alist[<span class="number">3</span>].string  <span class="comment"># 点赞者回答的问题数量</span></div><div class="line">        <span class="comment"># 打印关注者信息</span></div><div class="line">        string = title + <span class="string">'  '</span> + href + <span class="string">'  '</span> + votes + tks + ques + ans</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            <span class="keyword">print</span> string.decode(<span class="string">'utf-8'</span>)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">print</span> string.encode(<span class="string">'gbk'</span>, <span class="string">'ignore'</span>)</div><div class="line">        f.write(string + <span class="string">'\n'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 循环次数</span></div><div class="line">    n = total/<span class="number">20</span><span class="number">-1</span> <span class="keyword">if</span> total/<span class="number">20.0</span>-total/<span class="number">20</span> == <span class="number">0</span> <span class="keyword">else</span> total/<span class="number">20</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>, <span class="number">1</span>):</div><div class="line">        <span class="comment"># if num%30 == 0:</span></div><div class="line">          <span class="comment">#   time.sleep(1)</span></div><div class="line">        <span class="comment"># if num%50 == 0:</span></div><div class="line">          <span class="comment">#   time.sleep(2)</span></div><div class="line">        raw_hash_id = re.findall(<span class="string">'hash_id(.*)'</span>, content)</div><div class="line">        hash_id = raw_hash_id[<span class="number">0</span>][<span class="number">14</span>:<span class="number">46</span>]</div><div class="line">        _xsrf = xsrf</div><div class="line">        offset = <span class="number">20</span>*i</div><div class="line">        params = json.dumps(&#123;<span class="string">"offset"</span>: offset, <span class="string">"order_by"</span>: <span class="string">"created"</span>, <span class="string">"hash_id"</span>: hash_id&#125;)</div><div class="line">        payload = &#123;<span class="string">"method"</span>:<span class="string">"next"</span>, <span class="string">"params"</span>: params, <span class="string">"_xsrf"</span>: _xsrf&#125;</div><div class="line">        click_url = <span class="string">'http://www.zhihu.com/node/ProfileFolloweesListV2'</span></div><div class="line">        data = s.post(click_url, data=payload, headers=header_info)</div><div class="line">        <span class="comment"># print data</span></div><div class="line">        source = json.loads(data.content)</div><div class="line">        <span class="keyword">for</span> follower <span class="keyword">in</span> source[<span class="string">'msg'</span>]:</div><div class="line">            soup1 = BeautifulSoup(follower, <span class="string">'lxml'</span>)</div><div class="line">            tag =soup1.a</div><div class="line">            title = tag[<span class="string">'title'</span>]    <span class="comment"># 用户名</span></div><div class="line">            href = <span class="string">'http://www.zhihu.com'</span> + str(tag[<span class="string">'href'</span>])    <span class="comment"># 用户地址</span></div><div class="line">            <span class="comment"># 获取用户数据</span></div><div class="line">            num +=<span class="number">1</span></div><div class="line">            <span class="keyword">print</span> <span class="string">'%d   %f'</span> % (num, num/float(total))</div><div class="line">            <span class="comment"># Alist = soup1.find_all(has_attrs)</span></div><div class="line">            Alist = soup1.find_all(<span class="string">'a'</span>, &#123;<span class="string">'target'</span>: <span class="string">'_blank'</span>&#125;)</div><div class="line">            votes = Alist[<span class="number">0</span>].string  <span class="comment"># 点赞者获取的赞同</span></div><div class="line">            tks = Alist[<span class="number">1</span>].string  <span class="comment"># 点赞者获取的感谢</span></div><div class="line">            ques = Alist[<span class="number">2</span>].string  <span class="comment"># 点赞者提出的问题数量</span></div><div class="line">            ans = Alist[<span class="number">3</span>].string  <span class="comment"># 点赞者回答的问题数量</span></div><div class="line">            <span class="comment"># 打印关注者信息</span></div><div class="line">            string = title + <span class="string">'  '</span> + href + <span class="string">'  '</span> + votes + tks + ques + ans</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                <span class="keyword">print</span> string.decode(<span class="string">'utf-8'</span>)</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                <span class="keyword">print</span> string.encode(<span class="string">'gbk'</span>, <span class="string">'ignore'</span>)</div><div class="line">            f.write(string + <span class="string">'\n'</span>)</div><div class="line">    f.close()</div></pre></td></tr></table></figure>
<h2 id="提取用户头像"><a href="#提取用户头像" class="headerlink" title="提取用户头像"></a>提取用户头像</h2><p>再往下就是抓取用户头像了，给出某个唯一ID，下面的函数自动解析其主页，从中解析出该用户头像地址，抓取到图片并保存到本地文件，文件以用户唯一ID命名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_avatar</span><span class="params">(userId)</span>:</span></div><div class="line">    url = <span class="string">'https://www.zhihu.com/people/'</span> + userId</div><div class="line">    response = s.get(url, headers=header_info)</div><div class="line">    response = response.content</div><div class="line">    soup = BeautifulSoup(response, <span class="string">'lxml'</span>)</div><div class="line">    name = soup.find_all(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>: <span class="string">'name'</span>&#125;)[<span class="number">1</span>].string</div><div class="line">    <span class="comment"># print name</span></div><div class="line">    temp = soup.find(<span class="string">'img'</span>, &#123;<span class="string">'alt'</span>: name&#125;)</div><div class="line">    avatar_url = temp[<span class="string">'src'</span>][<span class="number">0</span>:<span class="number">-6</span>] + temp[<span class="string">'src'</span>][<span class="number">-4</span>:]</div><div class="line">    filename = <span class="string">'pics/'</span> + userId + temp[<span class="string">'src'</span>][<span class="number">-4</span>:]</div><div class="line">    f = open(filename, <span class="string">'wb'</span>)</div><div class="line">    f.write(requests.get(avatar_url).content)</div><div class="line">    f.close()</div></pre></td></tr></table></figure>
<p>结合其他函数，我们就可以抓取到某个答案下所有点赞者的头像，某个大V所有followers的头像等。</p>
<h2 id="抓取某个问题的所有答案"><a href="#抓取某个问题的所有答案" class="headerlink" title="抓取某个问题的所有答案"></a>抓取某个问题的所有答案</h2><p>给出某个唯一ID，下面的函数帮助爬取到该问题下的所有答案。注意，答案内容只抓取文字部分，图片省略，答案保存在txt文件中，txt文件以答主ID命名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_answer</span><span class="params">(questionID)</span>:</span></div><div class="line">    url = <span class="string">'http://www.zhihu.com/question/'</span> + str(questionID)</div><div class="line">    data = s.get(url, headers=header_info)</div><div class="line">    soup = BeautifulSoup(data.content, <span class="string">'lxml'</span>)</div><div class="line">    <span class="comment"># print str(soup).encode('gbk', 'ignore')</span></div><div class="line">    title = soup.title.string.split(<span class="string">'\n'</span>)[<span class="number">2</span>]    <span class="comment"># 问题题目</span></div><div class="line">    path = title</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(path):</div><div class="line">        os.mkdir(path)</div><div class="line">    description = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'zm-editable-content'</span>&#125;).strings    <span class="comment"># 问题描述，可能多行</span></div><div class="line">    file_name = path + <span class="string">'/description.txt'</span></div><div class="line">    fw = open(file_name, <span class="string">'w'</span>)</div><div class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> description:</div><div class="line">        each = each + <span class="string">'\n'</span></div><div class="line">        fw.write(each)</div><div class="line">    <span class="comment"># description = soup.find('div', &#123;'class': 'zm-editable-content'&#125;).get_text() # 问题描述</span></div><div class="line">        <span class="comment"># 调用.string属性返回None（可能是因为有换行符在内的缘故）,调用get_text()方法得到了文本，但换行丢了</span></div><div class="line">    answer_num = int(soup.find(<span class="string">'h3'</span>, &#123;<span class="string">'id'</span>: <span class="string">'zh-question-answer-num'</span>&#125;).string.split(<span class="string">' '</span>)[<span class="number">0</span>]) <span class="comment"># 答案数量</span></div><div class="line">    num = <span class="number">1</span></div><div class="line">    index = soup.find_all(<span class="string">'div'</span>, &#123;<span class="string">'tabindex'</span>: <span class="string">'-1'</span>&#125;)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(index)):</div><div class="line">        <span class="keyword">print</span> (<span class="string">'Scrapying the '</span> + str(num) + <span class="string">'th answer......'</span>).encode(<span class="string">'gbk'</span>, <span class="string">'ignore'</span>)</div><div class="line">        <span class="comment"># print ('正在抓取第' + str(num) + '个答案......').encode('gbk', 'ignore')</span></div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            a = index[i].find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>: <span class="string">'author-link'</span>&#125;)</div><div class="line">            title = str(num) + <span class="string">'__'</span> + a.string</div><div class="line">            href = <span class="string">'http://www.zhihu.com'</span> + a[<span class="string">'href'</span>]</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            title = str(num) + <span class="string">'__匿名用户'</span></div><div class="line">        answer_file_name = path + <span class="string">'/'</span> + title + <span class="string">'__.txt'</span></div><div class="line">        fr = open(answer_file_name, <span class="string">'w'</span>)</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            answer_content = index[i].find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'zm-editable-content clearfix'</span>&#125;).strings</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            answer_content = [<span class="string">'作者修改内容通过后，回答会重新显示。如果一周内未得到有效修改，回答会自动折叠。'</span>]</div><div class="line">        <span class="keyword">for</span> content <span class="keyword">in</span> answer_content:</div><div class="line">            fr.write(content + <span class="string">'\n'</span>)</div><div class="line">        num += <span class="number">1</span></div><div class="line"></div><div class="line">    _xsrf = xsrf</div><div class="line">    url_token = re.findall(<span class="string">'url_token(.*)'</span>, data.content)[<span class="number">0</span>][<span class="number">8</span>:<span class="number">16</span>]</div><div class="line">    <span class="comment"># 循环次数</span></div><div class="line">    n = answer_num/<span class="number">10</span><span class="number">-1</span> <span class="keyword">if</span> answer_num/<span class="number">10.0</span>-answer_num/<span class="number">10</span> == <span class="number">0</span> <span class="keyword">else</span> answer_num/<span class="number">10</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>, <span class="number">1</span>):</div><div class="line">        <span class="comment"># _xsrf = xsrf</span></div><div class="line">        <span class="comment"># url_token = re.findall('url_token(.*)', data.content)[0][8:16]</span></div><div class="line">        offset = <span class="number">10</span>*i</div><div class="line">        params = json.dumps(&#123;<span class="string">"url_token"</span>: url_token, <span class="string">"pagesize"</span>: <span class="number">10</span>, <span class="string">"offset"</span>: offset&#125;)</div><div class="line">        payload = &#123;<span class="string">"method"</span>:<span class="string">"next"</span>, <span class="string">"params"</span>: params, <span class="string">"_xsrf"</span>: _xsrf&#125;</div><div class="line">        click_url = <span class="string">'https://www.zhihu.com/node/QuestionAnswerListV2'</span></div><div class="line">        data = s.post(click_url, data=payload, headers=header_info)</div><div class="line">        data = json.loads(data.content)</div><div class="line">        <span class="keyword">for</span> answer <span class="keyword">in</span> data[<span class="string">'msg'</span>]:</div><div class="line">            <span class="keyword">print</span> (<span class="string">'Scrapying the '</span> + str(num) + <span class="string">'th answer......'</span>).encode(<span class="string">'gbk'</span>, <span class="string">'ignore'</span>)</div><div class="line">            <span class="comment"># print ('正在抓取第' + str(num) + '个答案......').encode('gbk', 'ignore')</span></div><div class="line">            soup1 = BeautifulSoup(answer, <span class="string">'lxml'</span>)</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                a = soup1.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>: <span class="string">'author-link'</span>&#125;)</div><div class="line">                title = str(num) + <span class="string">'__'</span> + a.string</div><div class="line">                href = <span class="string">'http://www.zhihu.com'</span> + a[<span class="string">'href'</span>]</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                title = str(num) + <span class="string">'__匿名用户'</span></div><div class="line">            answer_file_name = path + <span class="string">'/'</span> + title + <span class="string">'__.txt'</span></div><div class="line">            fr = open(answer_file_name, <span class="string">'w'</span>)</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                answer_content = soup1.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'zm-editable-content clearfix'</span>&#125;).strings</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                answer_content = [<span class="string">'作者修改内容通过后，回答会重新显示。如果一周内未得到有效修改，回答会自动折叠。'</span>]</div><div class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> answer_content:</div><div class="line">                fr.write(content + <span class="string">'\n'</span>)</div><div class="line">            num += <span class="number">1</span></div></pre></td></tr></table></figure>
<h2 id="数据库存取数据"><a href="#数据库存取数据" class="headerlink" title="数据库存取数据"></a>数据库存取数据</h2><p>在完成了上面的这些功能后，下一步要做的是将用户信息保存在数据库中，方便数据的读取使用。我刚刚接触了一下sqlite3，仅仅实现了将用户信息存储在表格中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_followeesInfo_toDB</span><span class="params">(userID)</span>:</span></div><div class="line">    <span class="comment"># 准备好sqlite3数据库，当抓取到数据时，加入表格中</span></div><div class="line">    conn = sqlite3.connect(<span class="string">"Zhihu.db"</span>)</div><div class="line">    curs = conn.cursor()</div><div class="line">    curs.execute(<span class="string">"create table if not exists userinfo(name TEXT, ID TEXT PRIMARY KEY, location TEXT, business TEXT, "</span></div><div class="line">                 <span class="string">"gender TEXT, employment TEXT, position TEXT, education TEXT, major TEXT, "</span></div><div class="line">                 <span class="string">"agree INTEGER, thanks INTEGER, asks INTEGER, answers INTEGER, posts INTEGER, "</span></div><div class="line">                 <span class="string">"collections INTEGER, logs INTEGER, followees INTEGER, followers INTEGER, "</span></div><div class="line">                 <span class="string">"scantime INTEGER)"</span>)</div><div class="line">    followees_url = <span class="string">'http://www.zhihu.com/people/'</span> + userID + <span class="string">'/followees'</span></div><div class="line">    file_name = userID + <span class="string">'.txt'</span></div><div class="line">    f = open(file_name, <span class="string">'w'</span>)</div><div class="line">    data = s.get(followees_url, headers=header_info)</div><div class="line">    <span class="keyword">print</span> data  <span class="comment"># 访问服务器成功，返回&lt;responce 200&gt;</span></div><div class="line">    content = data.content  <span class="comment"># 提取出html信息</span></div><div class="line">    soup = BeautifulSoup(content, <span class="string">"lxml"</span>)  <span class="comment"># 对html信息进行解析</span></div><div class="line">    <span class="comment"># 获取关注者数量</span></div><div class="line">    totalsen = soup.select(<span class="string">'span[class*="zm-profile-section-name"]'</span>)</div><div class="line">    total = int(str(totalsen[<span class="number">0</span>]).split(<span class="string">' '</span>)[<span class="number">4</span>])  <span class="comment"># 总的关注者数量</span></div><div class="line">    txt1 = <span class="string">'总的关注者人数：'</span></div><div class="line">    <span class="keyword">print</span> txt1.decode(<span class="string">'utf-8'</span>)</div><div class="line">    <span class="keyword">print</span> total</div><div class="line">    follist = soup.select(<span class="string">'div[class*="zm-profile-card"]'</span>)  <span class="comment"># 记录有关注者信息的list</span></div><div class="line">    num = <span class="number">0</span>  <span class="comment"># 用来在下面显示正在查询第多少个关注者</span></div><div class="line">    <span class="keyword">for</span> follower <span class="keyword">in</span> follist:</div><div class="line">        tag = follower.a</div><div class="line">        title = tag[<span class="string">'title'</span>]  <span class="comment"># 用户名</span></div><div class="line">        href = <span class="string">'http://www.zhihu.com'</span> + str(tag[<span class="string">'href'</span>])  <span class="comment"># 用户地址</span></div><div class="line">        <span class="comment"># 获取用户数据</span></div><div class="line">        num += <span class="number">1</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'%d   %f'</span> % (num, num / float(total))</div><div class="line">        <span class="comment"># Alist = follower.find_all(has_attrs)</span></div><div class="line">        Alist = follower.find_all(<span class="string">'a'</span>, &#123;<span class="string">'target'</span>: <span class="string">'_blank'</span>&#125;)</div><div class="line">        votes = Alist[<span class="number">0</span>].string  <span class="comment"># 点赞者获取的赞同</span></div><div class="line">        tks = Alist[<span class="number">1</span>].string  <span class="comment"># 点赞者获取的感谢</span></div><div class="line">        ques = Alist[<span class="number">2</span>].string  <span class="comment"># 点赞者提出的问题数量</span></div><div class="line">        ans = Alist[<span class="number">3</span>].string  <span class="comment"># 点赞者回答的问题数量</span></div><div class="line">        <span class="comment"># 打印关注者信息</span></div><div class="line">        string = title + <span class="string">'  '</span> + href + <span class="string">'  '</span> + votes + tks + ques + ans</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            <span class="keyword">print</span> string.decode(<span class="string">'utf-8'</span>)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">print</span> string.encode(<span class="string">'gbk'</span>, <span class="string">'ignore'</span>)</div><div class="line">        f.write(string + <span class="string">'\n'</span>)</div><div class="line">        <span class="keyword">if</span> title != <span class="string">'[已重置]'</span>:</div><div class="line">            <span class="comment"># 获取该followee的基本信息，存入数据库表格</span></div><div class="line">            <span class="keyword">print</span> <span class="string">'Analysising the data of this user...'</span></div><div class="line">            ID = href[<span class="number">28</span>:]</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                curs.execute(<span class="string">"insert or ignore into userinfo values (?, ?, ?, ?, ?, ?, ?, "</span></div><div class="line">                             <span class="string">"?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"</span>, get_userInfo(ID))</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                <span class="keyword">print</span> <span class="string">"This user account's state is abnormal..."</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">'This user account has been disabled...'</span></div><div class="line">        <span class="comment"># print get_userInfo(ID)</span></div><div class="line"></div><div class="line">    <span class="comment"># 循环次数</span></div><div class="line">    n = total / <span class="number">20</span> - <span class="number">1</span> <span class="keyword">if</span> total / <span class="number">20.0</span> - total / <span class="number">20</span> == <span class="number">0</span> <span class="keyword">else</span> total / <span class="number">20</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>, <span class="number">1</span>):</div><div class="line">        <span class="comment"># if num%30 == 0:</span></div><div class="line">        <span class="comment">#   time.sleep(1)</span></div><div class="line">        <span class="comment"># if num%50 == 0:</span></div><div class="line">        <span class="comment">#   time.sleep(2)</span></div><div class="line">        raw_hash_id = re.findall(<span class="string">'hash_id(.*)'</span>, content)</div><div class="line">        hash_id = raw_hash_id[<span class="number">0</span>][<span class="number">14</span>:<span class="number">46</span>]</div><div class="line">        _xsrf = xsrf</div><div class="line">        offset = <span class="number">20</span> * i</div><div class="line">        params = json.dumps(&#123;<span class="string">"offset"</span>: offset, <span class="string">"order_by"</span>: <span class="string">"created"</span>, <span class="string">"hash_id"</span>: hash_id&#125;)</div><div class="line">        payload = &#123;<span class="string">"method"</span>: <span class="string">"next"</span>, <span class="string">"params"</span>: params, <span class="string">"_xsrf"</span>: _xsrf&#125;</div><div class="line">        click_url = <span class="string">'http://www.zhihu.com/node/ProfileFolloweesListV2'</span></div><div class="line">        data = s.post(click_url, data=payload, headers=header_info)</div><div class="line">        <span class="comment"># print data</span></div><div class="line">        source = json.loads(data.content)</div><div class="line">        <span class="keyword">for</span> follower <span class="keyword">in</span> source[<span class="string">'msg'</span>]:</div><div class="line">            soup1 = BeautifulSoup(follower, <span class="string">'lxml'</span>)</div><div class="line">            tag = soup1.a</div><div class="line">            title = tag[<span class="string">'title'</span>]  <span class="comment"># 用户名</span></div><div class="line">            href = <span class="string">'http://www.zhihu.com'</span> + str(tag[<span class="string">'href'</span>])  <span class="comment"># 用户地址</span></div><div class="line">            <span class="comment"># 获取用户数据</span></div><div class="line">            num += <span class="number">1</span></div><div class="line">            <span class="keyword">print</span> <span class="string">'%d   %f'</span> % (num, num / float(total))</div><div class="line">            <span class="comment"># Alist = soup1.find_all(has_attrs)</span></div><div class="line">            Alist = soup1.find_all(<span class="string">'a'</span>, &#123;<span class="string">'target'</span>: <span class="string">'_blank'</span>&#125;)</div><div class="line">            votes = Alist[<span class="number">0</span>].string  <span class="comment"># 点赞者获取的赞同</span></div><div class="line">            tks = Alist[<span class="number">1</span>].string  <span class="comment"># 点赞者获取的感谢</span></div><div class="line">            ques = Alist[<span class="number">2</span>].string  <span class="comment"># 点赞者提出的问题数量</span></div><div class="line">            ans = Alist[<span class="number">3</span>].string  <span class="comment"># 点赞者回答的问题数量</span></div><div class="line">            <span class="comment"># 打印关注者信息</span></div><div class="line">            string = title + <span class="string">'  '</span> + href + <span class="string">'  '</span> + votes + tks + ques + ans</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                <span class="keyword">print</span> string.decode(<span class="string">'utf-8'</span>)</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                <span class="keyword">print</span> string.encode(<span class="string">'gbk'</span>, <span class="string">'ignore'</span>)</div><div class="line">            f.write(string + <span class="string">'\n'</span>)</div><div class="line">            <span class="keyword">if</span> title != <span class="string">'[已重置]'</span>:</div><div class="line">                <span class="comment"># 获取该followee的基本信息，存入数据库表格</span></div><div class="line">                <span class="keyword">print</span> <span class="string">'Analysising the data of this user...'</span></div><div class="line">                ID = href[<span class="number">28</span>:]</div><div class="line">                <span class="keyword">try</span>:</div><div class="line">                    curs.execute(<span class="string">"insert or ignore into userinfo values (?, ?, ?, ?, ?, ?, ?, "</span></div><div class="line">                             <span class="string">"?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"</span>, get_userInfo(ID))</div><div class="line">                <span class="keyword">except</span>:</div><div class="line">                    <span class="keyword">print</span> <span class="string">"This user account's state is abnormal..."</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">print</span> <span class="string">'This user account has been disabled...'</span></div><div class="line">            <span class="comment"># print get_userInfo(ID)</span></div><div class="line">    f.close()</div><div class="line">    conn.commit()</div><div class="line">    conn.close()</div></pre></td></tr></table></figure>
<p>等熟悉了sqlite3的使用，我的下一步工作是抓取大量用户信息和用户之间的follow信息，尝试着将大V间的follow关系进行可视化。再下面的工作应该就是学习python的爬虫框架scrapy和爬取微博了。</p>
<p>另外，在写这篇博客的时候我又重新测试了一下上面的这些函数，然后我再在火狐上访问知乎时，系统提示“因为该账户过度频繁访问”而要求输入验证码，看来知乎已经开始限制爬虫了，这样以来我们就需要使用一些反反爬虫技巧了，比如控制访问频率等等，这个等以后有了系统的了解之后再作补充吧。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/08/18/Python爬虫爬取知乎小结/" data-id="cjfguskg5000ucgurt8ze6xf3" class="article-share-link">分享到</a><div class="tags"><a href="/tags/爬虫/">爬虫</a><a href="/tags/Python/">Python</a></div><div class="post-nav"><a href="/2016/11/11/My-reading-list/" class="pre">My reading list</a><a href="/2016/07/20/Theano实现kaggle手写识别/" class="next">Theano实现kaggle手写识别</a></div><div data-thread-key="2016/08/18/Python爬虫爬取知乎小结/" data-title="Python爬虫爬取知乎小结" data-url="http://marcovaldong.github.io/2016/08/18/Python爬虫爬取知乎小结/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/08/18/Python爬虫爬取知乎小结/" data-title="Python爬虫爬取知乎小结" data-url="http://marcovaldong.github.io/2016/08/18/Python爬虫爬取知乎小结/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>