<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>Theano实现kaggle手写识别 | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Theano实现kaggle手写识别</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Theano实现kaggle手写识别</h1><div class="post-meta">Jul 20, 2016<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/07/20/Theano实现kaggle手写识别/" href="/2016/07/20/Theano实现kaggle手写识别/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression"><span class="toc-number">1.</span> <span class="toc-text">Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#结论"><span class="toc-number">1.1.</span> <span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MLP"><span class="toc-number">2.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LeNet5"><span class="toc-number">3.</span> <span class="toc-text">LeNet5</span></a></li></ol></div></div><div class="post-content"><p>前面写了使用theano来实现kaggle中的手写识别，分别用的是逻辑回归和多层感知机，这次加上卷积神经网络，将三者放在一块儿做个总结。下一步的工作是继续系统学习theano及其他深度学习工具，自己去实现更多模型和优化方法。</p>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>首先是逻辑回归，这里我用了个类似十折交叉验证的方法：将train.csv中的数据分成了十份，最后一份作为模型训练过程中的测试集，每次训练时选取前九份中的一份作为验证集，其余八份作为训练集，这样我们就可以得到同一个模型的九组参数设置。在预测阶段，前面得到的九组参数设置同时参与预测，具体做法是，对于同一组测试数据，我们分别用九组参数设置去预测，得到九个预测值，我们将这九个预测值中出现次数最多的那个label作为改组测试数据的最终预测值。<br><a id="more"></a><br>下面是代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_cross_train</span><span class="params">(file, partion)</span>:</span></div><div class="line">    rawData = []</div><div class="line">    file = csv.reader(open(file))</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</div><div class="line">        rawData.append(line)</div><div class="line">    rawData.pop(<span class="number">0</span>)  <span class="comment"># remove the first line which is the label line</span></div><div class="line">    data = numpy.array(rawData).astype(numpy.int32)</div><div class="line">    <span class="comment"># 数据已经存到data变量中</span></div><div class="line">    <span class="comment"># 下面将整个数据集分成train/validation/test三部分，其中validation为数据集的第partion部分，test为数据集的最后百分之十</span></div><div class="line">    train_x = [];   train_y = []</div><div class="line">    valid_x = [];   valid_y = []</div><div class="line">    test_x = [];    test_y = []</div><div class="line">    <span class="comment"># validation set</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(partion*<span class="number">4200</span>, (partion+<span class="number">1</span>)*<span class="number">4200</span>):</div><div class="line">        valid_x.append(data[i][<span class="number">1</span>:<span class="number">785</span>])</div><div class="line">        valid_y.append(data[i][<span class="number">0</span>])</div><div class="line">    data = numpy.delete(data, [i <span class="keyword">for</span> i <span class="keyword">in</span> range(partion*<span class="number">4200</span>, (partion+<span class="number">1</span>)*<span class="number">4200</span>)], <span class="number">0</span>)</div><div class="line">    <span class="comment"># train set</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">33600</span>):</div><div class="line">        train_x.append(data[i][<span class="number">1</span>:<span class="number">785</span>])</div><div class="line">        train_y.append(data[i][<span class="number">0</span>])</div><div class="line">    <span class="comment"># test set</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">33600</span>, <span class="number">37800</span>):</div><div class="line">        test_x.append(data[i][<span class="number">1</span>:<span class="number">785</span>])</div><div class="line">        test_y.append(data[i][<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="keyword">del</span> data</div><div class="line">    train_x, train_y = shared_dataset((train_x, train_y))</div><div class="line">    valid_x, valid_y = shared_dataset((valid_x, valid_y))</div><div class="line">    test_x, test_y = shared_dataset((test_x, test_y))</div><div class="line">    rval = [(train_x, train_y), (valid_x, valid_y),</div><div class="line">            (test_x, test_y)]</div><div class="line">    <span class="keyword">return</span> rval</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_test</span><span class="params">(file)</span>:</span></div><div class="line">    rawData = []</div><div class="line">    file = csv.reader(open(file))</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</div><div class="line">        rawData.append(line)</div><div class="line">    rawData.pop(<span class="number">0</span>)  <span class="comment"># the first line is title</span></div><div class="line">    data = numpy.array(rawData).astype(numpy.int32)</div><div class="line">    test_set_x = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</div><div class="line">        test_set_x.append(data[i])</div><div class="line">    <span class="keyword">del</span> data</div><div class="line">    shared_data = theano.shared(numpy.asarray(test_set_x), borrow=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> shared_data</div></pre></td></tr></table></figure>
<p>下面是逻辑回归类，这个类来自<a href="http://deeplearning.net/tutorial/" target="_blank" rel="external">Deep Learning Tutorials</a>中的<a href="http://deeplearning.net/tutorial/logreg.html#logreg" target="_blank" rel="external">Logistic Regression</a>，未作改动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Multi-class logistic regression class</div><div class="line">    The logistic regression is fully described by a weight matrix :math:'W'</div><div class="line">    and bias vector :math:'b'. Classification is done by projecting data</div><div class="line">    points onto a set of hyperplanes, the distance to which is used to</div><div class="line">    determine to a class membership probability.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input, n_in, n_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Initialize the parameters of the logistic regression</div><div class="line">        :type input: theano.tensor.TensorType</div><div class="line">        :param input: symbolic variable that describes the input of the</div><div class="line">                        architecture (one minibatch)</div><div class="line">        :type n_in: int</div><div class="line">        :param n_in: number of input units, the dimension of the space in</div><div class="line">                        which the datapoints lie</div><div class="line">        :type n_out: int</div><div class="line">        :param n_out: number of output units, the dimension of the space in</div><div class="line">                        which the labels lie</div><div class="line">        """</div><div class="line">        <span class="comment"># start-snippet-1</span></div><div class="line">        <span class="comment"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span></div><div class="line">        self.W = theano.shared(value=numpy.zeros((n_in, n_out),</div><div class="line">                               dtype=theano.config.floatX), name=<span class="string">'W'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        <span class="comment"># initialize the biases b as a vector of n_out 0s</span></div><div class="line">        self.b = theano.shared(value=numpy.zeros((n_out,),</div><div class="line">                                dtype=theano.config.floatX), name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">        <span class="comment"># symbolic expression for computing the matrix of class-membership</span></div><div class="line">        <span class="comment"># probabilities</span></div><div class="line">        <span class="comment"># Where:</span></div><div class="line">        <span class="comment"># W is a matrix where column-k represent the separation hyperplane for</span></div><div class="line">        <span class="comment"># class-k</span></div><div class="line">        <span class="comment"># x is a matrix where row-j represents input training sample-j</span></div><div class="line">        <span class="comment"># b is a vector where element-k represent the free parameter of hyperplane-k</span></div><div class="line">        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)</div><div class="line"></div><div class="line">        <span class="comment"># symbolic description of how to compute prediction as class whose</span></div><div class="line">        <span class="comment"># probability is matrix</span></div><div class="line">        self.y_pred = T.argmax(self.p_y_given_x, axis=<span class="number">1</span>)</div><div class="line">        <span class="comment"># end-snippet-1</span></div><div class="line"></div><div class="line">        <span class="comment"># parameters of the model</span></div><div class="line">        self.params = [self.W, self.b]</div><div class="line"></div><div class="line">        <span class="comment"># keep track of model input</span></div><div class="line">        self.input = input</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">negative_log_likelihood</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Return the mean of the negative log-likelihood of the prediction of</div><div class="line">        this model under a given target distribution.</div><div class="line"></div><div class="line">        :type y: theano.tensor.TensorType</div><div class="line">        :param y: corresponds to a vector that gives for each example the</div><div class="line">                    correct label</div><div class="line">        Note: we use the mean instead of the sum so that the learning rate</div><div class="line">                is less dependent on the batch size</div><div class="line">        """</div><div class="line">        <span class="comment"># start-snippet-2</span></div><div class="line">        <span class="comment"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</span></div><div class="line">        <span class="comment"># number of examples (call it n) in the minibatch</span></div><div class="line">        <span class="comment"># T.arange(y.shape[0]) is a symbolic vector which will contain</span></div><div class="line">        <span class="comment"># [0, 1, 2,..., n-1] T.log(self.p_y_given_x) is a matrix of</span></div><div class="line">        <span class="comment"># Log-Probabilities (call it LP) with one row per example and</span></div><div class="line">        <span class="comment"># one column per class LP[T.arange(y.shape[0]),y] is a vector</span></div><div class="line">        <span class="comment"># v containing [LP[0, y[0]], LP[1, y[1]], LP[2, y[2]], ...,</span></div><div class="line">        <span class="comment"># LP[n-1, y[n-1]]] and T.mean(LP[T.arange(y.shape[0]), y]) is</span></div><div class="line">        <span class="comment"># the mean (across minibatch examples) of the elements in v, i.e.,</span></div><div class="line">        <span class="comment"># the mean log-likelihood across the minibatch.</span></div><div class="line">        <span class="keyword">return</span> -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[<span class="number">0</span>]), y])</div><div class="line">        <span class="comment"># end-snippet-2</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">errors</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"""Return a float representing the number of errors in the minibatch</span></div><div class="line">         over the total number of examples of the minibatch; zero one</div><div class="line">         loss over the size of the minibatch</div><div class="line">        :type y: theano.tensor.TensorType</div><div class="line">        :param y: corresponds to a vector that gives for each example the correct label</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="comment"># check if y has same dimension of y_pred</span></div><div class="line">        <span class="keyword">if</span> y.ndim != self.y_pred.ndim:</div><div class="line">            <span class="keyword">raise</span> TypeError(</div><div class="line">                <span class="string">'y should have the same shape as self.y_pred'</span>,</div><div class="line">                (<span class="string">'y'</span>, y.type, <span class="string">'y_pred'</span>, self.y_pred.type)</div><div class="line">            )</div><div class="line">        <span class="comment"># check if y is of the correct datatype</span></div><div class="line">        <span class="keyword">if</span> y.dtype.startswith(<span class="string">'int'</span>):</div><div class="line">            <span class="comment"># the T.neq operator returns a vector of 0s and 1s, where a</span></div><div class="line">            <span class="comment"># represents a mistake in prediction</span></div><div class="line">            <span class="keyword">return</span> T.mean(T.neq(self.y_pred, y))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">raise</span> NotImplementedError()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shared_dataset</span><span class="params">(data_xy, borrow=True)</span>:</span></div><div class="line">    <span class="string">""" Function that loads the dataset into shared variables</span></div><div class="line"></div><div class="line">    The reason we store our dataset in shared variables is to allow</div><div class="line">    Theano to copy it into the GPU memory (when code is run on GPU).</div><div class="line">    Since copying data into the GPU is slow, copying a minibatch everytime</div><div class="line">    is needed (the default behaviour if the data is not in a shared</div><div class="line">    variable) would lead to a large decrease in performance.</div><div class="line">    """</div><div class="line">    data_x, data_y = data_xy</div><div class="line">    shared_x = theano.shared(numpy.asarray(data_x), borrow=borrow)</div><div class="line">    shared_y = theano.shared(numpy.asarray(data_y), borrow=borrow)</div><div class="line">    <span class="comment"># When storing data on the GPU it has to be stored as floats</span></div><div class="line">    <span class="comment"># therefore we will store the labels as ``floatX`` as well</span></div><div class="line">    <span class="comment"># (``shared_y`` does exactly that). But during our computations</span></div><div class="line">    <span class="comment"># we need them as ints (we use labels as index, and if they are</span></div><div class="line">    <span class="comment"># floats it doesn't make sense) therefore instead of returning</span></div><div class="line">    <span class="comment"># ``shared_y`` we will have to cast it to int. This little hack</span></div><div class="line">    <span class="comment"># lets ous get around this issue</span></div><div class="line">    <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">'int32'</span>)</div></pre></td></tr></table></figure>
<p>然后是模型训练函数，由<a href="http://deeplearning.net/tutorial/" target="_blank" rel="external">Deep Learning Tutorials</a>中的<a href="http://deeplearning.net/tutorial/logreg.html#logreg" target="_blank" rel="external">Logistic Regression</a>中的模型训练函数改造而来，其中参数多了一个partion，用来指定train.csv中的哪一部分作为此次训练的验证集。</p>
<p>其中参数的选择我们会在实验结论部分给出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_optimization_mnist</span><span class="params">(learning_rate=<span class="number">0.13</span>, n_epochs=<span class="number">1000</span>,</span></span></div><div class="line">                               data_path=<span class="string">r'train.csv'</span>,</div><div class="line">                               save_path=<span class="string">r'my_best_model.pkl'</span>, partion=<span class="number">8</span>, batch_size=<span class="number">800</span>):</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># load dataset</span></div><div class="line">    <span class="comment"># datasets = read_raw_train(data_path)</span></div><div class="line">    datasets = read_cross_train(data_path, partion)</div><div class="line">    train_set_x, train_set_y = datasets[<span class="number">0</span>]</div><div class="line">    valid_set_x, valid_set_y = datasets[<span class="number">1</span>]</div><div class="line">    test_set_x, test_set_y = datasets[<span class="number">2</span>]</div><div class="line"></div><div class="line">    <span class="comment"># compute number of minibatches for training, validation and testing</span></div><div class="line">    n_train_batches = train_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>] // batch_size  <span class="comment"># 25200 // batch_size</span></div><div class="line">    n_valid_batches = valid_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>] // batch_size  <span class="comment"># 12600 // batch_size</span></div><div class="line">    n_test_batches = test_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>] // batch_size  <span class="comment"># 4200 // batch_size</span></div><div class="line">    <span class="comment">######################</span></div><div class="line">    <span class="comment"># BUILD ACTUAL MODEL #</span></div><div class="line">    <span class="comment">######################</span></div><div class="line">    print(<span class="string">'...building the model'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># allocate symbolic variables for the data</span></div><div class="line">    index = T.lscalar()  <span class="comment"># index to a [mini]batch</span></div><div class="line"></div><div class="line">    <span class="comment"># generate symbolic variables for input (x and y represent a minibatch)</span></div><div class="line">    x = T.imatrix(<span class="string">'x'</span>)  <span class="comment"># data, presented as rasterized images</span></div><div class="line">    y = T.ivector(<span class="string">'y'</span>)  <span class="comment"># labels, presented as 1D vector of [int] labels</span></div><div class="line"></div><div class="line">    <span class="comment"># construct the logistic regression class</span></div><div class="line">    <span class="comment"># Each MNIST image has size 28*28</span></div><div class="line">    classifier = LogisticRegression(input=x, n_in=<span class="number">28</span> * <span class="number">28</span>, n_out=<span class="number">10</span>)</div><div class="line">    <span class="comment"># the cost we minimize during training is the negative log likelihood of</span></div><div class="line">    <span class="comment"># the model in symbolic format</span></div><div class="line">    cost = classifier.negative_log_likelihood(y)</div><div class="line"></div><div class="line">    <span class="comment"># compiling a Theano function that computes the mistakes that are made by</span></div><div class="line">    <span class="comment"># the model on a minibatch</span></div><div class="line">    test_model = theano.function(</div><div class="line">        inputs=[index],</div><div class="line">        outputs=classifier.errors(y),</div><div class="line">        givens=&#123;</div><div class="line">            x: test_set_x[index * batch_size: (index + <span class="number">1</span>) * batch_size],</div><div class="line">            y: test_set_y[index * batch_size: (index + <span class="number">1</span>) * batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    validate_model = theano.function(</div><div class="line">        inputs=[index],</div><div class="line">        outputs=classifier.errors(y),</div><div class="line">        givens=&#123;</div><div class="line">            x: valid_set_x[index * batch_size: (index + <span class="number">1</span>) * batch_size],</div><div class="line">            y: valid_set_y[index * batch_size: (index + <span class="number">1</span>) * batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment"># compute the gradient of cost with respect to theta = (W,b)</span></div><div class="line">    g_W = T.grad(cost=cost, wrt=classifier.W)</div><div class="line">    g_b = T.grad(cost=cost, wrt=classifier.b)</div><div class="line"></div><div class="line">    <span class="comment"># specify how to update the parameters of the model as a list of</span></div><div class="line">    <span class="comment"># (variable, update expression) pairs.</span></div><div class="line">    updates = [(classifier.W, classifier.W - learning_rate * g_W),</div><div class="line">               (classifier.b, classifier.b - learning_rate * g_b)]</div><div class="line"></div><div class="line">    <span class="comment"># compiling a Theano function `train_model` that returns the cost, but in</span></div><div class="line">    <span class="comment"># the same time updates the parameter of the model based on the rules</span></div><div class="line">    <span class="comment"># defined in `updates`</span></div><div class="line">    train_model = theano.function(</div><div class="line">        inputs=[index],</div><div class="line">        outputs=cost,</div><div class="line">        updates=updates,</div><div class="line">        givens=&#123;</div><div class="line">            x: train_set_x[index * batch_size: (index + <span class="number">1</span>) * batch_size],</div><div class="line">            y: train_set_y[index * batch_size: (index + <span class="number">1</span>) * batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment">###############</span></div><div class="line">    <span class="comment"># TRAIN MODEL #</span></div><div class="line">    <span class="comment">###############</span></div><div class="line">    print(<span class="string">'... training the model'</span>)</div><div class="line"></div><div class="line">    patience = <span class="number">5000</span>  <span class="comment"># look as this many examples regardless</span></div><div class="line">    patience_increase = <span class="number">2</span>  <span class="comment"># wait this much longer when a new best is</span></div><div class="line">    <span class="comment"># found</span></div><div class="line">    improvement_threshold = <span class="number">0.995</span>  <span class="comment"># a relative improvement of this much is</span></div><div class="line">    <span class="comment"># considered significant</span></div><div class="line">    validation_frequency = min(n_train_batches, patience // <span class="number">2</span>)</div><div class="line">    <span class="comment"># go through this many</span></div><div class="line">    <span class="comment"># minibatche before checking the network</span></div><div class="line">    <span class="comment"># on the validation set; in this case we</span></div><div class="line">    <span class="comment"># check every epoch</span></div><div class="line"></div><div class="line">    best_validation_loss = numpy.inf</div><div class="line">    best_test_loss = numpy.inf</div><div class="line">    test_score = <span class="number">0.</span></div><div class="line">    start_time = timeit.default_timer()</div><div class="line">    done_looping = <span class="keyword">False</span></div><div class="line">    epoch = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> (epoch &lt; n_epochs) <span class="keyword">and</span> (<span class="keyword">not</span> done_looping):</div><div class="line">        epoch += <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> range(n_train_batches):</div><div class="line">            minibatch_avg_cost = train_model(minibatch_index)</div><div class="line">            <span class="comment"># iteration number</span></div><div class="line">            iter = (epoch - <span class="number">1</span>) * n_train_batches + minibatch_index</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (iter + <span class="number">1</span>) % validation_frequency == <span class="number">0</span>:</div><div class="line">                <span class="comment"># compute zero-one loss on validation set</span></div><div class="line">                validation_losses = [validate_model(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_valid_batches)]</div><div class="line">                this_validation_loss = numpy.mean(validation_losses)</div><div class="line">                print(<span class="string">'epoch %i, minibatch %i/%i, validation error %f %%'</span> % (</div><div class="line">                    epoch, minibatch_index + <span class="number">1</span>, n_train_batches, this_validation_loss * <span class="number">100.</span>))</div><div class="line"></div><div class="line">                <span class="comment"># if we got the best validation score until now</span></div><div class="line">                <span class="keyword">if</span> this_validation_loss &lt; best_validation_loss:</div><div class="line">                    <span class="comment"># improve patience if loss improvement is good enough</span></div><div class="line">                    <span class="keyword">if</span> this_validation_loss &lt; best_validation_loss * \</div><div class="line">                            improvement_threshold:</div><div class="line">                        patience = max(patience, iter * patience_increase)</div><div class="line"></div><div class="line">                    best_validation_loss = this_validation_loss</div><div class="line">                    <span class="comment"># test it on the test set</span></div><div class="line">                    test_losses = [test_model(i)</div><div class="line">                                   <span class="keyword">for</span> i <span class="keyword">in</span> range(n_test_batches)]</div><div class="line">                    test_score = numpy.mean(test_losses)</div><div class="line"></div><div class="line">                    print(</div><div class="line">                        (<span class="string">'     epoch %i, minibatch %i/%i, test error of best model %f %%'</span>)</div><div class="line">                        % (epoch, minibatch_index + <span class="number">1</span>, n_train_batches, test_score * <span class="number">100.</span>))</div><div class="line">                    <span class="comment"># save the best model</span></div><div class="line">                    <span class="keyword">with</span> open(save_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">                        pickle.dump(classifier, f)</div><div class="line">            <span class="keyword">if</span> patience &lt; iter:</div><div class="line">                done_looping = <span class="keyword">True</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">    end_time = timeit.default_timer()</div><div class="line"></div><div class="line">    print(</div><div class="line">        (<span class="string">'Optimization complete with best validation score of %f %%, with test performance %f %%'</span>)</div><div class="line">        % (best_validation_loss * <span class="number">100.</span>, test_score * <span class="number">100.</span>))</div><div class="line">    print(<span class="string">'The code run for %d epochs, with %f epochs/sec'</span> % (</div><div class="line">        epoch, <span class="number">1.</span> * epoch / (end_time - start_time)))</div><div class="line">    print((<span class="string">'The code for file '</span> +</div><div class="line">           os.path.split(__file__)[<span class="number">1</span>] +</div><div class="line">           <span class="string">' ran for %.1fs'</span> % ((end_time - start_time))), file=sys.stderr)</div></pre></td></tr></table></figure>
<p>下面是预测函数，该函数有两个参数，参数data_path表示测试集所在文件，参数has_label表示测试集中是否含有label，有label的话预测函数会计算出错误率，没有label的话函数会将测试结果保存在文件answer.csv中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_cross</span><span class="params">(has_label=<span class="number">1</span>, data_path=<span class="string">r'train.csv'</span>)</span>:</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> has_label == <span class="number">1</span>:</div><div class="line">        <span class="comment"># load test set</span></div><div class="line">        <span class="comment"># data_path = r'E:\Lab\digitrecognizer\train.csv'</span></div><div class="line">        test_set_x, test_set_y = read_train(data_path)</div><div class="line">        test_set_x = test_set_x.get_value()</div><div class="line">        test_set_y = test_set_y.get_value()</div><div class="line"></div><div class="line">        <span class="comment"># from different pkl document load different model</span></div><div class="line">        classifiers = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">            model_path = <span class="string">'model'</span> + str(i) + <span class="string">'.pkl'</span></div><div class="line">            classifier = pickle.load(open(model_path))</div><div class="line">            classifiers.append(classifier)</div><div class="line"></div><div class="line">        <span class="comment"># compile 9 predictor functions with different params</span></div><div class="line">        models = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">            model = theano.function(inputs=[classifiers[i].input], outputs=classifiers[i].y_pred)</div><div class="line">            models.append(model)</div><div class="line"></div><div class="line">        <span class="comment"># 做预测</span></div><div class="line">        predicted_labels = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">            predicted = models[i](test_set_x[:<span class="number">42000</span>])</div><div class="line">            predicted_labels.append(predicted)</div><div class="line">        predicted_label = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">42000</span>):</div><div class="line">            dictionary = &#123;<span class="string">'0'</span>: <span class="number">0</span>, <span class="string">'1'</span>: <span class="number">0</span>, <span class="string">'2'</span>: <span class="number">0</span>, <span class="string">'3'</span>: <span class="number">0</span>, <span class="string">'4'</span>: <span class="number">0</span>, <span class="string">'5'</span>: <span class="number">0</span>, <span class="string">'6'</span>: <span class="number">0</span>, <span class="string">'7'</span>: <span class="number">0</span>, <span class="string">'8'</span>: <span class="number">0</span>, <span class="string">'9'</span>: <span class="number">0</span>&#125;</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">                dictionary[str(predicted_labels[j][i])] += <span class="number">1</span></div><div class="line"></div><div class="line">            <span class="comment"># 字典处理，找到被9个模型预测最多的那个label作为该组输入的输出</span></div><div class="line">            label_number = max(dictionary.values())</div><div class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> dictionary:</div><div class="line">                <span class="keyword">if</span> dictionary[key] == label_number:</div><div class="line">                    predicted_label.append(int(key))</div><div class="line">                    <span class="keyword">break</span></div><div class="line">        err = <span class="number">0.</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">42000</span>):</div><div class="line">            <span class="comment"># print("The %d th example's predict is %d, and it's target value is %d." % (i, predicted_label[i], test_set_y[i]))</span></div><div class="line">            <span class="keyword">if</span> predicted_label[i] != test_set_y[i]:</div><div class="line">                err += <span class="number">1</span></div><div class="line"></div><div class="line">        print(<span class="string">"The error rate of the combined model in 1000 examples is %f ."</span> % (err/<span class="number">420</span>))</div><div class="line">        errorate = [] <span class="comment"># [8.875, 8.800, 8.925, 8.975, 8.375, 8.300, 8.925, 9.225, 8.925]</span></div><div class="line">        <span class="comment"># 分别计算九个模型在整个训练集上的错误率</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">            err = <span class="number">0.</span></div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">42000</span>):</div><div class="line">                <span class="keyword">if</span> predicted_labels[i][j] != test_set_y[j]:</div><div class="line">                    err += <span class="number">1</span></div><div class="line">            errorate.append(err/<span class="number">420</span>)</div><div class="line">        print(<span class="string">"The mean error rate of 9 different models is %f"</span> % numpy.mean(errorate))</div><div class="line">        print(errorate)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># load test set</span></div><div class="line">        test_set_x = read_test(data_path)</div><div class="line">        test_set_x = test_set_x.get_value()</div><div class="line">        <span class="comment"># from different pkl document load different model</span></div><div class="line">        classifiers = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">            model_path = <span class="string">'model'</span> + str(i) + <span class="string">'.pkl'</span></div><div class="line">            classifier = pickle.load(open(model_path))</div><div class="line">            classifiers.append(classifier)</div><div class="line"></div><div class="line">        <span class="comment"># compile 9 predictor functions with different params</span></div><div class="line">        models = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">            model = theano.function(inputs=[classifiers[i].input], outputs=classifiers[i].y_pred)</div><div class="line">            models.append(model)</div><div class="line"></div><div class="line">        <span class="comment"># 做预测</span></div><div class="line">        predicted_labels = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">            predicted = models[i](test_set_x[:<span class="number">28000</span>])</div><div class="line">            predicted_labels.append(predicted)</div><div class="line">        predicted_label = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">28000</span>):</div><div class="line">            dictionary = &#123;<span class="string">'0'</span>: <span class="number">0</span>, <span class="string">'1'</span>: <span class="number">0</span>, <span class="string">'2'</span>: <span class="number">0</span>, <span class="string">'3'</span>: <span class="number">0</span>, <span class="string">'4'</span>: <span class="number">0</span>, <span class="string">'5'</span>: <span class="number">0</span>, <span class="string">'6'</span>: <span class="number">0</span>, <span class="string">'7'</span>: <span class="number">0</span>, <span class="string">'8'</span>: <span class="number">0</span>, <span class="string">'9'</span>: <span class="number">0</span>&#125;</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">                dictionary[str(predicted_labels[j][i])] += <span class="number">1</span></div><div class="line"></div><div class="line">            <span class="comment"># 字典处理，找到被9个模型预测最多的那个label作为该组输入的输出</span></div><div class="line">            label_number = max(dictionary.values())</div><div class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> dictionary:</div><div class="line">                <span class="keyword">if</span> dictionary[key] == label_number:</div><div class="line">                    predicted_label.append(int(key))</div><div class="line">                    <span class="keyword">break</span></div><div class="line">        <span class="comment"># print(predicted_label)</span></div><div class="line">        <span class="comment"># print(len(predicted_label))</span></div><div class="line">        saveResult(predicted_label, <span class="string">r'answer.csv'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveResult</span><span class="params">(result, file)</span>:</span></div><div class="line">    <span class="keyword">with</span> open(file, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">        <span class="comment"># file = open(f, 'wb')</span></div><div class="line">        ob = csv.writer(f)</div><div class="line">        ob.writerow([<span class="string">"ImageId"</span>, <span class="string">"Label"</span>])</div><div class="line">        ids = range(<span class="number">1</span>, len(result)+<span class="number">1</span>)</div><div class="line">        ob.writerows(zip(ids, result))</div><div class="line">        f.close()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(file1, file2)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># load the saved model</span></div><div class="line">    classifier = pickle.load(open(file1))</div><div class="line">    <span class="comment"># compile a predictor function</span></div><div class="line">    predict_model = theano.function(inputs=[classifier.input], outputs=classifier.y_pred)</div><div class="line"></div><div class="line">    <span class="comment"># make prediction</span></div><div class="line">    data_path = <span class="string">r'test.csv'</span></div><div class="line">    test_set_x= read_test(data_path)</div><div class="line">    test_set_x = test_set_x.get_value()</div><div class="line">    predicted_values = predict_model(test_set_x[:<span class="number">28000</span>])</div><div class="line">    <span class="comment"># 保存预测结果</span></div><div class="line">    saveResult(predicted_values, file2)</div></pre></td></tr></table></figure>
<p>最后是主函数，如下所示，当然我们可以根据自己的实际需要去做调整。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    start_time = timeit.default_timer()</div><div class="line"></div><div class="line">    <span class="comment"># 训练出9个模型</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">        save_model_path = <span class="string">r'model'</span> + str(i) + <span class="string">'.pkl'</span></div><div class="line">        sgd_optimization_mnist(learning_rate=<span class="number">0.13</span>, n_epochs=<span class="number">300</span>,</div><div class="line">                              data_path=<span class="string">r'train.csv'</span>,</div><div class="line">                              save_path=save_model_path, partion=i, batch_size=<span class="number">1600</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 在train集上检验模型</span></div><div class="line">    predict_cross(has_label=<span class="number">1</span>, data_path=<span class="string">r'train.csv'</span>)</div><div class="line">    <span class="comment"># 在test集上做预测，并将最终结果存到文件answer.csv中</span></div><div class="line">    predict_cross(has_label=<span class="number">0</span>, data_path=<span class="string">r'test.csv'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 使用单独的模型在test集上做测试，并保存测试结果</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</div><div class="line">        model_path = <span class="string">r'model'</span> + str(i) + <span class="string">'.pkl'</span></div><div class="line">        save_path = <span class="string">r'answer'</span> + str(i) + <span class="string">'.csv'</span></div><div class="line">        predict(model_path, save_path)</div><div class="line"></div><div class="line">    <span class="comment"># 然后分别在kaggle上提交十组结果，观察正确率如何</span></div><div class="line"></div><div class="line">    <span class="comment"># sgd_optimization_mnist(learning_rate=0.13, n_epochs=1000,data_path=r'E:\Lab\digitrecognizer\train.csv', save_path=r'E:\Lab\digitrecognizer\my_best_model.pkl', partion=8, batch_size=1400)</span></div><div class="line">    end_time = timeit.default_timer()</div><div class="line">    print(<span class="string">"Time consumption is %f sec."</span> % (end_time-start_time))</div></pre></td></tr></table></figure>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>在参数的选择上，我们做了多组测试（该测试是选择train.csv中的前3/5座训练集，最后1/10做测试集，中间3/10做验证集得到的），最终选定了上面的参数，下面给出了一组对照数据。测试的机器是12年的，CPU为i3-2350M，主频为2。30GHZ，内存为6G，无GPU（机器太渣，没办法）。在写这篇博客时，我又试验了一下bacth_size大于1000的几种情况，等后续试验数据得出后，如果有更小的错误率，我会更新数据。</p>
<table>
<thead>
<tr>
<th>batch_size</th>
<th>learning_rate</th>
<th>epoches</th>
<th>seconds</th>
<th>epoches/sec</th>
<th>valid_err</th>
<th>test_err</th>
</tr>
</thead>
<tbody>
<tr>
<td>300</td>
<td>0.13</td>
<td>126</td>
<td>2207</td>
<td>0.057</td>
<td>9.786</td>
<td>9.904</td>
</tr>
<tr>
<td>500</td>
<td>0.13</td>
<td>270</td>
<td>2736</td>
<td>0.097</td>
<td>10.064</td>
<td>9.700</td>
</tr>
<tr>
<td>800</td>
<td>0.13</td>
<td>162</td>
<td>1186</td>
<td>0.137</td>
<td>9.767</td>
<td>9.425</td>
</tr>
<tr>
<td>800</td>
<td>0.3</td>
<td>394</td>
<td>2922</td>
<td>0.135</td>
<td>9.98</td>
<td>9.875</td>
</tr>
<tr>
<td>800</td>
<td>0.03</td>
<td>202</td>
<td>1369</td>
<td>0.147</td>
<td>10.083</td>
<td>10.075</td>
</tr>
<tr>
<td>1000</td>
<td>0.13</td>
<td>201</td>
<td>1205</td>
<td>0.167</td>
<td>9.792</td>
<td>9.475</td>
</tr>
<tr>
<td>1400</td>
<td>0.13</td>
<td>408</td>
<td>2281</td>
<td>0.1789</td>
<td>9.6111</td>
<td>9.500</td>
</tr>
<tr>
<td>1600</td>
<td>0.13</td>
<td>334</td>
<td>1590</td>
<td>0.2101</td>
<td>9.089</td>
<td>8.4375</td>
</tr>
<tr>
<td>3200</td>
<td>0.13</td>
<td>714</td>
<td>1974</td>
<td>0.3622</td>
<td>9.083</td>
<td>8.5313</td>
</tr>
<tr>
<td>4200</td>
<td>0.13</td>
<td>834</td>
<td>2529</td>
<td>0.3297</td>
<td>9.2540</td>
<td>9.0714</td>
</tr>
</tbody>
</table>
<p>单单针对MNIST数据集和逻辑回归模型，我们从上面这张表可以得出一些结论：batch_size越大，计算速度越快，精度慢慢有所提高；learning rate大的话epoches就会变大，也就是在整个train集上计算的次数多。</p>
<p>通过实验，我们得到9个模型在train.csv上的单独错误率分别为[8.352380952380953, 7.071428571428571, 7.304761904761905, 6.809523809523809, 7.311904761904762, 7.604761904761904, 7.607142857142857, 7.609523809523809, 7.228571428571429]，平均错误率为7.4333333333333336，而综合9个模型去做预测的话在train.csv上的错误率为 7.433333，所以说，上面这种方法的效果不明显。</p>
<p>然后，我们又综合9个模型去预测test.csv中的数据，提交到kaggle上的正确率为0.91429，而9个模型分别单独去预测test.csv中的数据，得到的正确率分别为[0.91057, 0.90543, 0.90986, 0.91371, 0.91214]（数据还没有全部拿到，kaggle一天只允许提交5次），可以看到十折交叉验证的方法提高了0.4%的准确率，可能是实验方法不恰当，没有想象的高。</p>
<p>另外之前在老电脑上没有N卡，只能用cpu来跑，跑一个逻辑回归模型大概要20多分钟，现在换了GTX960M，速度明显提升，上面的整个程序跑下来用了700秒，快了十多倍。</p>
<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p>这里不再使用上面的数据读取函数，而是用pandas来读取csv文件，具体函数如下。我使用了train.csv中的前八份来做training set，第九份做validation set，第十份做test set。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></div><div class="line">    print(<span class="string">'...loading data'</span>)</div><div class="line">    train_df = pandas.DataFrame.from_csv(path+<span class="string">'train.csv'</span>, index_col=<span class="keyword">False</span>).fillna(<span class="number">0</span>).astype(int)</div><div class="line">    test_df = pandas.DataFrame.from_csv(path+<span class="string">'test.csv'</span>, index_col=<span class="keyword">False</span>).fillna(<span class="number">0</span>).astype(int)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shared_dataset</span><span class="params">(data_xy, borrow=True)</span>:</span></div><div class="line">        <span class="string">""" Function that loads the dataset into shared variables</span></div><div class="line"></div><div class="line">        The reason we store our dataset in shared variables is to allow</div><div class="line">        Theano to copy it into the GPU memory (when code is run on GPU).</div><div class="line">        Since copying data into the GPU is slow, copying a minibatch everytime</div><div class="line">        is needed (the default behaviour if the data is not in a shared</div><div class="line">        variable) would lead to a large decrease in performance.</div><div class="line">        """</div><div class="line">        data_x, data_y = data_xy</div><div class="line">        shared_x = theano.shared(numpy.asarray(data_x, dtype=theano.config.floatX), borrow=borrow)</div><div class="line">        shared_y = theano.shared(numpy.asarray(data_y, dtype=theano.config.floatX), borrow=borrow)</div><div class="line">        <span class="comment"># When storing data on the GPU it has to be stored as floats</span></div><div class="line">        <span class="comment"># therefore we will store the labels as ``floatX`` as well</span></div><div class="line">        <span class="comment"># (``shared_y`` does exactly that). But during our computations</span></div><div class="line">        <span class="comment"># we need them as ints (we use labels as index, and if they are</span></div><div class="line">        <span class="comment"># floats it doesn't make sense) therefore instead of returning</span></div><div class="line">        <span class="comment"># ``shared_y`` we will have to cast it to int. This little hack</span></div><div class="line">        <span class="comment"># lets ous get around this issue</span></div><div class="line">        <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">'int32'</span>)</div><div class="line"></div><div class="line">    train_set = [train_df.values[<span class="number">0</span>:<span class="number">42000</span>, <span class="number">1</span>:]/<span class="number">255.0</span>, train_df.values[<span class="number">0</span>:<span class="number">42000</span>, <span class="number">0</span>]]</div><div class="line">    valid_set = [train_df.values[<span class="number">0</span>:<span class="number">42000</span>, <span class="number">1</span>:]/<span class="number">255.0</span>, train_df.values[<span class="number">0</span>:<span class="number">42000</span>, <span class="number">0</span>]]</div><div class="line">    test_set = [train_df.values[<span class="number">0</span>:<span class="number">42000</span>, <span class="number">1</span>:]/<span class="number">255.0</span>, train_df.values[<span class="number">0</span>:<span class="number">42000</span>, <span class="number">0</span>]]</div><div class="line">    predict_set = test_df.values/<span class="number">255.0</span></div><div class="line">    train_set_x, train_set_y = shared_dataset(train_set,  borrow=<span class="keyword">True</span>)</div><div class="line">    valid_set_x, valid_set_y = shared_dataset(valid_set, borrow=<span class="keyword">True</span>)</div><div class="line">    test_set_x, test_set_y = shared_dataset(test_set, borrow=<span class="keyword">True</span>)</div><div class="line">    predict_set = theano.shared(numpy.asarray(predict_set, dtype=theano.config.floatX), borrow=<span class="keyword">True</span>)</div><div class="line">    datasets = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y), predict_set]</div><div class="line">    <span class="keyword">return</span> datasets</div></pre></td></tr></table></figure>
<p>下面定义了一个类，作为多层感知机的隐含层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HiddenLayer</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rng, input, n_in, n_out, W=None, b=None,</span></span></div><div class="line">                 activation=T.tanh):</div><div class="line">        <span class="string">"""</span></div><div class="line">        Typical hidden layer of a MLP: units are fully-connected and have</div><div class="line">        sigmoid activation function. Weight matrix W is of shape (n_in,n_out)</div><div class="line">        and the bias vector b is of shape (n_out,).</div><div class="line"></div><div class="line">        NOTE: The nonlinearity used here is tanh</div><div class="line"></div><div class="line">        Hidden unit activation is given by: tanh(dot(input,W)+b)</div><div class="line"></div><div class="line">        :type rng: numpy.random.RandomState</div><div class="line">        :param rng: a random number generator used to initialize weights</div><div class="line">        :type input: theano.tensor.dmatrix</div><div class="line">        :param input: a symbolic tensor of shape (n_examples, n_in)</div><div class="line">        :type n_in: int</div><div class="line">        :param n_in: dimensionality of input</div><div class="line">        :type n_out: int</div><div class="line">        :param n_out: number of hidden units</div><div class="line">        :param W:</div><div class="line">        :param b:</div><div class="line">        :type activation: theano.Op or function</div><div class="line">        :param activation: Non linearity to be applied in the hidden layer</div><div class="line">        """</div><div class="line">        self.input = input</div><div class="line">        <span class="comment"># end-snippet-1</span></div><div class="line"></div><div class="line">        <span class="comment"># 'W' is initialized with 'W_values' which is uniformely sampled</span></div><div class="line">        <span class="comment"># from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))</span></div><div class="line">        <span class="comment"># for tanh activation function</span></div><div class="line">        <span class="comment"># the output of uniform if converted using asarray to dtype</span></div><div class="line">        <span class="comment"># theano.config.floatX so that the code is runnable on GPU</span></div><div class="line">        <span class="comment"># Note: optimal initialization of weights is dependent on the</span></div><div class="line">        <span class="comment">#       activation function used (among other things).</span></div><div class="line">        <span class="comment">#       For example, results presented in [Xavier10] suggest that you</span></div><div class="line">        <span class="comment">#       should use 4 times larger initial weights for sigmoid</span></div><div class="line">        <span class="comment">#       compared to tanh</span></div><div class="line">        <span class="comment">#       We have no info for other function, so we use the same as tanh.</span></div><div class="line">        <span class="keyword">if</span> W <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            W_values = numpy.asarray(</div><div class="line">                rng.uniform(</div><div class="line">                    low=-numpy.sqrt(<span class="number">6.</span>/(n_in+n_out)),</div><div class="line">                    high=numpy.sqrt(<span class="number">6.</span>/(n_in+n_out)),</div><div class="line">                    size=(n_in,n_out)</div><div class="line">                ),</div><div class="line">                dtype=theano.config.floatX</div><div class="line">            )</div><div class="line">            <span class="keyword">if</span> activation == theano.tensor.nnet.sigmoid:</div><div class="line">                W_values *= <span class="number">4</span></div><div class="line">            W = theano.shared(value=W_values, name=<span class="string">'W'</span>, borrow=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> b <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)</div><div class="line">            b = theano.shared(value=b_values, name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">        self.W = W</div><div class="line">        self.b = b</div><div class="line"></div><div class="line">        lin_output = T.dot(input, self.W) + self.b</div><div class="line">        self.output = (</div><div class="line">            lin_output <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="keyword">None</span></div><div class="line">            <span class="keyword">else</span> activation(lin_output)</div><div class="line">        )</div><div class="line">        <span class="comment"># parameters of the model</span></div><div class="line">        self.params = [self.W, self.b]</div></pre></td></tr></table></figure>
<p>下面是多层感知机的类，具体如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Multi-layer Perceptron Class</span></div><div class="line">    A multilayer perceptron is a feedforward artificial neural network model</div><div class="line">    that has one layer or more of hidden units and nonlinear activations.</div><div class="line">    Intermediate layers usually have as activation function tanh or the</div><div class="line">    sigmoid function (defined here by a 'HiddenLayer' class) while the</div><div class="line">    top layer is a softmax layer (defined here by a 'LogisticRegression' class).</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rng, input, n_in, n_hidden, n_out)</span>:</span></div><div class="line">        <span class="string">"""Initialize the parameters for the multilayer perceptron</span></div><div class="line">        :type rng: numpy.random.RandomState</div><div class="line">        :param rng: a random number generator used to initialize weights</div><div class="line">        :type input: theano.tensor.TensorType</div><div class="line">        :param input: symbolic variable that describes the input of the</div><div class="line">                        architecture (one minibatch)</div><div class="line">        :type n_in: int</div><div class="line">        :param n_in: number of input units, the dimension of the space in which the datapoints lie</div><div class="line">        :type n_hidden: int</div><div class="line">        :param n_hidden: number of hidden units</div><div class="line">        :type n_out: int</div><div class="line">        :param n_out: number of output units, the dimension of the space in which the labels lie</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="comment"># Since we are dealing with a one hidden layer MLP, this will translate</span></div><div class="line">        <span class="comment"># into a HiddenLayer with a tanh activation function connected to the</span></div><div class="line">        <span class="comment"># LogisticRegression layer; the activation function can be replaced by</span></div><div class="line">        <span class="comment"># sigmoid or any other nonlinear function</span></div><div class="line">        self.hiddenLayer = HiddenLayer(</div><div class="line">            rng=rng, input=input, n_in=n_in, n_out=n_hidden, activation=T.tanh</div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="comment"># The logistic regression layer gets as input the hiddenlayer units</span></div><div class="line">        <span class="comment"># of the hidden layer</span></div><div class="line">        self.logRegressionLayer = LogisticRegression(</div><div class="line">            input=self.hiddenLayer.output, n_in=n_hidden, n_out=n_out</div><div class="line">        )</div><div class="line">        <span class="comment"># end-snippet-2 start-snippet-3</span></div><div class="line">        <span class="comment"># L1 norm; one regularization option is enforce L1 norm to be small</span></div><div class="line">        self.L1 = (abs(self.hiddenLayer.W).sum()+abs(self.logRegressionLayer.W).sum())</div><div class="line"></div><div class="line">        <span class="comment"># square of L2 norm; one regularization option is to enforce</span></div><div class="line">        <span class="comment"># square of L2 norm to be small</span></div><div class="line">        self.L2_sqr = (</div><div class="line">            (self.hiddenLayer.W**<span class="number">2</span>).sum() + (self.logRegressionLayer.W**<span class="number">2</span>).sum())</div><div class="line"></div><div class="line">        <span class="comment"># negative log likelihood of the MLP is given by the negative</span></div><div class="line">        <span class="comment"># log likelihood of the output of the model, computed in the</span></div><div class="line">        <span class="comment"># logistic regression layer</span></div><div class="line">        <span class="comment"># self.negative_log_likelihood = (self.logRegressionLayer.negative_log_likelihood)</span></div><div class="line">        <span class="comment"># same holds for the function computing the number of errors</span></div><div class="line">        <span class="comment"># self.errors = self.logRegressionLayer.errors</span></div><div class="line"></div><div class="line">        <span class="comment"># the parameters of the model are the parameters of the two</span></div><div class="line">        <span class="comment"># layer it is made out of</span></div><div class="line">        self.params = self.hiddenLayer.params + self.logRegressionLayer.params</div><div class="line">        <span class="comment"># end-snippet-3</span></div><div class="line"></div><div class="line">        <span class="comment"># keep track of model input</span></div><div class="line">        self.input = input</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">negative_log_likelihood</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="keyword">return</span> -T.mean(T.log(self.logRegressionLayer.p_y_given_x)[T.arange(y.shape[<span class="number">0</span>]), y])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">errors</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="keyword">if</span> y.ndim != self.logRegressionLayer.y_pred.ndim:</div><div class="line">            <span class="keyword">raise</span> TypeError(</div><div class="line">                <span class="string">'y should have the same shape as self.y_pred'</span>,</div><div class="line">                (<span class="string">'y'</span>, y.type, <span class="string">'y_pred'</span>, self.logRegressionLayer.y_pred.type)</div><div class="line">            )</div><div class="line">        <span class="comment"># check if y is of the correct datatype</span></div><div class="line">        <span class="keyword">if</span> y.dtype.startswith(<span class="string">'int'</span>):</div><div class="line">            <span class="comment"># the T.neq operator returns a vector of 0s and 1s, where a</span></div><div class="line">            <span class="comment"># represents a mistake in prediction</span></div><div class="line">            <span class="keyword">return</span> T.mean(T.neq(self.logRegressionLayer.y_pred, y))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">raise</span> NotImplementedError()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getstate__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.__dict__</div></pre></td></tr></table></figure>
<p>然后是多层感知机的训练函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_mlp</span><span class="params">(learning_rate=<span class="number">0.0005</span>, L1_reg=<span class="number">0.00</span>, L2_reg=<span class="number">0.0001</span>, n_epochs=<span class="number">200</span>,</span></span></div><div class="line">                path=<span class="string">r''</span>, batch_size=<span class="number">20</span>, n_hidden=<span class="number">500</span>):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Demonstrate stochastic gradient descent optimization for a multilayer perceptron</div><div class="line">    This is demonstrated on MNIST.</div><div class="line">    :type learning_rate: float</div><div class="line">    :param learning_rate: learning rate used (factor for the stochastic gradient)</div><div class="line">    :type L1_reg: float</div><div class="line">    :param L1_reg: L1-norm's weight when added to the cost (see regularization)</div><div class="line">    :type L2_reg: float</div><div class="line">    :param L2_reg: L2-norm's weight when added to the cost (see regularization)</div><div class="line">    :type n_epochs: int</div><div class="line">    :param n_epochs: maximal number of epochs to run the optimizer</div><div class="line">    :type dataset: string</div><div class="line">    :param dataset: the path of the MNIST dataset file from</div><div class="line">                    http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz</div><div class="line">    :param batch_size:</div><div class="line">    :param n_hidden:</div><div class="line">    :return:</div><div class="line">    """</div><div class="line"></div><div class="line">    datasets = load_data(path)</div><div class="line">    <span class="comment"># datasets = read_raw_train(dataset)</span></div><div class="line">    train_set_x, train_set_y = datasets[<span class="number">0</span>]</div><div class="line">    valid_set_x, valid_set_y = datasets[<span class="number">1</span>]</div><div class="line">    test_set_x, test_set_y = datasets[<span class="number">2</span>]</div><div class="line"></div><div class="line">    <span class="comment"># compute number of minibatches for training, validation and testing</span></div><div class="line">    n_train_batches = train_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>]//batch_size</div><div class="line">    n_valid_batches = valid_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>]//batch_size</div><div class="line">    n_test_batches = test_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>]//batch_size</div><div class="line"></div><div class="line">    <span class="comment">######################</span></div><div class="line">    <span class="comment"># BUILD ACTUAL MODEL #</span></div><div class="line">    <span class="comment">######################</span></div><div class="line">    print(<span class="string">'...building the model'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># allocate symbolic variables for the data</span></div><div class="line">    index = T.lscalar() <span class="comment"># index to a [mini]batch</span></div><div class="line">    x = T.matrix(<span class="string">'x'</span>)   <span class="comment"># the data is presented as rasterized images</span></div><div class="line">    y = T.ivector(<span class="string">'y'</span>)   <span class="comment"># the labels are presented as 1D vector of [int] labels</span></div><div class="line"></div><div class="line">    rng = numpy.random.RandomState(<span class="number">1234</span>)</div><div class="line"></div><div class="line">    <span class="comment"># construct the MLP class</span></div><div class="line">    classifier = MLP(rng=rng, input=x, n_in=<span class="number">28</span>*<span class="number">28</span>, n_hidden=n_hidden, n_out=<span class="number">10</span>)</div><div class="line">    <span class="comment"># start-snippet-4</span></div><div class="line">    <span class="comment"># the cost we minimize during training is the negative log likelihood of</span></div><div class="line">    <span class="comment"># the model plus the regularization terms (L1 and L2): cost is expressed</span></div><div class="line">    <span class="comment"># here symbolically</span></div><div class="line">    cost = (</div><div class="line">        classifier.negative_log_likelihood(y) + L1_reg*classifier.L1 + L2_reg*classifier.L2_sqr</div><div class="line">    )</div><div class="line">    <span class="comment"># end-snippet-4</span></div><div class="line"></div><div class="line">    <span class="comment"># compiling a Theano function that computes the mistakes that are made</span></div><div class="line">    <span class="comment"># by the model on a minibatch</span></div><div class="line">    test_model = theano.function(</div><div class="line">        inputs=[index],</div><div class="line">        outputs=classifier.errors(y),</div><div class="line">        givens=&#123;</div><div class="line">            x: test_set_x[index*batch_size: (index+<span class="number">1</span>)*batch_size],</div><div class="line">            y: test_set_y[index*batch_size: (index+<span class="number">1</span>)*batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    validate_model = theano.function(</div><div class="line">        inputs=[index],</div><div class="line">        outputs=classifier.errors(y),</div><div class="line">        givens=&#123;</div><div class="line">            x: valid_set_x[index*batch_size:(index+<span class="number">1</span>)*batch_size],</div><div class="line">            y: valid_set_y[index*batch_size:(index+<span class="number">1</span>)*batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment"># start-snippet-5</span></div><div class="line">    <span class="comment"># compute the gradient of cost with respect to theta (sorted in params)</span></div><div class="line">    <span class="comment"># the resulting gradients will be stored in a list gparams</span></div><div class="line">    gparams = [T.grad(cost, param) <span class="keyword">for</span> param <span class="keyword">in</span> classifier.params]</div><div class="line"></div><div class="line">    <span class="comment"># specify how to update the parameters of the model as a list of</span></div><div class="line">    <span class="comment"># (variable, update expression) pairs</span></div><div class="line"></div><div class="line">    <span class="comment"># given two lists of the same length, A=[a1, a2, a3, a4] and</span></div><div class="line">    <span class="comment"># B=[b1, b2, b3, b4], zip generates a list C of same size, where each</span></div><div class="line">    <span class="comment"># element is a pair formed from the two lists:</span></div><div class="line">    <span class="comment"># C=[(a1, b1), (a2, b2), (a3, b3), (a4, b4)]</span></div><div class="line">    updates=[</div><div class="line">        (param, param-learning_rate*gparam)</div><div class="line">         <span class="keyword">for</span> param, gparam <span class="keyword">in</span> zip(classifier.params, gparams)</div><div class="line">     ]</div><div class="line"></div><div class="line">    <span class="comment"># compiling a Theano function 'train_model' that returns the cost, but</span></div><div class="line">    <span class="comment"># in the same time updates the parameter of the model based on the rules</span></div><div class="line">    <span class="comment"># defined in 'updates'</span></div><div class="line">    train_model = theano.function(</div><div class="line">        inputs=[index],</div><div class="line">        outputs=cost,</div><div class="line">        updates=updates,            <span class="comment"># RMSprop(gparams, classifier.params, learning_rate),</span></div><div class="line">        givens=&#123;</div><div class="line">            x: train_set_x[index*batch_size: (index+<span class="number">1</span>)*batch_size],</div><div class="line">            y: train_set_y[index*batch_size: (index+<span class="number">1</span>)*batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line">    <span class="comment"># end-snippet-5</span></div><div class="line"></div><div class="line">    <span class="comment">###############</span></div><div class="line">    <span class="comment"># TRAIN MODEL #</span></div><div class="line">    <span class="comment">###############</span></div><div class="line">    print(<span class="string">'...training'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># early-stopping parameters</span></div><div class="line">    patience = <span class="number">10000</span>    <span class="comment"># look as this many examples regardless</span></div><div class="line">    patience_increase = <span class="number">2</span> <span class="comment"># wait this much longer when a new best is found</span></div><div class="line">    improvement_threshold = <span class="number">0.995</span> <span class="comment"># a relative improvement of this much is considered significant</span></div><div class="line">    validation_frequency = min(n_train_batches, patience//<span class="number">2</span>)</div><div class="line">            <span class="comment"># go through this many minibatche before checking the network</span></div><div class="line">            <span class="comment"># on the validation set; in this case we check every epoch</span></div><div class="line"></div><div class="line">    best_validation_loss = numpy.inf</div><div class="line">    best_iter = <span class="number">0</span></div><div class="line">    test_score = <span class="number">0.</span></div><div class="line">    start_time = timeit.default_timer()</div><div class="line"></div><div class="line">    epoch = <span class="number">0</span></div><div class="line">    done_looping = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="keyword">while</span> (epoch &lt; n_epochs) <span class="keyword">and</span> (<span class="keyword">not</span> done_looping):</div><div class="line">        epoch += <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> epoch &gt; <span class="number">60</span>:</div><div class="line">            learning_rate = <span class="number">0.001</span></div><div class="line">        <span class="keyword">if</span> epoch &gt; <span class="number">300</span>:</div><div class="line">            learning_rate = <span class="number">0.0005</span></div><div class="line">        <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> range(n_train_batches):</div><div class="line">            minibatch_avg_cost = train_model(minibatch_index)</div><div class="line">            <span class="comment"># iteration number</span></div><div class="line">            iter = (epoch<span class="number">-1</span>)*n_train_batches + minibatch_index</div><div class="line">            <span class="keyword">if</span> (iter+<span class="number">1</span>)%validation_frequency==<span class="number">0</span>:</div><div class="line">                <span class="comment"># compute zero-one loss on validation set</span></div><div class="line">                validation_losses = [validate_model(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_valid_batches)]</div><div class="line">                this_validation_loss = numpy.mean(validation_losses)</div><div class="line">                print(<span class="string">'epoch %i, minibatch %i/%i, validation error %f %%'</span> %</div><div class="line">                      (epoch, minibatch_index+<span class="number">1</span>, n_train_batches, this_validation_loss*<span class="number">100.</span>))</div><div class="line"></div><div class="line">                <span class="comment"># if we got best validation score until now</span></div><div class="line">                <span class="keyword">if</span> this_validation_loss &lt; best_validation_loss:</div><div class="line">                    <span class="comment"># improve patience if loss improvement is good enough</span></div><div class="line">                    <span class="keyword">if</span> (this_validation_loss&lt;best_validation_loss*improvement_threshold):</div><div class="line">                        patience = max(patience, iter*patience_increase)</div><div class="line">                    best_validation_loss = this_validation_loss</div><div class="line">                    best_iter = iter</div><div class="line"></div><div class="line">                    <span class="comment"># test it on the test set</span></div><div class="line">                    test_losses = [test_model(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_test_batches)]</div><div class="line">                    test_score = numpy.mean(test_losses)</div><div class="line">                    print((<span class="string">'     epoch %i, minibatch %i/%i, test error of best model '</span></div><div class="line">                           <span class="string">'%f %%'</span>) % (epoch, minibatch_index+<span class="number">1</span>, n_train_batches, test_score*<span class="number">100.</span>))</div><div class="line"></div><div class="line">                    <span class="comment"># save the best model</span></div><div class="line">                    <span class="keyword">with</span> open(<span class="string">'mymlp_best_model.pkl'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">                       pickle.dump(classifier, f)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> patience &lt;= iter:</div><div class="line">                done_looping = <span class="keyword">True</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">    end_time = timeit.default_timer()</div><div class="line">    print((<span class="string">'Optimization complete. Besat validation score of %f %% '</span></div><div class="line">           <span class="string">'obtained at iteration %i, with test performance %f %%'</span>) %</div><div class="line">          (best_validation_loss*<span class="number">100.</span>, best_iter+<span class="number">1</span>, test_score*<span class="number">100.</span>))</div><div class="line">    print((<span class="string">'The code for file '</span> + os.path.split(__file__)[<span class="number">1</span>] +</div><div class="line">           <span class="string">'ran for %.2fm'</span> % ((end_time - start_time)/<span class="number">60.</span>)), file=sys.stderr)</div></pre></td></tr></table></figure>
<p>最后是预测函数    和主函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_kaggle</span><span class="params">(file1, file2)</span>:</span></div><div class="line">    <span class="comment"># load the saved model</span></div><div class="line">    classifier = pickle.load(open(file1))</div><div class="line">    <span class="comment"># compile a predictor function</span></div><div class="line">    predict_model = theano.function(inputs=[classifier.input], outputs=classifier.logRegressionLayer.y_pred,</div><div class="line">                                    allow_input_downcast=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="comment"># make prediction</span></div><div class="line">    data_path = <span class="string">r'test.csv'</span></div><div class="line">    datasets = load_data(<span class="string">r''</span>)</div><div class="line">    test_set_x = datasets[<span class="number">3</span>]</div><div class="line">    test_set_x = test_set_x.get_value()</div><div class="line">    predicted_values = predict_model(test_set_x[:<span class="number">28000</span>])</div><div class="line">    <span class="comment"># 保存预测结果</span></div><div class="line">    saveResult(predicted_values, file2)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    file1 = <span class="string">r'mymlp_best_model.pkl'</span></div><div class="line">    file2 = <span class="string">r'answer_myMLP.csv'</span></div><div class="line">    test_mlp(learning_rate=<span class="number">0.1</span>, L1_reg=<span class="number">0.00</span>, L2_reg=<span class="number">0.0001</span>, n_epochs=<span class="number">1000</span>, path=<span class="string">r''</span>, batch_size=<span class="number">20</span>, n_hidden=<span class="number">500</span>)</div><div class="line">    predict_kaggle(file1, file2)</div></pre></td></tr></table></figure>
<p>提交后的正确率为0.97486，如果将整个train.csv作为training set呢，最后的正确率达到0.97843，只是此时模型就过拟合了。</p>
<h2 id="LeNet5"><a href="#LeNet5" class="headerlink" title="LeNet5"></a>LeNet5</h2><p>前面我们用逻辑回归和MLP实现了kaggle中的手写识别，下面我们用LeNet来实现之。读取数据的方式和上面一样，但是这里我不知道怎么用保存得到的模型去预测未标记的数据。因为前面的逻辑回归和MLP的模型都只有一层，很容易拿来去进行预测，关于这部分我会在写好预测函数后修改博客，现在先将预测部分写到训练函数的尾部。</p>
<p>LeNet的本质是卷积神经网络（Convolutional Neural Networks），其原理在前面的博客中有提到过，具体可阅读<a href="http://marcovaldong.github.io/2016/05/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%AC%E4%BA%94%E8%AE%B2%EF%BC%9ACNN/">机器学习中使用的神经网络第五讲笔记</a>，也可以阅读<a href="http://deeplearning.net/tutorial/" target="_blank" rel="external">Deep Learning Tutorials</a>中的<a href="http://deeplearning.net/tutorial/lenet.html#lenet" target="_blank" rel="external">Convolutional Neural Networks (LeNet)</a>。下面给出LeNet的卷积池化层类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNetConvPoolLayer</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Pool Layer of a convolutional network """</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rng, input, filter_shape, image_shape, poolsize=<span class="params">(<span class="number">2</span>, <span class="number">2</span>)</span>)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Allocate a LeNetConvPoolLayer with shared variable internal parameters.</div><div class="line"></div><div class="line">        :type rng: numpy.random.RandomState</div><div class="line">        :param rng: a random number generator used to initialize weights</div><div class="line"></div><div class="line">        :type input: theano.tensor.dtensor4</div><div class="line">        :param input: symbolic image tensor, of shape image_shape</div><div class="line"></div><div class="line">        :type filter_shape: tuple or list of length 4</div><div class="line">        :param filter_shape: (number of filters, num input feature maps,</div><div class="line">                              filter height, filter width)</div><div class="line"></div><div class="line">        :type image_shape: tuple or list of length 4</div><div class="line">        :param image_shape: (batch size, num input feature maps,</div><div class="line">                             image height, image width)</div><div class="line"></div><div class="line">        :type poolsize: tuple or list of length 2</div><div class="line">        :param poolsize: the downsampling (pooling) factor (#rows, #cols)</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="keyword">assert</span> image_shape[<span class="number">1</span>] == filter_shape[<span class="number">1</span>]</div><div class="line">        self.input = input</div><div class="line"></div><div class="line">        <span class="comment"># there are "num input feature maps * filter height * filter width"</span></div><div class="line">        <span class="comment"># inputs to each hidden unit</span></div><div class="line">        fan_in = numpy.prod(filter_shape[<span class="number">1</span>:])</div><div class="line">        <span class="comment"># each unit in the lower layer receives a gradient from:</span></div><div class="line">        <span class="comment"># "num output feature maps * filter height * filter width" /</span></div><div class="line">        <span class="comment">#   pooling size</span></div><div class="line">        fan_out = (filter_shape[<span class="number">0</span>] * numpy.prod(filter_shape[<span class="number">2</span>:]) //</div><div class="line">                   numpy.prod(poolsize))</div><div class="line">        <span class="comment"># initialize weights with random weights</span></div><div class="line">        W_bound = numpy.sqrt(<span class="number">6.</span> / (fan_in + fan_out))</div><div class="line">        self.W = theano.shared(</div><div class="line">            numpy.asarray(</div><div class="line">                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),</div><div class="line">                dtype=theano.config.floatX</div><div class="line">            ),</div><div class="line">            borrow=<span class="keyword">True</span></div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="comment"># the bias is a 1D tensor -- one bias per output feature map</span></div><div class="line">        b_values = numpy.zeros((filter_shape[<span class="number">0</span>],), dtype=theano.config.floatX)</div><div class="line">        self.b = theano.shared(value=b_values, borrow=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">        <span class="comment"># convolve input feature maps with filters</span></div><div class="line">        conv_out = conv2d(</div><div class="line">            input=input,</div><div class="line">            filters=self.W,</div><div class="line">            filter_shape=filter_shape,</div><div class="line">            input_shape=image_shape</div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="comment"># pool each feature map individually, using maxpooling</span></div><div class="line">        pooled_out = pool.pool_2d(</div><div class="line">            input=conv_out,</div><div class="line">            ds=poolsize,</div><div class="line">            ignore_border=<span class="keyword">True</span></div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="comment"># add the bias term. Since the bias is a vector (1D array), we first</span></div><div class="line">        <span class="comment"># reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will</span></div><div class="line">        <span class="comment"># thus be broadcasted across mini-batches and feature map</span></div><div class="line">        <span class="comment"># width &amp; height</span></div><div class="line">        self.output = T.tanh(pooled_out + self.b.dimshuffle(<span class="string">'x'</span>, <span class="number">0</span>, <span class="string">'x'</span>, <span class="string">'x'</span>))</div><div class="line"></div><div class="line">        <span class="comment"># store parameters of this layer</span></div><div class="line">        self.params = [self.W, self.b]</div><div class="line"></div><div class="line">        <span class="comment"># keep track of model input</span></div><div class="line">        self.input = input</div></pre></td></tr></table></figure>
<p>下面是LeNet的训练函数主体，LeNet模型共4层：layer0层首先将28<em>28的输入降维到(28-5+1)</em>(28-5+1)=24<em>24，这一步就是卷积化，然后再进行2</em>2的池化，最后layer0层的输出就变成了12<em>12；layer1层首先将12</em>12的输入降维到(12-5+1)<em>(12-5+1)=8</em>8，再进行2<em>2的池化，最后layer1层的输出变成了4</em>4；layer2层是隐含层，layer3层是逻辑回归层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_CNN</span><span class="params">(learning_rate=<span class="number">0.03</span>, n_epochs=<span class="number">300</span>,</span></span></div><div class="line">                path=<span class="string">r''</span>, batch_size=<span class="number">20</span>, patience=<span class="number">10000</span>, mu=<span class="number">0.9</span>):</div><div class="line">    datasets = load_data(path)</div><div class="line">    <span class="comment"># datasets = read_raw_train(dataset)</span></div><div class="line">    train_set_x, train_set_y = datasets[<span class="number">0</span>]</div><div class="line">    valid_set_x, valid_set_y = datasets[<span class="number">1</span>]</div><div class="line">    test_set_x, test_set_y = datasets[<span class="number">2</span>]</div><div class="line">    predict_set = datasets[<span class="number">3</span>]</div><div class="line">    nkerns = [<span class="number">20</span>, <span class="number">50</span>]</div><div class="line">    <span class="comment"># compute number of minibatches for training, validation and testing</span></div><div class="line">    n_train_batches = train_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>] // batch_size</div><div class="line">    n_valid_batches = valid_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>] // batch_size</div><div class="line">    n_test_batches = test_set_x.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>] // batch_size</div><div class="line">    n_predict_batches = predict_set.get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>] // batch_size</div><div class="line">    <span class="comment">######################</span></div><div class="line">    <span class="comment"># BUILD ACTUAL MODEL #</span></div><div class="line">    <span class="comment">######################</span></div><div class="line">    print(<span class="string">'...building the model'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># allocate symbolic variables for the data</span></div><div class="line">    index = T.lscalar()  <span class="comment"># index to a [mini]batch</span></div><div class="line"></div><div class="line">    x = T.matrix(<span class="string">'x'</span>)  <span class="comment"># the data is presented as rasterized images</span></div><div class="line">    y = T.ivector(<span class="string">'y'</span>)  <span class="comment"># the labels are presented as 1D vector of</span></div><div class="line">    <span class="comment"># [int] labels</span></div><div class="line"></div><div class="line">    <span class="comment">######################</span></div><div class="line">    <span class="comment"># BUILD ACTUAL MODEL #</span></div><div class="line">    <span class="comment">######################</span></div><div class="line">    print(<span class="string">'... building the model'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Reshape matrix of rasterized images of shape (batch_size, 28 * 28)</span></div><div class="line">    <span class="comment"># to a 4D tensor, compatible with our LeNetConvPoolLayer</span></div><div class="line">    <span class="comment"># (28, 28) is the size of MNIST images.</span></div><div class="line">    layer0_input = x.reshape((batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</div><div class="line"></div><div class="line">    <span class="comment"># Construct the first convolutional pooling layer:</span></div><div class="line">    <span class="comment"># filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)</span></div><div class="line">    <span class="comment"># maxpooling reduces this further to (24/2, 24/2) = (12, 12)</span></div><div class="line">    <span class="comment"># 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)</span></div><div class="line">    layer0 = LeNetConvPoolLayer(</div><div class="line">        rng,</div><div class="line">        input=layer0_input,</div><div class="line">        image_shape=(batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>),</div><div class="line">        filter_shape=(nkerns[<span class="number">0</span>], <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>),</div><div class="line">        poolsize=(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment"># Construct the second convolutional pooling layer</span></div><div class="line">    <span class="comment"># filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)</span></div><div class="line">    <span class="comment"># maxpooling reduces this further to (8/2, 8/2) = (4, 4)</span></div><div class="line">    <span class="comment"># 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)</span></div><div class="line">    layer1 = LeNetConvPoolLayer(</div><div class="line">        rng,</div><div class="line">        input=layer0.output,</div><div class="line">        image_shape=(batch_size, nkerns[<span class="number">0</span>], <span class="number">12</span>, <span class="number">12</span>),</div><div class="line">        filter_shape=(nkerns[<span class="number">1</span>], nkerns[<span class="number">0</span>], <span class="number">5</span>, <span class="number">5</span>),</div><div class="line">        poolsize=(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment"># the HiddenLayer being fully-connected, it operates on 2D matrices of</span></div><div class="line">    <span class="comment"># shape (batch_size, num_pixels) (i.e matrix of rasterized images).</span></div><div class="line">    <span class="comment"># This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),</span></div><div class="line">    <span class="comment"># or (500, 50 * 4 * 4) = (500, 800) with the default values.</span></div><div class="line">    layer2_input = layer1.output.flatten(<span class="number">2</span>)</div><div class="line"></div><div class="line">    <span class="comment"># construct a fully-connected sigmoidal layer</span></div><div class="line">    layer2 = HiddenLayer(</div><div class="line">        rng,</div><div class="line">        input=layer2_input,</div><div class="line">        n_in=nkerns[<span class="number">1</span>] * <span class="number">4</span> * <span class="number">4</span>,</div><div class="line">        n_out=<span class="number">500</span>,</div><div class="line">        activation=T.tanh</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment"># classify the values of the fully-connected sigmoidal layer</span></div><div class="line">    layer3 = LogisticRegression(input=layer2.output, n_in=<span class="number">500</span>, n_out=<span class="number">10</span>)</div><div class="line">    classifier = [layer0, layer1, layer2, layer3]</div><div class="line">    <span class="comment"># the cost we minimize during training is the NLL of the model</span></div><div class="line">    cost = layer3.negative_log_likelihood(y)</div><div class="line"></div><div class="line">    <span class="comment"># create a function to compute the mistakes that are made by the model</span></div><div class="line">    test_model = theano.function(</div><div class="line">        [index],</div><div class="line">        layer3.errors(y),</div><div class="line">        givens=&#123;</div><div class="line">            x: test_set_x[index * batch_size: (index + <span class="number">1</span>) * batch_size],</div><div class="line">            y: test_set_y[index * batch_size: (index + <span class="number">1</span>) * batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    validate_model = theano.function(</div><div class="line">        [index],</div><div class="line">        layer3.errors(y),</div><div class="line">        givens=&#123;</div><div class="line">            x: valid_set_x[index * batch_size: (index + <span class="number">1</span>) * batch_size],</div><div class="line">            y: valid_set_y[index * batch_size: (index + <span class="number">1</span>) * batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment"># create a list of all model parameters to be fit by gradient descent</span></div><div class="line">    params = layer3.params + layer2.params + layer1.params + layer0.params</div><div class="line"></div><div class="line">    <span class="comment"># create a list of gradients for all model parameters</span></div><div class="line">    grads = T.grad(cost, params)</div><div class="line"></div><div class="line">    <span class="comment"># train_model is a function that updates the model parameters by</span></div><div class="line">    <span class="comment"># SGD Since this model has many parameters, it would be tedious to</span></div><div class="line">    <span class="comment"># manually create an update rule for each model parameter. We thus</span></div><div class="line">    <span class="comment"># create the updates list by automatically looping over all</span></div><div class="line">    <span class="comment"># (params[i], grads[i]) pairs.</span></div><div class="line">    updates = [</div><div class="line">        (param_i, param_i - learning_rate * grad_i)       <span class="comment"># param_i - mu * param_i + (1 + mu) * (mu * param_i - learning_rate * grad_i))</span></div><div class="line">        <span class="keyword">for</span> param_i, grad_i <span class="keyword">in</span> zip(params, grads)</div><div class="line">        ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">RMSprop</span><span class="params">(gparams, params, learning_rate, rho=<span class="number">0.9</span>, epsilon=<span class="number">1e-6</span>)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        param:rho,the fraction we keep the previous gradient contribution</div><div class="line">        """</div><div class="line">        updates = []</div><div class="line">        <span class="keyword">for</span> p, g <span class="keyword">in</span> zip(params, gparams):</div><div class="line">            acc = theano.shared(p.get_value() * <span class="number">0.</span>)</div><div class="line">            acc_new = rho * acc + (<span class="number">1</span> - rho) * g ** <span class="number">2</span></div><div class="line">            gradient_scaling = T.sqrt(acc_new + epsilon)</div><div class="line">            g = g / gradient_scaling</div><div class="line">            updates.append((acc, acc_new))</div><div class="line">            updates.append((p, p - learning_rate * g))</div><div class="line">        <span class="keyword">return</span> updates</div><div class="line"></div><div class="line">    train_model = theano.function(</div><div class="line">        [index],</div><div class="line">        cost,</div><div class="line">        updates=updates,    <span class="comment"># RMSprop(grads, params, learning_rate, rho=mu, epsilon=1e-6),</span></div><div class="line">        givens=&#123;</div><div class="line">            x: train_set_x[index * batch_size: (index + <span class="number">1</span>) * batch_size],</div><div class="line">            y: train_set_y[index * batch_size: (index + <span class="number">1</span>) * batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="comment">###############</span></div><div class="line">    <span class="comment"># TRAIN MODEL #</span></div><div class="line">    <span class="comment">###############</span></div><div class="line">    print(<span class="string">'...training'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># early-stopping parameters</span></div><div class="line">    <span class="comment"># patience = 10000  # look as this many examples regardless</span></div><div class="line">    patience_increase = <span class="number">2</span>  <span class="comment"># wait this much longer when a new best is found</span></div><div class="line">    improvement_threshold = <span class="number">0.995</span>  <span class="comment"># a relative improvement of this much is considered significant</span></div><div class="line">    validation_frequency = min(n_train_batches, patience // <span class="number">2</span>)</div><div class="line">    <span class="comment"># go through this many minibatche before checking the network</span></div><div class="line">    <span class="comment"># on the validation set; in this case we check every epoch</span></div><div class="line"></div><div class="line">    best_validation_loss = numpy.inf</div><div class="line">    best_iter = <span class="number">0</span></div><div class="line">    test_score = <span class="number">0.</span></div><div class="line">    start_time = timeit.default_timer()</div><div class="line"></div><div class="line">    epoch = <span class="number">0</span></div><div class="line">    done_looping = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="keyword">while</span> (epoch &lt; n_epochs) <span class="keyword">and</span> (<span class="keyword">not</span> done_looping):</div><div class="line">        epoch += <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> range(n_train_batches):</div><div class="line">            minibatch_avg_cost = train_model(minibatch_index)</div><div class="line">            <span class="comment"># iteration number</span></div><div class="line">            iter = (epoch - <span class="number">1</span>) * n_train_batches + minibatch_index</div><div class="line">            <span class="keyword">if</span> (iter + <span class="number">1</span>) % validation_frequency == <span class="number">0</span>:</div><div class="line">                <span class="comment"># compute zero-one loss on validation set</span></div><div class="line">                validation_losses = [validate_model(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_valid_batches)]</div><div class="line">                this_validation_loss = numpy.mean(validation_losses)</div><div class="line">                print(<span class="string">'epoch %i, minibatch %i/%i, validation error %f %%'</span> %</div><div class="line">                      (epoch, minibatch_index + <span class="number">1</span>, n_train_batches, this_validation_loss * <span class="number">100.</span>))</div><div class="line"></div><div class="line">                <span class="comment"># if we got best validation score until now</span></div><div class="line">                <span class="keyword">if</span> this_validation_loss &lt; best_validation_loss:</div><div class="line">                    <span class="comment"># improve patience if loss improvement is good enough</span></div><div class="line">                    <span class="keyword">if</span> (this_validation_loss &lt; best_validation_loss * improvement_threshold):</div><div class="line">                        patience = max(patience, iter * patience_increase)</div><div class="line">                    best_validation_loss = this_validation_loss</div><div class="line">                    best_iter = iter</div><div class="line"></div><div class="line">                    <span class="comment"># test it on the test set</span></div><div class="line">                    test_losses = [test_model(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(n_test_batches)]</div><div class="line">                    test_score = numpy.mean(test_losses)</div><div class="line">                    print((<span class="string">'     epoch %i, minibatch %i/%i, test error of best model '</span></div><div class="line">                           <span class="string">'%f %%'</span>) % (epoch, minibatch_index + <span class="number">1</span>, n_train_batches, test_score * <span class="number">100.</span>))</div><div class="line"></div><div class="line">                    <span class="comment"># save the best model</span></div><div class="line">                    <span class="keyword">with</span> open(<span class="string">'kaggle_CNN_best_model.pkl'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">                        pickle.dump(classifier, f)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> patience &lt;= iter:</div><div class="line">                done_looping = <span class="keyword">True</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">    end_time = timeit.default_timer()</div><div class="line">    print((<span class="string">'Optimization complete. Besat validation score of %f %% '</span></div><div class="line">           <span class="string">'obtained at iteration %i, with test performance %f %%'</span>) %</div><div class="line">          (best_validation_loss * <span class="number">100.</span>, best_iter + <span class="number">1</span>, test_score * <span class="number">100.</span>))</div><div class="line">    print((<span class="string">'The code for file '</span> + os.path.split(__file__)[<span class="number">1</span>] +</div><div class="line">           <span class="string">'ran for %.2fm'</span> % ((end_time - start_time) / <span class="number">60.</span>)), file=sys.stderr)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># batch_size = 20</span></div><div class="line">    print(<span class="string">'... Predicting.'</span>)</div><div class="line">    predict_model = theano.function(</div><div class="line">        [index],</div><div class="line">        outputs=layer3.y_pred,</div><div class="line">        givens=&#123;</div><div class="line">            x: predict_set[index * batch_size: (index + <span class="number">1</span>) * batch_size]</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line">    answer = []</div><div class="line">    <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> range(n_predict_batches):</div><div class="line">        <span class="comment"># print(predict_model(minibatch_index))</span></div><div class="line">        minibatch_answer = predict_model(minibatch_index)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</div><div class="line">            answer.append(minibatch_answer[i])</div><div class="line"></div><div class="line">    print(len(answer))</div><div class="line">    print(<span class="string">'... saving predict answer.'</span>)</div><div class="line">    saveResult(answer, <span class="string">'lenet.csv'</span>)</div></pre></td></tr></table></figure>
<p>注意，这里我不知道怎么用保存下来的模型去预测未标记的数据，所以这里没有写单独的预测函数，而是把预测的部分放在了训练函数的末尾。</p>
<p>kaggle的手写识别中，其训练数据共42000组，我这里将训练数据按照18:1:1的比例分给了训练集/交叉验证集和测试集。然后将learning rate定为0.03，batch size是20，epoch是80，最后得到的正确率为0.99086，排名是135/1037。当我把42000组数据全部用来训练的话，模型跑到第24个epoch时，错误率就到了0了，也就是已经完全过拟合了，然后用这时候的模型去预测未标记的那28000组数据，提交后正确率为0.99129，排名是118/1037。</p>
<p>下一步的工作是继续系统学习theano及其他深度学习工具，自己去实现更多模型和优化方法。另外，在新电脑上搭建环境用了十多天，反复出错，有时间我会整理一篇博客，方便以后查找。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2016/07/20/Theano实现kaggle手写识别/" data-id="cjfguskgz000ycgurzszrj9wg" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Theano/">Theano</a></div><div class="post-nav"><a href="/2016/08/18/Python爬虫爬取知乎小结/" class="pre">Python爬虫爬取知乎小结</a><a href="/2016/07/08/Ubuntu14-04-Theano-OpenCL-libgpuarray实现GPU运算/" class="next">Ubuntu14.04+Theano+OpenCL+libgpuarray实现GPU运算</a></div><div data-thread-key="2016/07/20/Theano实现kaggle手写识别/" data-title="Theano实现kaggle手写识别" data-url="http://marcovaldong.github.io/2016/07/20/Theano实现kaggle手写识别/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2016/07/20/Theano实现kaggle手写识别/" data-title="Theano实现kaggle手写识别" data-url="http://marcovaldong.github.io/2016/07/20/Theano实现kaggle手写识别/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>