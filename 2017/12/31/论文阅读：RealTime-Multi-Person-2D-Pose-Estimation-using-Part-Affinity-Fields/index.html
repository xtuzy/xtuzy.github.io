<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="日拱一卒，功不唐捐"><title>论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields | Marcovaldo</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</h1><a id="logo" href="/.">Marcovaldo</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/LeetCode/"><i class="fa fa-list"> LeetCode</i></a><a href="/Booklist/"><i class="fa fa-book"> Booklist</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</h1><div class="post-meta">Dec 31, 2017<span> | </span><span class="category"><a href="/categories/Deep-Learning/">Deep Learning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/#comments" class="ds-thread-count"></a><div class="post-content"><p>这是CMU在CVPR 2017上的一篇工作，实际上我在两个月前看AI Challenger比赛的时候就读了，并且看了关于它的一个MxNet实现，大概搞明白了，限于没有机器没有跑一下，也就没有参加比赛。这篇博客算是整理一下我对这篇论文的理解。<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>论文首先指出了Pose Estimation的三个关键性挑战：一是我们不知道一张图像中包含多少人，这些人的姿态和尺寸如何也都是未知的；二是多个人体之间的接触、遮挡等使得情况更加复杂；三是实时性的要求，随着图像中人数的增多计算也越来越复杂。然后论文中指出了现有的一些方法存在的问题：现有的一些方法主要由person detector和single-person pose estimation两部分组成，这些top-down的方法在性能上很依赖于这两个部分，如果person detector检测失败了（当多个人挨得很近的时候），这些方法可能就不奏效了。另外一个就是时间的问题，这些方法都没有一个很好的处理。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_2.png" alt="Figure 2"></p>
<h2 id="Simultaneous-Detection-and-Association"><a href="#Simultaneous-Detection-and-Association" class="headerlink" title="Simultaneous Detection and Association"></a>Simultaneous Detection and Association</h2><p>Figure 2中给出了模型的整个处理过程：将一张尺寸为$\mathit{w} \times \mathit{h}$的图像输入进去，然后模型同时得到人体部位位置的confidence maps集合S和一个用来说明关节点连接关系的part affinities集合L。其中，$S=(S_1, S_2,…,S_J)$中包含J个confidence map，每个表示一个关键点，$S_j \in \mathbb{R}^{\mathit{w} \times \mathit{h}}, j \in [1…J]$。集合$L=(L_1, L_2, …, L_C)$中包含C个vector field，每一个都表示一个limb（即两个关键点之间的连线），其中$L_c \in \mathbb{R}^{\mathit{w} \times \mathit{h} \times 2}, c \in [1…C]$，$L_c$中的每个图像位置都编码成一个2D的向量。最后，confidence maps和affinity fields被拿来推理出greedy inference，得到一张图像中所有人的2D关键点。</p>
<p>Figure 3给出了模型示意图，图像输入进去，然后同时预测出confidence maps和affinity fields。网络分成两个部分，上边米色的那部分预测出confidence maps，下边蓝色的那部分预测出affinity fields。每个分支都是一个迭代预测结构，整个模型包含了T个stage，每个stage都加入中间监督（intermediate supervision）。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_3.png" alt="Figure 3"></p>
<p>图像先经过微调过的VGG19的前十层得到一组feature maps F，将其输入到每一个分支的第一个stage中。在第一个stage中，网络输出一组detection confidence maps $S^1 = \rho^1(F)$和一组part affinity fields $L^1 = \phi^1(F)$，其中$\rho^1$和$\phi^1$表示第一个stage的CNN结构。在随后的每个stage中，我们将前一个阶段的输出和F给concatenate到一块儿输入进去，输出的是refined predictions。</p>
$$S^t = \rho^t(F, S^{t-1}, L^{t-1}), \forall t \geq 2, \qquad \qquad \qquad \nonumber{(1)}$$
$$L^t = \phi^t(F, S^{t-1}, L^{t-1}), \forall t \geq 2, \qquad \qquad \qquad \nonumber{(2)}$$
Figure 4中给出了不同stage的confidence maps和affinity fields的改良过程。我们使用L2损失来评估每个阶段的预测结果，公式(3)和(4)中给出了损失函数的公式，其中$S_{j}^{*}$是groundtruth part confidence map，$L_{c}^{*}$是groundtruth part affinity vector field，W是一个二进制标志（当图像中位置p的标注数据缺失时$W(p)=0$，避免在训练过程中惩罚true positive predictions）。中间监督通过阶段性地补充梯度，能够有效得解决一部分梯度消失的问题。整个模型的目标即公式(5)。
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_4.png" alt="Figure 4"></p>
$$f^{t}_{S} = \sum\limits_{j=1}^{J} \sum_{p}W(p) \cdot \left|S_{j}^{t}(p) - S_{j}^{*}(p)\right|_{2}^{2}, \qquad \qquad \qquad \nonumber{(3)}$$
$$f^{t}_{L} = \sum\limits_{c=1}^{C} \sum_{p}W(p) \cdot \left|L_{c}^{t}(p) - L_{c}^{*}(p)\right|_{2}^{2}, \qquad \qquad \qquad \nonumber{(4)}$$
$$f = \sum_{t=1}^{T}(f_S^t + f_L^t), \qquad \qquad \qquad \qquad \qquad \qquad \qquad \nonumber{(5)}$$
<h2 id="Confidence-Maps-for-Part-Detection"><a href="#Confidence-Maps-for-Part-Detection" class="headerlink" title="Confidence Maps for Part Detection"></a>Confidence Maps for Part Detection</h2><p>下边给出根据标注数据计算groundtruth confidence maps $S^{*}$的方法，每个confidence map都是一个2D表示。理想情况下，当图像中只包含一个人时，如果一个关键点是可见的话对应的confidence map中只有一个峰值；当图像中有多个人时，对于每一个人k的每一个可见关键点j在对应的confidence map中都会有一个峰值。首先给出每一个人k的单个confidence maps $S_{j,k}^{*}$，$x_{j,k} \in \mathbb{R}^2$表示图像中人k对应的位置j对应的groundtruth position，数值如公式(6)所示，其中$\sigma$用来控制峰值在confidence map中的传播范围。对应多个人的confidence map见公式(7)，这里用最大值而不是平均值能够更准确地将同一个confidence map中的峰值保存下来。</p>
$$S_{j,k}^{*}(p) = exp(- \frac{|p-x_{j,k}|^2_2}{\sigma^2}), \qquad \qquad \qquad \qquad \qquad \nonumber{(6)}$$
$$S_{j}^{*}(p) = \max_{k}S_{j,k}^{*}(p), \qquad \qquad \qquad \qquad \qquad \qquad \nonumber{(7)}$$
<h2 id="Part-Affinity-Fields-for-Part-Association"><a href="#Part-Affinity-Fields-for-Part-Association" class="headerlink" title="Part Affinity Fields for Part Association"></a>Part Affinity Fields for Part Association</h2><p>给定一组关键点，如Figure 5(a)所示，我们如何把它们组装成未知数量的人的整个身体的pose呢？我们需要一个置信方法来确定每队关键点之间的连接，即它们属于同一个人。一个可能的方法是找到一个位于每一对关键点之间的一个中间点，后检查中间点是真正的中间点的概率，如Figure 5(b)所示。但是当人们挤在一块儿的时候，通过这样的中间点可能得出错误的连接线，如Figure 5(b)中绿线所示。出现这种情况的原因有两个：(1)这种方式只编码了位置信息，而没有方向；(2)躯体的支撑区域已经缩小到一个点上。为了解决这些限制，我们提出了称为part affinity fields的特征表示来保存躯体的支撑区域的位置信息和方向信息，如Figure 5(c)所示。对于每一条躯干来说，the part affinity是一个2D的向量区域。在属于一个躯干上的每一个像素都对应一个2D的向量，这个向量表示躯干上从一个关键点到另一个关键点的方向。Each type of limb has a corresponding affinity field joining its two associated body parts.(这句话不知道应该咋说)</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_5.png" alt="Figure 5"></p>
考虑下图中给出的一个躯干（手臂），令$X_{j_1,k}$和$x_{j_2,k}$表示图中的某个人k的两个关键点$j_1$和$j_2$对应的真实像素点，如果一个像素点p位于这个躯干上，值$L_{c,k}^{*}(p)$表示一个从关键点$j_1$到关键点$j_2$的单位向量，对于不在躯干上的像素点对应的向量则是零向量。下面这个公式给出了the groundtruth part affinity vector，对于图像中的一个点p其值$L_{c,k}^{*}(p)$的值如下：
$$L_{c,k}^{*}(p) = \begin{cases}
v,\qquad if\,p\,on\,limb\,c, k  \\[2ex]
0,\qquad otherwise
\end{cases} \qquad \qquad \qquad \qquad \qquad \qquad \nonumber{(8)}
$$
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_chatu.png" alt=""></p>
其中，$v = (x_{j_2,k} - x_{j_1,k})/|x_{j_2,k} - x_{j_1,k}|_2$表示这个躯干对应的单位方向向量。属于这个躯干上的像素点满足下面的不等式，其中$\sigma_{l}$表示像素点之间的距离，躯干长度为$l_{c,k} = |x_{j_2,k}-x{j_1,k}|_2$，$v_{\perp}$表示垂直于v的向量。<br><br>$$0 \leq v \cdot (p-x_{j_1,k}) \leq l_{c,k} and |v_{\perp} \cdot (p-x_{j_1,k})| \leq \sigma_l$$
<p>整张图像的the groundtruth part affinity field取图像中所有人对应的affinity field的平均值，其中$n_c(p)$是图像中k个人在像素点p对应的非零向量的个数。</p>
$$L_{c}^{*}(p) = \frac{1}{n_{c}(p)}\sum_{k}L_{c,k}^{*}(p) \qquad \nonumber{(9)}$$
<p>在预测的时候，我们用候选关键点对之间的PAF来衡量这个关键点对是不是属于同一个人。具体地，对于两个候选关键点对应的像素点$d_{j<em>1}$和$d</em>{j_2}$，我们去计算这个PAF，如下式所示。</p>
$$E = \int_{u=0}^{u=1}L_{c}(p(u)) \cdot \frac{d_{j_2}-d_{j_1}}{|d_{j_2}-d_{j_1}|_2}du \qquad \nonumber{(10)}$$
<p>其中，p(u)表示两个像素点$d_{j_1}$和$d_{j_2}$之间的像素点：</p>
$$ p(u) = (1-u)d_{j_1} + ud_{j_2} \qquad \qquad \nonumber{(11)}$$
<h2 id="Multi-Person-Parsing-using-PAFs"><a href="#Multi-Person-Parsing-using-PAFs" class="headerlink" title="Multi-Person Parsing using PAFs"></a>Multi-Person Parsing using PAFs</h2><p>借助非最大抑制，我们从预测出的confidence maps得到一组离散的关键点候选位置。因为图像中可能有多个人或者存在false positive，每个关键点可能会有多个候选位置，因此也就组成了很大数量的关键点对，如Figure 6(b)所示。按照公式(10)，我们给每一个候选关键点对计算一个分数。从这些关键点对中找到最优结果，是一个NP-Hard问题。下面给出本文的方法。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_6.png" alt="Figure 6"></p>
假定模型得到的所有候选关键点构成集合$D_J=\{d_{j}^{m}: for \, j \in \{1...J\}, m \in \{1...N_{j}\}\}$，其中$N_j$表示关键点j的候选位置数量，$d_j^m \in \mathbb{R}^2$是关键点j的第m个候选位置的像素坐标。我们需要做的是将属于同一个人的关键点连成躯干（胳膊，腿等），为此我们定义变量$z_{j_1j_1}^{mn} \in \{0, 1\}$表示候选关键点$d_{j_1}^m$和$d_{j_2}^n$是否可以连起来。如此以来便得到了集合$Z=\{z_{j_1j_2}^{mn}: for \, j_1,j_2 \in \{1...J\},m\in\{1...N_{j_1}\},n\in\{1...N_{j_2}\}\}$。现在单独考虑第c个躯干如脖子，其对应的两个关键点应该是$j_1$和$j_2$，这两个关键点对应的候选集合分别是$D_{j_1}$和$D_{j_2}$，我们的目标如下所示。
$$\max_{Z_c}E_c = \max_{Z_c}\sum_{m \in D_{j_1}}\sum_{n \in D_{j_2}}E_{mn} \cdot z_{j_1j_2}^{mn}, \qquad \nonumber{(12)}$$

$$s.t. \qquad \qquad \forall m \in D_{j_1}, \sum_{n \in D_{j_2}}z_{j_1j_2}^{mn} \leq 1, \qquad \nonumber{(13)}$$

$$\qquad \qquad \qquad \forall n \in D_{j_2}, \sum_{m \in D_{j_1}}z_{j_1j_2}^{mn}\leq1, \qquad \nonumber{(14)}$$
<p>其中，$E_c$表示躯干c对应的权值总和，$Z_c$是躯干c对应的Z的子集，$E_{mn}$是关键点$d_{j_1}^m$和$d_{j_2}^n$对应的part affinity，公式(13)和公式(14)限制了任意两个相同类型的躯干（例如两个脖子）不会共享关键点。问题扩展到所有C个躯干上，我们优化目标就变成了公式(15)。</p>
$$\max_{Z}E = \sum_{c=1}^{C}\max_{Z_c}E_c, \qquad \nonumber{(15)}$$
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>至此，整个模型就说完了，下面是模型在COCO 2016和MPII两个数据集上多的结果。在预测的时候，论文中使用了同一张图像的3个尺寸($\times0.7,\times1,\times1.3$)输入进去来得到比单个尺寸更好的结果。</p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_table_1_2.png" alt=""></p>
<p><img src="http://7xsbsy.com1.z0.glb.clouddn.com/PAF_table_3.png" alt=""></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://marcovaldong.github.io/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/" data-id="cjfguskx8003hcguryxgbl3l9" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Pose-Estimation/">Pose Estimation</a></div><div class="post-nav"><a href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/" class="pre">关于sematic segmentation的几篇论文（二）</a><a href="/2017/12/31/关于semantic-segmentation的几篇论文/" class="next">关于semantic segmentation的几篇论文</a></div><div data-thread-key="2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/" data-title="论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields" data-url="http://marcovaldong.github.io/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  owner: 'marcovaldong',
  repo: 'marcovaldong.github.io',
  oauth: {
    client_id: '3f1a34510c57772de8f8',
    client_secret: '69b8be94d1b53df548e46b9be32356b79e974d3c',
  },
})
gitment.render('container')
</script><div data-thread-key="2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/" data-title="论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields" data-url="http://marcovaldong.github.io/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://marcovaldong.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/信息隐藏/">信息隐藏</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/Neural-Network/">Neural Network</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/信息隐藏/" style="font-size: 15px;">信息隐藏</a> <a href="/tags/Steganography/" style="font-size: 15px;">Steganography</a> <a href="/tags/语义分割/" style="font-size: 15px;">语义分割</a> <a href="/tags/面经/" style="font-size: 15px;">面经</a> <a href="/tags/Neural-Network/" style="font-size: 15px;">Neural Network</a> <a href="/tags/机器学习基石/" style="font-size: 15px;">机器学习基石</a> <a href="/tags/steganalysis/" style="font-size: 15px;">steganalysis</a> <a href="/tags/Pose-Estimation/" style="font-size: 15px;">Pose Estimation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/My-reading-list2/">My reading list</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/小米面经/">小米面经</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/10/关于sematic-segmentation的几篇论文（二）/">关于sematic segmentation的几篇论文（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/论文阅读：RealTime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">论文阅读：RealTime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/31/关于semantic-segmentation的几篇论文/">关于semantic segmentation的几篇论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/于众目睽睽之下隐藏图像：深度隐写术/">于众目睽睽之下隐藏图像：深度隐写术</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/08/深度学习在信息隐藏中的应用（下）/">深度学习在信息隐藏中的应用（下）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/06/深度学习在信息隐藏中的应用（上）/">深度学习在信息隐藏中的应用（上）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/14/使用Tensorflow实现Titanic比赛/">使用Tensorflow实现Titanic比赛</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/19/Python爬虫小结之Selenium/">Python爬虫小结之Selenium</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://killersdeath.github.io" title="抄作业的小东" target="_blank">抄作业的小东</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Marcovaldo.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'marcovaldo'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2be92f134440f46356c71aa55035a144";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>